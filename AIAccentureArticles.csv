Article title,Article text
Can we solve the data privacy/utility problem?,"When the Monetary Authority of Singapore (MAS)-led Veritas consortium wanted to test a new fairness assessment framework, it faced a key challenge.They knew that the most rigorous results would come from using real case studies with real data. But for obvious privacy and competitive reasons, the financial services companies involved could not share that data.The solution? To use synthetic and/or anonymous data. But this itself created a new problem. They would need synthetic data close enough to the real data to be useful, but different enough to protect the privacy of the individuals involved. How could they find the right balance?This speaks to a broader problem that organizations face in many different contexts. The opportunities to get value from data are exploding. But they come with more and more privacy concerns. Especially when personal microdata are at stake.Consider just how easy it is to identify an individual from their data. Studies have shown that 87% of the US population has characteristics that make them identifiable from only three data points: Zip code, gender, and date of birth. The risks associated with disclosing personal data are therefore obvious – for both individuals and for companies who must protect the personal data of others.One answer is to anonymize data. This is a good compliance strategy for companies, and it’s recommended by most data protection rules. By processing a collection of personal data for anonymization, it’s possible to irreversibly alter it to prevent straightforward identification of the individuals who contributed the data. Yet the anonymized data can still be used for larger statistical analysis. That keeps companies in compliance with data protection rules, and lets them drive value from the original data without directly “using” it.100% anonymity = 0% value?However, anonymization creates one very particular challenge. There’s always a trade-off between data privacy and data value. The “further away” from reality synthetic data becomes, the less useful it is for analytics or for developing AI algorithms. So what you gain in privacy, you lose in value.You can’t eliminate this trade off entirely. But where you land on the spectrum between privacy and utility will vary depending on the approach you take. It’s therefore crucial to think proportionately about anonymization strategy, considering both the nature of the data and what it will be used for, as well as the privacy risks.This need for proportionality has been nicely summarised by Mark James Elliot:“We can have secure houses or usable houses but not both … An absolutely secure house would lack doors and windows and therefore be unusable. But that does not mean that all actions to make one’s house more secure are pointless, and nor does it mean that proportional efforts to secure my house are not a good idea. The deadbolt on my door may not help if a burglar comes armed with a battering ram or simply smashes my living room window but that does not mean that my lock is useless, merely that it does not (and cannot) provide absolute security.”For businesses, the objective is to find the most acceptable balance between privacy and value when dealing with personal data. To do that effectively, they need to fully understand the extent of the trade-offs they’re making. But until now, there haven’t been any off-the-shelf assessment methods or audit tools to do this.This is where Labs’ new Automated Privacy & Value Assessment Tool (APAT) comes in. APAT takes a dataset and evaluates both the privacy and utility of various anonymization strategies, providing a recommendation on the best option for a particular use case.It’s fully automated and generalizable to any anonymization process.Having uploaded the original dataset and the different anonymized versions they want to compare, the user is given a percentage score for three key metrics.The combination of these three metrics allows the user to make a more informed decision about which anonymization approach to take in each case.How did APAT help the the Veritas consortium solve its data privacy problem?Accenture’s Labs and Applied Intelligence groups helped the MAS-led Veritas consortium select a suitable synthetic dataset for one of its key use cases: predictive underwriting for life insurance.To do this, different anonymized dataset versions were synthesized from the original data. This synthetic data was then tested using APAT, producing scores for the level of utility, privacy and similarity to the original dataset.Armed with this information, the Veritas consortium was able to select the most appropriate anonymized versions of the original dataset. They could produce a synthetic dataset with accuracy in line with marketplace benchmarks – and that could not be used to identify individuals in the seed portfolio it was generated from.A strategic tool for all sensitive data use casesAnonymisation and synthetic data generation are promising solutions to the data privacy challenge. But it’s critical to understand the trade-offs between privacy and utility.  APAT offers a new audit capability that helps organizations make more informed decisions about anonymization strategies.  And it can be used in any industry use cases that deal with sensitive data and the need to balance privacy with data value. It can even be extended to questions of fairness and bias as well.We’re excited about the potential of this tool to help companies manage the ethical and regulatory challenges of working with data. If you’d like to know more, or see the tool in action, contact Medb Corcoran, Managing Director, Accenture Labs. And for more on this topic, read our recent report on the business value of synthetic data, “Flipping the script on deepfake technologies.”The authors would also like to acknowledge the efforts of Jer Hayes and Richard Vidal from Accenture Labs; and Dimitrios Vlitas, Henrietta Ridley, and Aurora Armiento from Accenture Applied Intelligence on this work."
Driving real value with synthetic data,"Ask someone if they’re familiar with generative adversarial networks, and they might say no. But ask that same person if they’ve heard of deepfakes, and they’ll probably say yes. Deepfakes – a combination of “deep learning” and “fake” – get a lot of attention, and it’s easy to see why. With technology that’s already available, almost anyone can generate convincing audio or video of someone saying or doing things they never said. In a well-known example, a scammer used an audio deepfake to con a U.K. energy firm out of €220,000.In fact, though, deepfakes are just one application of generative adversarial networks (GANs). And the focus on only negative applications of GANs means businesses are leaving real value on the table.Generative network technology is about creating realistic synthetic data. In the case of a deepfake, the goal is to create audio or video that can fool human viewers. For businesses, though, synthetic data can be used to generate value: in product development, better training for AI systems, artistic enhancement, and even consumer privacy.Researchers at Labs are working in these spaces and more. They’ve used synthetic data to speed up the process of testing new product formulations, while also exploring more possible formulations than before. They’re looking at how synthetic data can be used to better train computer vision systems; this will drive improved retail experiences for customers and companies alike. And they’re working to help businesses strike the right balance while using synthetic data for privacy purposes. An automated privacy assessment tool will give companies a way to evaluate different anonymization strategies for data, including synthetic datasets, and choose the one that’s best for a given task.Of course, even as companies use synthetic data to generate value, bad actors will continue their efforts. Our Labs researchers have also been active in deepfake detection, applying an ensemble of AI models to analyze content. Identifying malicious content that’s the result of deepfake technologies will be key as companies look to drive value with synthetic data while guarding against the reality of bad actors.It’s shortsighted, though, to focus only on the potential for negative impact – and businesses doing so are leaving value on the table. There’s a lot to gain from creative, thoughtful use of these innovations. The true story: synthetic data offers real value, in everything from product development to healthcare to the entertainment industry. How will you capture it?To learn more about these technologies and the opportunities they’re driving today, read our new report, “Deepfakes, real value: flipping the script on deepfake technologies.”"
Driving better decisions with knowledge graphs,"Knowledge graphs are an important and highly practical emerging technology. That’s why Accenture sponsored the Knowledge Graph Conference for the third year in a row. We also organized the second annual Knowledge Graphs for Social Good workshop in collaboration with Lambert Hogenhaut, Chief of Data Analytics for the United Nations.Knowledge graph technologies offer a way to semantically represent and connect concepts in various domains. What does that mean? Knowledge graphs are a tool that help companies connect the dots – or more accurately, connect their data. They help resolve big enterprise challenges like data silos, tracking lineage and domain data mapping. With these challenges tamed, companies can put their data to better use in decision-making.KGC 2021 showcased the latest advances in knowledge graph implementations and applications. We delivered a keynote talk at the conference on accelerating industry data integration. We covered our approach in building a knowledge graph that can improve the data supply chain with data profiling and mapping of siloed data sources. We can also put this type of knowledge graph to use as a catalog for the data mesh, a new paradigm shift in data platforms (more on that later).Building on last year's workshop and our ongoing efforts to advance sustainability through technology, we also organized the second Knowledge Graphs for Social Good workshop. This year’s workshop discussed the use of knowledge graphs in applications ranging from preserving indigenous culture to pursuing gender equality. Tina Comes, Associate Professor at Delft University of Technology, delivered a keynote talk on “Data for Good – A Resilience Perspective.”This talk focused on building a resilient environment. Dr. Comes showed how knowledge graphs can be used to help respond to natural disasters, understanding local policies by enabling information sharing among communities and across organizations. She also suggested using knowledge graphs to build a digital twin that represents key elements in risk management and disaster resilience, to predict trends in data and identify the sources of problems.Other talks showed how to build a knowledge graph solution with some very low-resource and low-connectivity hardware (for example, based on Raspberry Pi devices). This offers a way to bring the power of knowledge graph approaches to rural areas that may lack sophisticated tech infrastructure. Attendees also discussed how diverse traditional food recipes can be connected to various cultures, and how knowledge graphs can support building these connections for making more culturally aware intelligent agents. The workshop talks and discussions continued to show how knowledge graphs can contribute to ensuring fairness, sustainability, and transparency in automated systems.Evolving data strategies and opportunitiesThroughout the larger KGC 2021 conference, data mesh architectures came up frequently, and for good reason. Just as knowledge graphs are an evolution of master data management, the data mesh is an evolution of what people call data lakes. It's a paradigm shift in analytical data architecture: moving from everything-in-one-place monolithic approaches to a distributed architecture. And considering the very distributed nature of most large companies, the data mesh is a much more practical approach. This new architecture saves a lot of time that data scientists previously had to spend understanding, locating, and cleaning up the data they would need for a given use.There are four “ingredients” to the data mesh approach: decentralizing the data responsibility to domains serving it; treating data as a product; making self-serve infrastructures and building federated data governance.These four “ingredients” are a big part of why knowledge graphs and the data mesh approach complement one another. We mentioned treating data as a product. Data products are the main component of the data mesh approach. They can be produced either closer to raw sources (for example, data coming from a wearable device like a smartwatch), or they can be more insight-oriented, derived from other data products (and therefore further from the original sources).In either case, though, these data products need to follow the quality metrics that make them user-friendly for their consumers. They should be complete, clean, documented, and accompanied with the code that maintains and serves them. The semantics locally defined for data products should be linked with other data products in a standard way – and that's only possible with the knowledge graph emerging from data mesh. That’s why we build machine learning approaches to make the data products more contextualized and connected with each other.The way forwardWhether it’s for broader goals around social good or for specific industry use cases, knowledge graphs provide a path toward better data-driven decision-making. They’re the backbone of digital twins – another key technology that we’re exploring at Labs – powering both data integration and analytics. We’ve also used knowledge graphs to build a scriptless conversation platform, developed tools to extract causal graphs from unstructured text data, and developed a graph-based platform to discover and mitigate active cyberthreats.Where will knowledge graphs drive innovation next? Watch this space to find out!To learn more about the knowledge graph conference, visit the KGC 2021 website. For more information about the Knowledge Graphs for Social Good workshop, read the United Nations report on the workshop and watch the video recording. Details of our work on accelerating data integration is also available to watch.  For more information on our work with knowledge graphs, contact Neda Abolhassani or Vivek Khetan. "
"AGI, GPT-3 and French patisserie—driving value with AI","Even with true artificial general intelligence many years away, the pursuit of AGI offers businesses tangible value today.Suppose you ask an airline customer service agent “Can my mother bring crutches on board?”As a person hearing or reading this, we recognize that there’s more to it than just a policy question. Sure, the question itself is about whether the rules allow crutches on board a flight. But we also understand that a customer is calling on behalf of someone who is physically incapacitated in some way. Maybe it’s due to age or injury; we don’t know the cause, but we realize that they may also need a wheelchair when they get to the airport, or help getting on and off the plane, or transportation that can accommodate their mobility issues upon arrival.It might take some training to know how to arrange these services. But it takes no training to recognize the need for these services when we hear the question. Similarly, a five-year-old seeing a traveler on crutches at the airport will immediately understand her need for assistance. As humans, we effortlessly bring common-sense reasoning to our interactions with the world; it’s how we navigate situations even when we haven’t experienced them before. Yet for all the advancements made in artificial intelligence, this type of inference is still beyond the reach of today’s AI systems.Of course, this lack of common-sense reasoning in AI hasn’t kept us from applying AI effectively in a wide range of applications. Computer vision systems, for example, may outperform physicians in spotting certain forms of cancer in images. It’s just one instance where machine learning has been applied to a narrow problem in a very useful way. These systems can’t explain why cancer is bad – they don’t have that kind of common-sense reasoning – but we don’t need that system to know why cancer is bad, just to find it. There are lots of similar situations where high performance on a narrow task is very useful.If we wanted this same kind of AI system to help our incapacitated traveler, though, it would fall short. It might answer the policy question – does the airline allow crutches on board or not? – but it would not understand that the customer likely needs more help to have a safe and pleasant travel experience.That’s why there’s a renewed focus on enabling common-sense reasoning in AI systems. This research theme is referred to as “artificial general intelligence” (AGI). The intent is to enable broad competence on a wide range of cognitive abilities, similar to that exhibited by humans, and even animals, to a degree. AGI is really just a return to the original goals of the field: in a nutshell, to replicate human intelligence. And while this goal was articulated in the famed 1956 Dartmouth conference, full AGI remains, by most accounts, at least decades away.So why am I bothering you with something decades away? Because in practice, even incremental improvements on the way to AGI still offer practical value to businesses today. And to see that, we can start with something you’ve probably already heard about: GPT-3.Language models and artificial intelligenceGPT-3 is OpenAI’s newest language model, a deep learning model trained on around 300 billion words. It’s able to “predict” the text that is most likely to follow a given prompt.For example, if you ask GPT-3 a question, it will produce a stream of text that the model predicts (based on all the text it’s been trained on) is the most likely combination of words that would follow. In other words, an answer.To be clear, this is not “artificial general intelligence” (AGI) as we currently understand it. GPT-3 is not replicating any of the broad human cognitive capabilities that we associate with AGI, such as common-sense reasoning. Rather, it is an extremely powerful language prediction tool. The results, though, can give an uncanny impression of intelligence.We’ve seen examples of entire newspaper articles written by GPT-3. And it can produce a surprisingly real sounding “fake” press release given two just samples as an input. So while GPT-3 isn’t really doing common-sense reasoning, it’s a step closer to it.GPT-3 as a practical business tool?What does this mean in practical terms for business? This is where French patisserie comes in. (I know you’ve been wondering.)In one of our own experiments with GPT-3, we wanted to explore how it could handle the “localization” of a particular passage of text. In other words, change the text to be more customized for a particular geography. This is a key need for marketing teams trying to tailor their global content and make it resonate in local markets.Among the many things that GPT-3 has learned is the association between locations, typical foods, and landmarks. So, given a series of prompts like “Stroll down Michigan Avenue then stop for a deep-dish pizza in this casual and trendy jacket” it was able to produce localized versions for different cities:These were really interesting—and impressive—results. But did you spot the mistake? Yes, GPT-3 mixed up a macaroon and a macaron. A minor error for the model perhaps, but a potentially big deal for a brand that wants to appeal to French consumers.So, while this example neatly illustrates the strengths of GPT-3 and how they might be applied to offer business value today, it also shows its limits. We will often still need a human “sanity check” of the results of language models as they’re being put to practical use in business.A step closer to AGI?There are lots of other business problems that GPT-3 could help solve, from populating knowledge graphs to enriching metadata—situations where imperfect but broad results from a language model can complement deep but narrow results from other forms of machine learning. Plus, of course, it can offer a complement to good old human common sense.This brings us back to the larger point about AGI. Whether or not language models represent a step closer to machines having common-sense reasoning, there are numerous real-world scenarios where just a little more machine intelligence can make a big difference.For example, we’ve been looking into areas like “scriptless” chatbots that allow a virtual assistant to draw a whole series of inferences from a conversation, and start solving broader problems outside of the narrow questions being asked. The intent is precisely to allow us to provide assistance of the kind needed for our incapacitated traveler and her cumbersome crutches.This is still not AGI. But it’s a potentially very big step forward for customer service all the same, while we’re on the path to AGI.Focus on the here and nowAnd that’s really the key point here. Businesses don’t need to wait for all-powerful general artificial intelligence to develop significantly more powerful applications today.There’s a great deal to be achieved with systems that go beyond traditional machine learning and begin to use a little more knowledge and reasoning to handle a broader range of use cases. And as AGI research progresses, those use cases will continue to expand.So companies should be looking for business problems in which a little more knowledge or reasoning would let a particular system do a lot more – even if it still needs to be complemented by some human common sense.To learn more about these technologies and the opportunities they’re driving today, read our new report, “The Path to Artificial General Intelligence” — and don’t hesitate to get in touch with me at andrew.e.fano@accenture.com. "
Tackling supply chain challenges with AI,"COVID-19 sent a clear message to companies: supply chain disruption is a real and constant threat. Chip shortages made carmakers pause assembly lines. Scarce ingredients slowed production of sanitizers. And rising demand for key drugs and vaccines put pharma businesses under real pressure. The good news: artificial intelligence can help. We’ve applied two novel AI methods to address supply chain disruptions like these.First, we’ve used what are called counterfactual explanations to help clients quickly update existing products. This identifies specific changes companies can make to reduce or avoid potential disruptions. For example, if a key ingredient for a chocolate bar is suddenly going to be unavailable in large quantities, counterfactual explanations can help find recipes that use less of the problematic ingredient, but still create a chocolate bar that meets the company’s criteria for taste, cost, and so on.So how does it work? You may remember how we’ve used counterfactual explanations in the past. Simply put, we look at the effect that making small tweaks or changes to various inputs to a model have on its output. In our previous blog, we showed how this could be used to explain why an applicant for a loan was denied, and also show them what changes they would need to make to be approved. Well, counterfactuals are versatile – they work for product development too!In this case, we look at the effect that small tweaks or changes to ingredients would have on predictions returned by a machine learning model about the recipe’s results. The model assesses a specific key performance indicator defined by the product manufacturer. This could be something like how much many calories are in a chocolate bar, for example.This approach reveals two things. First, it uncovers what matters most in the model in terms of ingredients. And second, what changes can be made to an existing product, while taking factors such as cost into account. This lets companies “test” a wide range of possible recipes and uncover likely candidates before moving on to physical tests in a lab.In contrast, traditional ways to find alternatives from a near-limitless range of options can often miss the best possible substitute. Some ingredients may be more expensive in one region than another. Alternatives might have undesirable properties or effects on the resulting product. Or there simply may not be enough of them. And because finding substitutions manually requires people with domain expertise, who are already in high demand, the whole process is going to be much slower.  The second approach we’ve applied is called deep generative networks. These come into play when a more fundamental change to a product or formulation is needed. We’ve used deep generative networks for the discovery of new drug molecules, for example.In the context of supply chain disruption, we’ve used these networks to explore product formulations and suggest component substitutions. The AI uses data from current and historical formulations to find brand new ones. The results? New products at potentially lower cost with higher performance.And the same technique could be used across countless industries. For example, they could assist in the drive toward more sustainable products by finding alternatives to components that don’t meet sustainability standards.These two approaches give companies a way to use AI to mitigate risk and ensure continuity through global supply chain disruptions. And of course, their use isn’t limited to times of disruption.We could soon be using generative networks to inspire the design of novel products for completely new markets. Or we could use counterfactuals to improve the performance of the equipment that underpins supply chains. Picture a model that identifies the minimum changes needed in operating conditions to keep a piece of manufacturing equipment at peak performance. Or one that finds the changes needed to keep a delivery truck on the road for more time without maintenance. AI is helping address today’s problems – and uncovering tomorrow’s possibilities.For more information on Counterfactual Explanations at Accenture Labs, contact Luca Costabello. To learn more about Deep Generative Networks for novel product formulations, reach out to Jer Hayes."
Driving collaboration with privacy-preserving computation,"It’s no secret that data drives intelligent decision-making. Across industries, companies with superior data and analytics practices outperform their competitors. It also shouldn’t be a surprise that companies can unlock even more value by sharing data with each other. Gartner has predicted that by 2023 – just two years from now! – organizations that promote data sharing will outperform their peers on most business metrics.But most companies are wary of collaborating when it comes to sharing data. Sometimes they’re worried about preserving their intellectual property. Other times they aren’t sure how to collaborate without running afoul of regulations like GDPR or HIPAA, which can prevent direct sharing of data. Whatever the reason, companies are leaving a lot of value untapped.That’s why we’re building on our previous work with privacy-preserving technologies to develop a new framework for privacy-preserving data cooperatives. With a well-designed cooperative, companies can share data and collaborate without concerns around trust, compliance, privacy and data control or ownership. There are many technological techniques available to make this possible; confidential computing, homomorphic encryption and multiparty computation can all contribute. But most important, we’re working to standardize the interfaces between organizations, making it easy for them to join a cooperative effort.Given the challenges of the last year, the idea of data sharing has received particular interest from healthcare. But in the healthcare industry, perhaps more than anywhere else, maintaining data privacy is essential. Patients’ medical records must be protected. Enter privacy-preserving data cooperatives! If we can deliver ways to share data between health institutions while maintaining patient privacy, AI and other tools can help expand knowledge by analyzing data from multiple sources.We built a proof of concept to show the value of our framework for a very practical case: detecting sepsis. Sepsis is a life-threatening condition that can occur as the body responds to infection. It’s a serious challenge for hospitals. It’s easy, then, to understand why hospitals would be eager for a better way to detect it. And by getting multiple hospitals to participate in a data cooperative, we can build a more accurate sepsis detection model.We evaluated different techniques to bring this scenario to life. Ultimately, we built a proof of concept that uses a combination of Intel SGX secure enclave technology, enabled by the Fortanix Confidential Computing Manager, along with federated learning. It demonstrates how to train and use an AI model to detect sepsis in just this way.Individual models are trained at the individual participants’ ends – that is, at the participating hospitals. The resulting models are then aggregated inside a secure enclave. This creates a single, global and “more knowledgeable” AI model without any hospital directly sharing its data with any other hospital, nor with the cooperative itself!Each hospital would then benefit from a globally trained model to be used for their local “predictions” on their private patients’ datasets. If they used a model trained with this approach, individual hospitals could apply the model to real-time data from their own current patients in order to better detect cases of sepsis early, when there’s a better chance to treat it. The cooperative framework approach itself can also be reused by the participants for different, future use cases with different data sets. They can continue to build collaborative insights while maintaining their data privacy.The proof of concept is built as a prototyped extension of Accenture Applied Intelligence’s AIP+ service, a collection of modular, pre-integrated AI services and capabilities designed to simplify the adoption of AI solutions.Using privacy preserving data cooperatives guarantees that all participants will have a standardized infrastructure to share. It also means they can stay in compliance with data regulations and maintain their own data privacy while using it. As these cooperatives become more established, they will also offer standardized on-boarding: new participants can join easily, encouraging continued and expanded collaboration.Data privacy concerns are always top of mind for enterprise. Rightly so! But with the evolution of privacy-preserving and confidential computing techniques, companies can reap the benefits of data sharing for garnering richer and deeper comprehensive intelligence without sharing the data itself. Watch this space for more about our efforts in privacy-preserving computation. For more information or to schedule a demo, email Giuseppe Giordano and Kuntal Dey."
Uncovering Human+AI opportunities through experiments,"We’ve talked a lot about the human+AI approach, and for good reason. Humans and artificial intelligence working together can accomplish so much more than either can do apart. But it’s not always clear how to begin harnessing the potential of these powerful collaborations.That’s where experimentation comes in. Purposeful experiments on the use of human+AI systems in your workplace can help you uncover the path to new business value. We’ve done experiments with the human+AI approach, and the lessons we learned can help you to craft your own experiment and use it to ensure that when there is change, it’s for the better.And it’s more important than ever to ensure that we’re implementing AI thoughtfully. 2019 focused the business world’s attention on the needs of stakeholders beyond the shareholder, including the need for greater security for employees and greater equality of opportunity in communities. The pandemic and recession of 2020 deepened those needs. Then, at the end of 2020, research revealed that machines were taking over tasks from humans faster than ever, the result of shifts driven by the pandemic and recession. And it’s well understood that those with lower levels of skills and education are at the most risk, since automation and AI take on lower complexity tasks most readily.As an AI tech researcher, building and promoting AI systems, I might have wondered whether we’re only making matters worse if I hadn’t learned otherwise from a prior experiment. It was run at our Lab and Accenture’s innovation hub, The Dock, in Dublin. My colleagues and I had co-created and tested a new role for workers who had an AI teammate. You can read more about our experiment at Sloan Management Review, but in short: bringing AI into the workforce doesn’t have to be a zero-sum game. It’s not human versus AI.In fact, our experiment proved that people gained valuable skills and knowledge while training an AI system designed to help them in their roles. People were enabled to teach their AI colleague, improving the accuracy of the process for customers. We then saw how the up-skilled workers could support the development of new products and services. Employee up-skilling, customer service improvement and enterprise value – all as a result of how we had experimented with AI.For me personally, this was a uniquely interesting project. People often cite lessons from history that technology innovations create more jobs than they eliminate – but these had fallen short of being completely reassuring for me. What if this time it’s different? It certainly feels different from previous waves of automation. Machines are getting ever better at mimicking activities we consider especially human, like talking and reasoning. Can we make sure that new jobs are created, and that it happens in time to not leave people behind?We ran our project as an experiment precisely because we had more questions than answers (these and many others). As you’ll see below, the experiment showed that the outcomes we valued were possible and illuminated practical steps to make them real. You can use a similar approach to map the steps from your organization’s shared values to new human+AI value in your business. Where do you start? Here are guidelines based on our experience, along with examples of what they helped us to learn.For many teams, experiments are unfamiliar. Projects with more certain outcomes are the norm. Some may use “A/B testing” of alternatives, but that essentially precludes learning about “unknown unknowns.” Others might use the word experiment to mean “let’s try X, and who knows what will happen!” But if you haven’t done the homework to shape a crisp hypothesis, you might not be asking the questions that will result in the greatest learning.We planned to test our hypothesis by enhancing an AI system that was already in place. Registered nurses working as medical coders had been using the existing AI system to help them annotate medical records. The records, produced as patients interact with care providers, are often unstructured. Annotating them with standardized codes helps with payment processing and data analysis that can improve patient care.We found that the existing AI was not a medical know-it-all. Rather, it learned from the medical coders, who had extensive medical knowledge and clinical experience and used it to help improve the system’s outputs. That learning was a clunky process, though. It required coders to correct the AI several times before a data scientist was alerted to update the AI’s knowledge base. So, the relationship between the old system and the medical coders did not fit our vision of being “symbiotic.” To create symbiosis, we needed to tighten the loop between the coders and the AI. The AI would continue help the coders, but the coders would take on a direct role in teaching the AI. Ultimately, the new role was a success. Medical coders who had no previous data science training learned to take on the role of training the AI. The accuracy of AI training by the coders was high, and they strongly agreed that they were acquiring new knowledge and skills; meanwhile, assessments also showed that coders could apply the new concepts to other scenarios. Eight out of nine coders were more positive about working with AI than they had been prior to taking part in the experiment.It’s going to be quite some time until AI surpasses human abilities in many areas. Meanwhile, there’s much value to be had by teaming humans and AI together. Spotting that value and how to tap it sustainably and responsibly in your context might be best discovered through an experiment like ours.This research was a collaboration between colleagues at Accenture Labs, The Dock, Accenture Research, Accenture Insights Driven Health and Dr. Claire O’Connell, Irish Science Writer of the Year 2016.To learn more about our ongoing work in the human+AI space or about how an experimental approach can help you unlock its value for your business, contact Diarmuid Cahalane.

"
Neuromorphic computing brings intelligence to the edge,"Today, it seems everything is getting “smart”. From home appliances to vehicles to industrial equipment, more and more products and services are using artificial intelligence—AI—to interpret commands, analyze data, recognize patterns, and make decisions for us.It’s easy to see why AI-powered products are so popular. Smart interfaces let people control devices by voice and gesture rather than buttons or touchscreens. It’s a much more natural way to interact with a device.AI can also make a product more autonomous, relieving us from tedious work or repetitive activities. On top of this, smart products can enable data analysis and continuous optimization, such as monitoring and alerting us about our health or predicting when a piece of equipment needs to be serviced or replaced.Taking AI to the edgeThe catch? The successes of smart devices to date are fueling demand for ever more sophisticated AI-powered experiences. And we’re starting to hit the limits of what existing hardware can deliver.Much of the processing that powers today’s smart products is actually handled remotely (in the cloud or a data center) where there’s enough computing power to run the necessary algorithms. This means that a network connection is essential, and that can also increase latency as data is transferred back and forth. There are also real and perceived data privacy considerations to think about when sending some kinds of data to the cloud.These considerations point to advantages of putting more of the smart processing within the device itself. This is known as “edge AI” since the processing is done in devices at the edge of the network rather than the centralized cloud.But edge processors are often constrained to be compact, and many are mobile, which often means powered by batteries. How can we run power-hungry AI algorithms on smaller, low-power edge devices? For that, we’re going to need some new thinking about how AI hardware is designed.Time for some brain powerThis is where neuromorphic computing comes in. It’s a new kind of computing architecture based on the way a biological brain processes information.Consider the fact that the average human brain contains between 80 and 100 billion neurons, each of which works highly efficiently and asynchronously to provide massive parallel processing. It’s this power and efficiency combination that enables us to be so smart without needing to continuously ingest vast amounts of energy.One of the most promising forms of neuromorphic computing uses “spiking neural networks” to emulate the way a biological neuron fires (or “spikes”) to transmit a signal before returning to a silent state.The result is a system which is far more power efficient than the artificial neural networks used for most AI systems today. And that efficiency opens up the possibility of carrying out much more AI processing on smaller low-power devices at the network edge.In fact, in addition to high efficiency, neuromorphic systems have several key advantages:Some way to go … but the journey will be worth itWe’ve already seen a lot of progress in scaling and industrializing neuromorphic architectures. Accenture Labs has been experimenting with Intel’s Loihi system since 2019, for example.But there are still challenges to resolve in building complete neuromorphic solutions. For a start, existing machine learning algorithms won’t be directly compatible. So some modification or rework will be needed for them to take advantage of a neuromorphic system.What’s more, to scale up adoption, we need the tools for developing, debugging, and deploying neuromorphic solutions to be as robust and user-friendly as the tools we already have for existing AI hardware. Finally, the processors themselves are still in development. There are several research-grade chips available now, but none yet being produced at the large industrial scale of CPUs and GPUs.In an emerging field, challenges like these are to be expected. But as the technology matures, the potential business benefits are significant. Early use cases are likely to be in areas like adaptive robots, smart vehicles, and advanced consumer interfaces.Neuromorphic is made for the edgeAs AI-enabled services become ever more pervasive, organizations will need to think about how their computational strategies need to evolve to keep pace with edge technology—and with customer expectations.Neuromorphic processors and sensors will fill an important niche in that strategy by enabling real-time intelligence and continuous onboard learning—on a tight energy budget—at the network edge.For many businesses, that’s going to be a key source of future competitive advantage. So it’s important to understand the potential and start experimenting with neuromorphic now.To learn more about this technology, click here to read our recent report."
Developing energy-efficient machine learning,"Machine learning is everywhere. It now drives everything from tumor-detecting algorithms to facial recognition programs and, for those who remember Silicon Valley, an app that tells you whether or not something is a hot dog.This type of artificial intelligence is now so ubiquitous that we don’t think twice about it. It’s foundational technology. But as people have gotten used to “ML everywhere,” they’ve overlooked one of the challenges with machine learning: its energy impact. We’re working to mitigate that impact, looking toward sustainable, energy-efficient machine learning.Today, training highly complex models often requires staggering energy consumption. Researchers who reviewed a prominent architecture used for natural language processing found that training the model once required more than 650,000kWh of energy over 84 hours. This generated roughly the same estimated CO2 emissions impact as 57 humans would over the course of an entire year of their lives.Without new approaches, machine learning’s energy impact could quickly become unsustainable. So how can we use it responsibly?To start, we need a good understanding of the relationship between the approaches used to train a model and the energy they require. We conducted several experiments to measure energy consumption for model training, tweaking different parameters of the architecture.We used a publicly available data set introduced by the British statistician and biologist Ronald Fisher in 1936. For each of three species of iris flowers, it contains 50 samples. We decided to investigate what happens when we train a small neural network on this (tiny!) data set.Iris virginica. Photo by Frank Mayfield.A machine learning algorithm makes a number of passes, or “epochs,” over a data set while being trained. In our research, there was always a threshold of epochs at which the accuracy of the model quickly plateaued, but energy consumption continued to increase. For example, as we progressed through the training, the model consumed only 964 joules of energy to achieve a training accuracy of 96.17%. But to gain an additional 2.5% accuracy improvement, the model required more than 15 times more energy in additional training – 15,077 additional joules!To put that in practical terms, that amount of energy could light a 7W LED lightbulb in a rural household for almost 40 minutes. If that doesn’t seem like much of an impact at first glance, remember what we said about machine learning being ubiquitous. If we can save even small amounts of energy each time machine learning models are trained, we could have a significant impact on energy use and sustainability.We also found that larger training data sets require significantly more energy to train models on (as you might expect) but don’t necessarily lead to a proportional benefit in accuracy. In one experiment with a small convolutional neural network (CNN) model, we tried using just 70% of the training data and compared that with the results of using the entire set. Using the whole set consumed 47% more energy, but the resulting model’s accuracy barely outperformed the one trained on just 70% of the set. The improvement in accuracy wasn’t even 1%.In short: there are viable paths today to training machine learning models in a sustainable, energy-efficient way. You might start by thinking about your use case: just how accurate does your model need to be? If you’re classifying medical imaging to help doctors diagnose patients, maximizing accuracy via more training epochs or a larger data set may be worth the extra energy impact. If you’re using the technology for a less critical purpose, there might be a lower accuracy target that would meet your needs and save energy in training.There are other technological options to consider as well. Do you even need to create and train a new model from scratch? Transfer learning, where an existing model is repurposed for a different task, may be another option to save energy as well as time.We’re proposing an overall approach for machine learning that’s akin to what’s done when doing final software testing to triage any remaining bugs in a system. In those cases, the software’s overall reliability level is balanced against the effort that would be required to find and remove any further bugs without introducing new ones. If that effort would be highly intensive and the software is already acceptably reliable, the software is released.We need a similar approach to make informed decisions about training and model accuracy while being energy efficient with ML. We’re creating an advisor that highlights the implications of ML design, development, and testing choices on energy efficiency and sustainability. And don’t forget the ongoing rapid advances in specialized hardware and computing frameworks for machine learning. Traditional computing architectures need a lot of power to perform machine learning tasks. But approaches like neuromorphic computing (which our colleagues in the Future Technologies group are working on) are a better match for machine learning needs, and as they reach maturity, they’ll provide another path toward energy-efficient ML.There’s a growing community and effort around creating more efficient and sustainable machine learning, and for good reason. Stay tuned to learn more about our research here at Labs! Want more information about our work, or interested in collaborating? Contact Vibhu Sharma and Vikrant Kaulgud."
Battling misinformation: detecting malicious deepfakes,"The term “deepfakes” has become well-known as AI-generated videos of real people, including prominent government and business leaders, have become more common. And while the original term referred to altered images and videos, deepfakes have branched out into a slightly broader definition to include spoofed audio too. Earlier this year we talked about using deepfakes for good, an area that Labs continues to explore. At the same time, we know that bad actors will continue their efforts to use deepfakes for malicious reasons – so we’re also exploring ways to automatically detect them and stop their spread.Many researchers have tried to train artificial intelligence models to detect deepfaked videos or images. One type of model in particular, a convolutional neural network, is very well known in the space. These models tend to perform well with training data; they can teach themselves to detect deepfaked images and videos from a curated dataset. But they often struggle to detect deepfakes “in the wild” when presented with new data. It’s a bit like performing great on a practice exam but failing the real test.Why is this happening? It has a lot to do with the way these AI models learn. What we’ve seen is that when the models learn from the training data (curated collections of real and fake videos and images), they’re cheating. They’re not learning from the digital artifacts that would suggest something might be a deepfake, like identifying the blending occurring between forged and non-forged regions of an image. Instead, they’re learning which faces belong to which category of data. If a certain face shows up repeatedly among the faked training data, the next time the model sees that face, it says it’s found a deepfake. They’ve basically learned a shortcut to find deepfakes in the training data – a cheat for the “practice test” that won’t work in the real world.We set out to build a better approach. We developed an ensemble of models, using some of the previously known methods as well as patent-pending novel approaches we developed here at Accenture’s DC Cyber Lab. The models learn different features of the content being analyzed. This helps minimize the chances that the results will be based on a “cheat.” Once each model has done its job, the solution calculates the likelihood that the content being examined is the result of deepfake technology.We tested our approach on the “Celeb-DF” dataset. Two other well-known models have also been benchmarked against this dataset, which gave us a chance to compare our solution’s performance to others. Using the two metrics they leveraged, we found that our approach led to a more robust and reliable analysis. On average precision – the accuracy of a model when it predicts that a piece of content is a deepfake – our solution scored about 9% and 32% better than the other two models. Our approach also significantly outperformed the same models on the ability to identify deepfakes without incorrectly flagging real data as fake.We see solutions like ours as part of a multi-pronged effort to protect and defend against malicious deepfakes. Faked videos and audio can be easily propagated through social media to spread misinformation or disinformation, which not only affects those being misled, but hurts social media platforms and news organizations themselves by eroding trust. These organizations have good reason for concern about malicious deepfake content being circulated on their platforms, but human content moderators need help from automated detection algorithms like ours to stay ahead of the increasing stream of fakes.And while misinformation campaigns will certainly be fertile ground for deepfakes, the use cases will not end there. As time goes on, there will be many more direct attacks. A European energy firm fell victim to direct attack with a deepfaked voice: a CEO was on the phone with someone he believed was his boss, the CEO of the parent organization. Unfortunately, the person on the other end of the line was not his boss but rather software impersonating his boss. The caller requested an urgent transfer of funds to a supplier. Believing the call was authentic, the executive complied. The company was defrauded out of $243,000 and alarm bells were only set off once the fraudster called back requesting a second transfer.As time moves forward, integrating automated software solutions like ours to content screening and perhaps even direct communications systems will be crucial to the battle against malicious deepfake content. Of course, as deepfake detection improves, so will the deepfakes themselves. That’s why it’s critical for researchers in this space to continue sharing ideas and building on previous efforts. Be on the lookout for our soon-to-be-published paper, “An Exploration into Multiple Deepfake Detection Approaches and the Case for Model Stacking” which will outline both successful and failed approaches.For more information about deepfake detection work from our DC Cyber Lab, contact Neil Liberman and Malek Ben Salem. "
Tackling medical misinformation in social media with AI,"As COVID-19 has spread throughout the world, it has driven more and more of our lives online. We’re relying on online shopping to buy goods and services, digital collaboration tools for work and learning – and increasingly, on social media platforms for information about the pandemic. A recent study saw a 25% increase in the volume of posts on Twitter since the spread of COVID-19 began. Unfortunately, some of that increasing volume includes misinformation about COVID-19 – enough that Twitter outlined the criteria it uses to assess and remove misleading pandemic information on its platform.Misinformation online isn’t an isolated phenomenon related to COVID-19. A well-known 2018 MIT study found that misinformation spreads rapidly online no matter the topic, reaching more people than the truth and spreading faster. Unintentional sharing of incorrect information, sensationalism, rumor, and urban legends proliferate; we see spamming and trolling attempts; we even see complex and deliberate attempts to misinform, like deepfakes.Working with Indraprastha Institute of Information Technology Delhi, we’ve developed a robust AI-driven approach to this problem. Our solution: a semi-supervised end-to-end attention neural network that detects Twitter posts with misinformation about COVID-19. In early pilots on a dataset of more than 21 million COVID-19-related tweets, it identifies posts with misinformation with 95% accuracy, significantly outperforming comparable algorithms.This kind of semi-automated detection is key in light of the growing misinformation challenge. When it comes to COVID-19, the WHO director general stated recently that “We’re not just fighting an epidemic; we’re fighting an infodemic.” The spread, speed and complexity of misinformation in social media is overwhelming the human capacity to manually fact-check and regulate it, and companies are increasingly deploying artificial intelligence to assist human fact-checkers. Still, most current AI models need humans to manually label or categorize large amounts of data before the systems can work. Even then, they struggle to identify misinformation that differs from what was found in the training data.With the evolving avalanche of misinformation about COVID-19, these are significant challenges. The types of related misinformation range from incorrect health advice (for example, “eating garlic cures the virus,”) to false information about its origin and spread (for example, “5G networks are related to the spread of the virus,”) and false information about its severity (“Coronavirus is just like a normal cold, or a mild flu”). It’s hard to say which of these causes the most harm, but all are potentially dangerous to people’s health and safety.Unlike other attempts at AI detection of misinformation, our solution considers multiple pieces of context to determine if information is genuine. It doesn’t just look at the content of a tweet, but also information about the user who posted it, for example – and finds the right balance with which to weigh those inputs.It’s semi-supervised in that it can leverage both labeled and unlabeled data; it learns the semantics and meaning from unlabeled data.Why end-to-end? Because it also keeps up with changing information and emerging misinformation trends by leveraging external knowledge (from both reliable and unreliable sources). And finally, it’s explainable – it can tell you why it thinks a particular post contains misinformation.Our approach uses linguistic analysis of the message content itself, such as the terms in the post, incongruity and sarcasm, the sentiment expressed, and so on. But it can also look at the background of the user, the social network context, number of reposts, etc., to find the tweet’s virality – false and sensational viral posts tend to spread faster and wider. It also incorporates automated checking of the topic and claims against fact-checking sites such as Snopes in real-time. Being able to identify individual claims within a larger piece of content helps catch misinformation embedded in otherwise innocuous material – something that other approaches often miss. None of these approaches may be effective while applied individually, but they can be quite powerful when applied together.Just how powerful? To test our approach, we began by developing a dataset of publicly available tweets. The dataset is a mix of both labeled and unlabeled posts – more than 45,000 labeled tweets, about 60% of which contained misinformation, and more than 21 million additional unlabeled COVID-related tweets We compared the accuracy of the model on this dataset with seven state-of-the-art models for detection of misinformation, and it outperformed them all by at least 9%. We’re in the process of doing additional testing on other published datasets.This is an early effort, but with promising results, especially since our AI can quickly respond to emerging events that generate more misinformation. It could be easily incorporated into workstreams to assist human moderators, identifying possible misinformation with supporting information. This would not only make it easier for moderators to find and remove misinformation, but potentially respond to those who inadvertently shared the content with links to reliable sources. Over time, this could reduce the amount of misinformation that’s inadvertently shared in the first place.We are working to make the system scalable and capable of tackling a wide range of topics, pandemic related or otherwise. Stay tuned to learn more about our efforts!The authors would like to acknowledge Professor Tanmoy Chakraborty of Indraprastha Institute of Information Technology Delhi for his collaboration on this research. For more information about our work in this space, contact Shubhashis Sengupta."
Why AI’s fairness hinges on who develops it,"Using algorithms for decision-making offers huge benefits. From reducing drug discovery times, to speeding up and increasing the accuracy of medical diagnoses, to helping to feed the world by boosting agricultural productivity, algorithms contribute to progress in many different spaces.And using an algorithm to make a decision takes emotion out of the equation, giving a more fair, unbiased decision – right? Well, maybe not. In practice, we’ve seen this isn’t always the case.Résumé-scanning algorithms have overlooked female candidates for technical positions. Medical imaging diagnostic systems have failed for dark-skinned patients. Criminal sentencing algorithms have discriminated against black defendants. The list goes on.There are many reasons algorithms can make biased decisions. Even a perfectly “fair” algorithm can lead to biased decisions if it’s used incorrectly in a real-world environment. In that case, it’s a lack of transparency around the way the system is used that often causes problems. But I want to focus on here on bias that’s encoded during development – for example, if the data used to train an algorithm in the first place is biased.Without concerted effort in the model development process, bias can be encoded into all of the decisions that the resulting algorithm makes. Humans make biased decisions all of the time, of course. But automated algorithmic decision-making can be scaled to impact huge populations at the click of a button. That makes the risk of encoding bias into an algorithm much worse. And with algorithms being used to make more decisions in more aspects of our daily lives, we need to identify, quantify and address potential bias at all stages of the model life cycle. I’ve been leading a key Accenture Labs effort in this space in collaboration with our global Responsible AI practice and the Dock, our global innovation center. We’ve developed tools that let data scientists quantitatively assess for fairness across the end-to-end model life cycle. This includes state-of-the-art black-box AI models. Many of the algorithms developed before it became the norm to check for bias fall into this category. Those models are already in use and making critical decisions today.Accenture’s Responsible AI practice conducted a week-long hackathon with the Alan Turing Institute in 2018, aimed at translating existing qualitative definitions of fairness into quantitative measures. Building on those efforts, we developed a proof of concept for an algorithmic fairness tool – one that presents its results in a way that is understandable to both data scientists and business users. We tested and refined the tool in a pilot with industry experts at a financial services company.The algorithmic fairness tool works on real-world problems, taking this academic thinking from a proof of concept into everyday life. Although we initially applied it to financial services, we are now applying it to a number of industry use cases including health, public service and HR. To make the tool relevant for real-world problems, we had to reexamine a number of assumptions. Many assumptions make sense when using small, clean datasets and straightforward scenarios, but don’t work for complex data in real-life settings. Technical solutions can address many of these, alongside governance and standards to guide the data scientists and business users. Interpreting the outcomes of fairness metrics isn’t always straightforward – we almost always need a deep dive to determine the root cause of potential bias. Working on the technical side and having the right tools, governance and processes to assess and address fairness is both important and fascinating to me. But just as important to address bias is tackling the lack of diversity in the artificial intelligence field.Women and ethnic minorities remain underrepresented in computer science both in university and in the workforce. This lack of diversity leads to one of the most difficult-to-combat types of bias: unconscious bias. Diversity among experts helps ensure that the potential impacts (and in this case, biases) of a technology are considered from multiple perspectives from the start. Without that diversity, the chance for bias increases sharply.And this kind of bias can lead to life or death situations. For example, we’ve seen instances of facial recognition systems not recognizing black women as humans due to training data being mainly from white males. This would have serious implications in a self-driving car looking to determine if there is anyone in its path. Unfortunately this isn’t a new problem in tech: 20 years ago, due to crash test dummies being based on mainly on the adult male body, women (and children) were much more likely to die in car crashes.This long-standing problem is much harder to address than the technical one. It takes concerted effort over time from a wide range of stakeholders across society, including industry, government, and educators.  But we can and must address it, and we need efforts on multiple fronts.Last year I co-sponsored the Accenture’s six week Women in Data Sci Accelerator in Dublin, which is now an annual event being scaled across other countries. The goal of the Accelerator is to empower women already in careers working with data with the skills required to be a data scientist.There is also a longer-term effort of working with the younger generation. Only about a quarter of computer science undergraduates are female. Unless we address this imbalance, it will be very difficult to address the continued imbalance in the workforce. So, I focus most of my attention in getting more girls and young women interested in the options out there for a career in STEM.I work with third level institutions to shape new data science and AI courses. I also help them think about how to make these courses, and the way they are advertised, more attractive to females – sometimes by tackling the already ingrained unconscious bias that often already exists at this stage.I’ve developed efforts for second level students as well. Initiatives that I’ve rolled out here include a girls-only work experience week to learn about STEM, and also sponsoring Junior Math Achievement weekend classes for about 100 11-15 year-olds, aimed at giving children who are good at mathematics a place to excel and have fun at the subject. The sponsorship is contingent on ensuring good representation across both males and females and also across disadvantaged and privileged schools.And finally, because all the research shows “You can’t be what you can’t see,” I make myself get out there and speak on these topics (getting myself out of my comfort zone!) to give younger women a role model.Fairness in algorithmic decision making is of growing importance. But making sure that it becomes the norm depends on both technical and societal solutions. The technical piece has to be addressed by the people already in place with expertise, but the second part – ensuring diversity in those groups of experts going forward – is something we all have an obligation, and an opportunity to make happen.The efforts I’ve been part of to date do make a difference, but we can and must do more. By making sure that younger generations of all backgrounds, ethnicities and gender get the opportunities and feel empowered to pursue careers in STEM and specifically AI, we can help apply this technology to its full potential for everyone.To learn more about our efforts in algorithmic fairness or get involved with our efforts to increase diversity in STEM and AI research, contact Medb Corcoran."
