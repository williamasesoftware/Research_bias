Article title,Article text
The benefits of code visualization and how to leverage it with Augoor,"Achieving code structure understanding is critical for modern software development teams that want to improve quality, reduce time to market, and easily collaborate. To help developers make their code readable and maintainable, the AI-powered Code Intelligence platform Augoor now introduces Codemap. This feature enables visual representations of codebases to better understand their behavior and relationships.Code visualization tools todayAlthough tools like Find all references and debuggers can assist devs in exploring individual lines of code or switching between files, they cannot provide a comprehensive view of the entire codebase and its dependencies. And without a dependable overview of the code, development could become slower, and more difficult to control.Its easy to lose track of the overall structure when working with complex architectures. Code visualization tools act as a bridge between the perception of the system and its actual representation. They enable code comprehension and facilitate organizing and collaborating with confidence. Even if teams have clarity of their future plans and a method for managing files, theoretical documents and a few folders dont provide a picture of the codes current state. So visualization helps to optimize the coding process, developing structured, efficient, and effective code.Technologies like abstract syntax trees, data visualization libraries, code analysis tools, graph databases, and augmented reality enable developers to create graphical or visual representations of code. The innovation behind code visualization is constantly evolving to meet the needs of modern software development.Augoor introduces CodemapDid you know developers spend 70 to 90% of their time reading code instead of writing it? Augoors AI-powered Code Intelligence platform helps to invert that percentage, boosting dev teams output like never before. By enabling faster and better code exploration, analysis, understanding, and documentation, it unlocks dev teams potential, accelerating companies go-to-market without jeopardizing quality. Augoor is now introducing Codemap, easy-to-understand code graphs of repositories. With Codemap, technical leads and software architects can quickly glance from an eagle point of view, the organization, plus source code repositories activity and health. Our objective is to help identify, compare, understand, and track projects evolution, interaction, and efficiency through time, says Benjamin Prieto, Technical Director at Augoor.This feature enables graphs that automatically reveal valuable information about repositories to identify how components are classified, their properties, and their interactions within complex architectures. Using hierarchical levels of representation, circle-packed graphs, and code analysis tools such as SonarQube plugins, Codemap provides a complete analysis of the codebases health status and complexity.With Augoors Codemap feature, users can have a global visualization and quick understanding of the code structure that makes up the product, as well as its level of complexity, possible risks, bottlenecks, and overall health. It allows users to understand the context of any product quickly, locate the source and determine the cause of problems at specific and traceable points in time. It also enables delegating the responsibility of solving these issues. Therefore, this code visualization feature allows companies to accelerate the coding process and better control product quality.To learn more about Augoor, visit augoor.ai and keep up with updates on LinkedIn | Twitter | YouTube."
The role of virtual screening in drug discovery,"How can scientists streamline drug discovery, a process that takes up to 15 years and costs around $2.6 billion? The convergence of biology, chemistry, and technology is changing the landscape for pharmaceutical companies. In the last decade, AI has stepped into the field, providing cost-efficiency and greater speed and precision to developing medicine.Pharmaceutical companies are investing in AI, including Pfizer, one of the most prominent players during the outbreak of COVID-19. With multiple benefits in the biotech sector, the global use of AI in the drug discovery market is expected to account for USD 24,618.25 million by 2029.AI can deliver value in drug discovery by identifying new therapeutic targets (biological molecules, typically proteins, associated with disease processes). This blog focuses on the role of virtual screening in drug discovery, a process powered by AI.How do scientists find drug candidates?The process of finding drug candidates consist of two main steps:Discover a therapeutic target: a biological molecule or a biomolecule that plays a critical role in the disease process at the cellular level.Identify molecules that might serve as drug candidates. Scientists achieve this through high-throughput screening (HTS), a process of automated testing of many chemical compounds to discern their biological activity and, thereby, a therapeutic potential based on the effect they exhibit in the test.Even though the HTS technique has significantly improved, it can be costly and time-consuming. Traditionally, HTS has access to large and diverse chemical libraries; screening millions of molecules can take months.Virtual screening in actionScientists can streamline the HTS process with the use of virtual screening. This approach involves screening chemical compounds in silico utilizing computer simulation or modeling. It enables the identification of small molecules (ligands) that may bind to a therapeutic target and modulate its function, thus correcting its detrimental role in disease development.For virtual screening to succeed, we must know the biological targets three-dimensional (3D) structure in as much detail as possible. Recent advances in the AI tool AlphaFold, developed by DeepMind (a British subsidiary of Alphabet, Inc.), have enabled the prediction of the 3D structure of nearly all proteins found in living organisms with a high level of accuracy.Dive deeper into AI and pharmaVirtual screening offers a fast and efficient way to identify new drug candidates and design new chemical compounds that could enable novel treatments. Learn more about the challenges and advances in virtual screening in our white paper."
Tapping the power of PSP data,"Enable insight-driven care with Globants Patient Journey Accelerator and TableauPatient Support Programs (PSPs)  a term used to describe any initiative led by pharmaceutical organizationsto improve the patient journey, access, adherence, or outcomes related to a given treatment  have come to play an essential role in enhancing patient care.But, until now, data from PSPs have remained with the manufacturer, which means that other stakeholders within the patient journey, such as insurers, health care practitioners, and even patients, have not been able to access the program data that can help them better use data-driven insights to understand and assess the effectiveness of a given treatment or how a PSP influences outcomes.As the healthcare industry faces mounting pressure to find new ways to improve care for patients and drive positive outcomes, PSPs can be one way to enhance coordination and collaboration between different stakeholder groups to enhance the patient experience and increase results.A closer look at the value  and challenges  of PSP dataPSPs collect a wealth of patient-level data, such as personal characteristics, drug utilization, and key clinical outcomes. This information could be of great use to other stakeholders in the patient lifecycle.However, providing clear and compelling insights to various stakeholders requires additional capabilities for the PSP, including complex analysis of data at different moments of the patients journey, a safe and secure data model that enables sharing in a way that maintains patient privacy and adheres to regulations, and the ability to continuously gather, update and disseminate data.Further, drawing accurate insights from PSP data requires a deep and comprehensive understanding of the specific data fields captured, the purpose of data collection, and any existing gaps. Without this context, there is a risk that the data can be misinterpreted, leading to false conclusions that decrease the scientific validity of the results.Unlocking the power of PSP data with GlobantTo help pharmaceutical organizations safely and securely share insights from PSPs with other stakeholder groups, Globant recently launched the Patient Journey Accelerator in the Tableau Exchange.The Accelerator enables the PSP organizer and other stakeholders to analyze patients progress throughout the programs different stages and track key indicators, such as the conversion of referred patients, time to treatment, and concentration of patients in hospitals, clinics, and geographical points.For providers, the Accelerator produces data-driven insights into patients journeys in their system across different specialties, geographies, and diagnoses. In addition, care program managers can use this Accelerator to analyze referrals and better align the necessary resources to effectively treat patients.Patient Journey Executive SummaryPatient Journey Active Patients ConsolePatient Journey Referrals ConsoleUsers: Managers and care program coordinatorsUse case: Understand the current state of patient care in their organizationUser: Care program coordinatorsUse case: Understand the behavior of active patients according to their care plan, medical institution, or other data pointsUser: Care program coordinatorsUse case: Understand the behavior of referred patient by referral date, care plan, doctor, or other data pointsUnleashing PSP data with the Patient Journey AcceleratorTo further guide the use of this Accelerator, Globant has created a framework for implementing and adopting the Patient Journey Accelerator that identifies sensitive points throughout the patient journey. These points include diagnosis and referral, patient enrollment, financial help, and treatment monitoring.The figure below depicts a process that considers the different components of a PSP operational flow within the context of these sensitive phases. The process includes the security of data, integration of several data sources, and a methodological approach to ensure the PSP is able to generate insights that help improve the patient journey.Figure 1: patient journey within psp.As an analytical tool, the Patient Journey Accelerator is focused on answering high-impact strategic questions and improving the experience of HCPs, program support specialists, financial support teams, and, of course, the patient. Drawing on our extensive experience in both the healthcare sector and data and analytics, this new accelerator is based on five distinct elements:The identification of the phases in the JourneyThe identification of strategic questions in each phaseThe discovery of data sources that can help answer the strategic questionsThe indicators also help to support the answer to the questionThe predictive elements help the organization make more proactive decisionsFor example, the analytic capabilities provided by the Accelerator can help stakeholders answer strategic questions focused on specific parts of the journey. These include:What are the factors that the HCP takes into account when enrolling a patient?Is the PSP exposed to legal risks derived from mandatory information in enrollment forms?Do we have enough capacity to attend to the patient pipeline in the program?What factors affect the enrollment process not being completed?How does the type of coverage affect future adherence and churn to treatment?What factors influence the early start of treatments?In addition, the extension of the accelerator enables new analysis components, such as:The retention of patients in different time horizons.The evolution in treatment administration and changes in medications over time.Pharmacy fill monitoring (Medication placement in pharmacies and demand analysis).Monitoring and analysis of medical orders and patient consent.Follow-up of reimbursements and analysis of resources and coverage concentration.Take your PSP data to the next level with GlobantThe ability to draw accurate, relevant, and timely insights from PSPs can drive valuable improvements in patient care and coordination with the broader stakeholder group. Globants Salesforce, Analytics, and Healthcare & Life Sciences Studios are proud to launch the Patient Journey Accelerator in Tableau and help PSP organizers and the extended healthcare ecosystem generate compelling and informative patient journey insights to take patient care to the next level.To learn more about the Patient Journey Accelerator and our Tableau capabilities, please contact us to schedule a consultation today."
Generative AI and the dilemma of our future,"Success is not final, failure is not fatal: It is the courage to continue that counts, said Winston Churchill. Over the last decade at Globant, we have been researching and developing technologies around artificial intelligence (AI). We have seen it progress with typical quasi-linear growth, sustained and slow, a common characteristic of the beginning of exponential curves. In recent months, this growth has entered the most accelerated part, surprising everyone and showing us a new way computers can interact with humans.Today, new applications based on large language models (LLMs) such as Open AIs ChatGPT, Googles Bard, or GitHub Co-Pilot allow us to maintain an intelligent dialogue with them and see how they generate text and images in an almost human-like way. However, this brutal evolution comes hand in hand with new philosophical questions. For example, Yuval Harari, Tristan Harris, and Elon Musk, among other leaders, recently raised concerns through a public letter expressing the need to rethink the future of AI.AI as a complement to humanityOne of the hottest discussions today is: Can AI replace jobs, and especially, as a novelty, knowledge-based jobs? A few days ago, OpenAI published a paper on this topic. What is happening now could resemble other technological changes that occurred in the past: the industrial revolution, where the machine replaced force, or, less impactful but more illustrative, what happened with the replacement of slide rules with calculators. In both cases, significant technological evolutions did not impact the importance of the work of engineers but improved their capabilities to carry out their tasks.AI will accelerate the work of programmers, writers, and other creative jobs, but it still cannot replace them. Its not for lack of capacity but because these systems lack information about the context we humans have. They cannot know what is happening in our community, work, or society, so it is impossible for them to generate the perfect text or code we were looking for at that precise moment.There is another reason. These models have a great capacity to create dialogues and texts, but they do not have the ability to execute. Let me give you a simple example: Bard can write texts for an advertising campaign, but it is less likely that it can publish these texts in a medium and make a decision about the campaign. Not only that, but it would also have no responsibility for what it writes. These models can invent data or create incorrect information since they are neural networks that aim to make a semantically meaningful and coherent text but are not necessarily true. The lack of final responsibility for the information is undoubtedly a significant issue to solve, and it is a very big difference with humans. The fact that people trust these responses will be a major issue to be resolved since it is not easy to know the origin of the information that led to that response.In short, we must imagine artificial intelligence as a personalized assistant that helps us improve our productivity. And together with this increase in productivity, there will be (as always in history) an increase in our ambition to build more things in less time. And now it will be possible.Welcome to the real conversationFollowing the massive landing of generative AI tools, we have seen a new way of interacting with machines, more conversational and less transactional. Today we can ask our favorite brands for a different, more human, more emotional, more aware interaction of each customers needs. And this applies to all companies, from telecommunications to banks, passing through the entertainment industry, travel, pharmaceuticals, and even industrial control companies. The dream of hyper-personalization is knocking at the door.The LLMs are even challenging something that seemed impossible: how we search for information online. Successful models like Googles are undergoing revision to find new ways of accessing information and creating texts without the need to navigate through thousands of links. Since ChatGPT was made available to the public, the battle for leadership among new ways of delivering search results has accelerated between major players like Microsoft and Google.In this context, organizations are increasingly called upon to create AI applications and conversational interfaces combining large language models with their proprietary information. Channels like WhatsApp, for example, will become increasingly relevant.The world beyond AISince the beginning of my career, I have witnessed the advancement of technology, from the arrival of the internet and social networks in the 90s and early 2000s, to web 3.0, the Metaverse, and artificial intelligence. All of these innovations have revolutionized the way we interact as humans. We can now communicate and connect with people worldwide more efficiently and easily. Web 3.0 has allowed us to carry out direct and reliable transactions, eliminating intermediaries, and the metaverse will give us a 3D digital space that could significantly impact how we work, learn, and entertain ourselves in the future. As I have mentioned, artificial intelligence has also progressed exponentially. And we will surely see even more significant disruptions, such as energy production through nuclear fusion or the massification of quantum computing, which will change security and processing speed paradigms. Technological development is truly in its exponential phase.In the ever-expanding world of technology, humans will always remain at the center, with our abilities and flaws. One day, ChatGPT will be obsolete, replaced by a much larger and interconnected model. When that day comes, we will revisit what we think and what we know.Click here to learn more about how Globant is thinking about the power of AI."
Einstein GPT: What Salesforces generative AI tool means for you,"4 steps you can take today to prepare your organization for Einstein GPTChatGPT has dominated news cycles since its launch in late 2022  and now Salesforce is getting in on the action. Earlier this month, the company announced Einstein GPT, a new generative AI tool to help companies connect with their customers in more intelligent, automated, and personalized ways. But what is Einstein GPT exactly, and what does it mean for your business?In this post, our Salesforce Studio and Data & AI Studio leaders break down the latest Salesforce announcement and everything you need to know about Einstein GPT.What is Einstein GPT?Einstein GPT is the next iteration of Einstein, Salesforces integrated set of AI technologies for its CRM platform. Einstein GPT leverages OpenAIs ChatGPT technology and Salesforces proprietary AI models to deliver AI-generated content across the Salesforce ecosystem, including the Sales, Service, Marketing, and Commerce Clouds, Slack, and the Developers Portal.What are some examples of content Einstein GPT can produce?Einstein GPT promises to help Salesforce customers automate all forms of content creation. This could include:Personalized sales emailsResponses and replies to customer service requestsPersonalized campaign content, such as ads, web copy, emails, and notificationsSupport for developers in writing, searching, and commenting on codeIn addition, the new ChatGPT for Slack integration gives teams the option to summarize conversations, research a variety of topics and automatically draft messages and responses.How does Einstein GPT work?Data and AI are at the heart of Einstein GPT. Heres how it works:Salesforce Data Cloud ingests, harmonizes, and unifies the companys customer data.That data and generative AI technology from the Salesforce partner ecosystem are fed into Salesforces proprietary AI models.Einstein GPT will be trained with the companys customer data in Salesforce, ensuring security and privacy principles.Natural-language prompts can then be generated directly within Salesforce to create content.Content can be continuously updated and adapted in real-time based on changes to the customer data set.Human intervention will still be required to oversee what the AI has generated  as Salesforce stated, AI is here to help us, not replace us.Because data is the foundation of Einstein and Einstein GPT, it is imperative that companies have a clear data strategy and comprehensive data capabilities to inform the AI models that will power Einstein GPT. Organizations need to properly prepare their data set and maintain it to use these tools to their fullest potential and generate content that is relevant, accurate, meaningful, and timely.Need help with your data strategy? Globant can help. Learn more about our Data & AI Studio.How can I use Einstein GPT today?As of publication, Einstein GPT is currently in a closed pilot. Availability for all users is expected later this year.That said, there are steps organizations can take now to prepare to make use of Einstein GPT or seize the opportunity of forms of generative AI:Prepare your data. Data is the engine behind Einstein GPT. Organizations that want to use this new tool should ensure their data is ready to go when Einstein GPT is made available to all users. This may include: Ensuring all data is clean, timely, and relevant; Fine-tuning data collection, management, and governance structures; and defining the enterprise data architecture. If companies are not already leveraging Salesforce Data Cloud (formerly Salesforce CDP), now is the time to start since these tools are essential to achieving a 360-degree customer view and enhanced CX.Conduct a Salesforce assessment. Einstein GPT is an exciting addition to the Salesforce ecosystem. However, many features and functionalities on the platform today can help your teams automate tasks, improve workflows, and drive efficiency. Undergoing a Salesforce assessment with a Salesforce partner is one way to ensure your company is maximizing its current investment and ready to deploy Einstein GPT when it becomes available.Experiment with AI. Organizations eager to reap AIs benefits  including generative AI  need not wait to begin using this technology. There are countless tools and solutions available on the market today that can meet a variety of use cases  from automating content creation to developing code and everything in between. Start with a small-scale pilot to better understand not just how generative AI technology works but also the other aspects of your organization that will need to evolve to use it at scale.Build on your own! If your team is familiar with building Salesforce Lightning components and coding within the platform, you can make those components even smarter using the ChatGPT API. For example, the Globant Salesforce Studio team built our own solution within Marketing Cloud to auto-generate email responses, propose the Next Best Action, summarize long conversations, and more. While organizations prepare for the full launch of Einstein GPT to all Salesforce users, building a personalized ChatGPT solution to meet your specific needs today is possible.On your marks. Get set. GPT! Getting started with Globants Salesforce Studio and Data & AI StudioExciting as the generative AI landscape is, researching the market and identifying the ideal partner can be complex and time-consuming. But forward-thinking companies cannot let complexity stand in the way of using emerging technologies to achieve a competitive advantage.As a digital native and pioneer in artificial intelligence, process optimization, automation, and BPM, Globant is here to help. Our robust Studio model combines expertise from across our Salesforce, Data & AI, and Industry Reinvention Studios to offer clients a comprehensive set of tools, apps, accelerators, and capabilities to help them craft a customized solution and technology strategy based on their unique needs, existing investments, and future goals. Were the team that can help you seize the opportunity of generative AI  whether that means preparing the organization for the full launch of Einstein GPT, identifying a generative AI application to meet your current use cases, or building your own custom solution.Ready to learn more about how your organization can use Einstein GPT and prepare for its official launch to all users? Contact Globant to schedule a consultation with leaders of our Salesforce Studio and Data & AI Studio today to learn more about generative AI and what it means for your business."
ChatGPT: Using the right tool for the right job,"Despite the momentous hype surrounding ChatGPT, it may prove to be justified for developers at least. At the speed at which new frameworks, platforms, and even languages are popping up, being able to keep up is becoming a truly daunting task.ChatGPT could be the cure for this overwhelming situation. With ChatGPT as my guide, Im picking up Python faster than Ive ever learned anything in my life.I set out to rely primarily on ChatGPT but did use additional documentation to verify its guidance (1). One of the difficult things in getting started is that we simply do not know what we dont know (yet). So, I began with the simple prompt: what core principles and concepts do I need to learn to master Python?Within two weeks, I went from having zero knowledge of Python, let alone ChatGPT, to leveraging it to create a basic web app using Flask and Bootstrap (2). Taking weather reports from a public API, passing them through OpenAIs text-DaVinci-003 model, which returns the reports in remarkably human-sounding prose.While this modern version of the hello world app is simplistic, it nicely exhibits the core skillset for building solutions with API (3) integrations. It also serves as a familiar backdrop to the almost surreal experience of learning with ChatGPT for the first time.The support it provided, for lack of a better term, was astounding. I peppered it with questions and nagged it for advice. At every stage, it was there. Explaining what my server and framework options were, showing me how to arrange my folders and files for a typical Flask setup, responding to my doubts, expounding on topics I wasnt clear on, and finally providing highly useful code samples.As youll see later, it wasnt without its hiccups, but overall, Im sold. I expect this tool (and others like it) to become the de facto way I work going forward.Where ChatGPT exceeded my expectationsChatGPTs ability to provide code is probably its best-known feature among developers, but what really stands out is its ability to respond to follow-up questions. As simple (or silly) as that sounds, how many times have you been reading something and found you had questions that couldnt be readily answered by the text, let alone where to start looking for clarification in the documentation? I have spent far too many hours in my career doing this sort of follow-up investigation.With ChatGPT coming to the market, that essentially ended. The fact that I can simply ask: Do you mean x or y when talking about z? has already saved me a surprising amount of time. Being able to ask a chain of related questions is transformational when it comes to learning. Weve entered the age of living documentation. Like having a mentor and a massive research team all rolled into one at your disposal.Needs to improveWhile my experience has been incredible, it is important to note this journey hasnt always been smooth sailing. One factor is ChatGPTs rapidly expanding popularity. During this process, I suddenly began to run into at capacity messages. I had to switch to a paid account to ensure I retained access. But this highlights a critical point, to make this a part of your regular workflow, youll need a paid account. They recently introduced ChatGPT Plus for $20 a month, which, in my mind, is worth every penny.Dont use ChatGPT  for codingOne area where ChatGPT struggled was producing longer coding samples. When solutions exceeded 50 lines, it almost always errored out. Maybe it was the load it was under or OpenAI explicit policies, but as Ive since learned, I shouldnt have asked ChatGPT for complex coding solutions. Thats where OpenAIs code-DaVinci-002 model comes in. ChatGPT was trained on a vast amount of textual information to give natural language responses. However, OpenAIs documentation makes the difference clear:The Codex model series is a descendant of our GPT-3 series thats been trained on both natural language and billions of lines of code. (Emphasis mine) (4)ChatGPT is excellent for short explanatory code samples that help you understand concepts or syntax youre unfamiliar with. But for day-to-day coding, well need to get up to speed on using the codex models. They do require more know-how, but with that comes a much finer degree of control and a deeper reservoir of coding knowledge. (5)The key to success: Prompt EngineeringTo take AI tools like ChatGPT to the next level, youll need to hone your skills in prompt engineering. The ability to carefully craft what you ask of them will be what differentiates average users from those who can fully wield their power. Not to fear, OpenAI has provided a ton of examples on their site to get you started. You can even sharpen your skills in a playground theyve set up, which provides more nuanced control, such as the responses temperature. A measure of how creative the AI will be when generating its responses. Using the codex models in the playground, where I can adjust this value, would have prevented the problems I ran into with ChatGPTs coding samples.Final thoughtsThis has been a fantastic and eye-opening experience so far and Im really looking forward to seeing how far ChatGPT and other AI tools like it can be pushed. We are at an inflection point in our profession and in learning in general. To stay relevant, we will need to reinvent ourselves and our approach to virtually every aspect of our job. Thankfully, ChatGPT will be there to guide the way.(1) Automate the Boring Stuff with Python, 2nd Edition: Practical Programming for Total Beginners 2nd Edition10 Visual Studio Code extensions for Python developmentOpenAI OverviewAdvanced ChatGPT Guide  How to build your own Chat GPT SiteGetting Started with GitHub AutoPilotFor Globant Employees: Globant U chatGPT TrainingFlask QuickStart(2) Python Documentation(3) Open Weather API(4) OpenAIs introduction to Codex models(5) Example of codex model prompting in OpenAIs playgroundSome of the codexs abilities OpenAI touts: Turn comments into code, complete your next line or function in context, bring knowledge to you, such as finding a useful library or API call for an application, add comments, rewrite code for efficiency"
GeneXus Next: Leveraging AI assistants to reinvent software development,"Globant has been at the forefront of the AI movement for over ten years, heavily investing in research and development with projects like MagnifAI, Augoor, and Navigate. The company has always kept an eye on opportunities to strengthen its capabilities in this specialization and the process of software development, so it acquired 30 years of AI assistant research expertise as part of the GeneXus acquisition. It was all part of a grander vision.GeneXus, on the other hand, has had a mantra for its users since its beginnings: Describe what you need and let GeneXus AI create the code and databases.With the advent of ChatGPT and LLMs, Globant and GeneXus envisioned taking the describe an application approach to a new level by merging emerging AI technology with GeneXuss proven AI-powered platform. The result is GeneXus Next, a combination of different AI assistants and powerful GeneXus technology that simplifies the creation, delivery, and maintenance of enterprise software solutions in record time.Bridging the understanding gapThrough deterministic symbolic AI with LLM capabilities, GeneXus Next levels the playing field. Routinely, the software creation process involves many different profiles of professionals, each using their own programming languages, and unique interactions: from designers to developers to testers. By allowing standardized inputs, no matter what discipline or coding language someone is knowledgeable in, they can create valuable outputs by leveraging the tool.You describe, GeneXus builds.GeneXus Next allows using natural language as common input for each profile. In essence, you can use natural language requirements and prompts to create complex business workflows or for data modeling, use sketched images to generate fully interactive UIs, or use audio instructions to evolve the system functionality. Simply put, if you can describe it, GeneXus can build it.Why AI?The demand for AI is growing rapidly, especially with consumer-facing generative AI products, such as in-market Chatgpt and Googles Bard, on the horizon. Recognizing the seismic shift in how information is gathered, how things are built, and the overall capabilities of AI, we leveraged our existing AI capabilities and combined them with our expertise in emerging technology to create a powerful tool that continues to push the boundaries of software development.Globant has been at the forefront of AI technology for over ten years, providing sustainable value to our clients, creating best-in-class solutions from our in-house experts, and advising on third-party AI platforms.Coming soon to GeneXus NextGeneXus and Globant are always evolving, as will the GeneXus Next product. In fact, there are even more improvements to our AI model in the works. The next iterations of the AI functionality will include:App generation. Ask the AI Assistant to create a draft of custom Design Systems on the GeneXus Panel. The AI will take care of it for you, automatically generate all the tokens and styles, and apply them to your project. The code will be generated and deployed automatically.Business process optimization. Ask our AI Assistant for suggestions of business processes so you can initiate them based on Standard BPM Notation. This workflow can be automatically deployed, generating the web and mobile inboxes ready to run.Image generation. Request the AI for quick image generation during development to improve prototyping time.Application panel modeling. When you need a new screen for your project, simply jot it down, describe it to the AI Assistant, and watch it deliver it along with the appropriate data you need to manage.Transaction modeling. Based on a simple description, the AI Assistant can model the Data Structure that GeneXus will later use to create the Data Models and Data Bases you need.AI technology has been a key part of Globants value proposition for many years and a factor in how we revolutionize how companies reinvent themselves; software development plays a big role. In addition to GeneXus, we are revolutionizing the industry with Augoor, which applies AI to make the coding process faster, more creative, and more effective.GeneXus Next is revolutionizing the way software development is conducted, click here to learn more. As part of our broad technology and innovation expertise, Globant is skilled in creating AI solutions that impact businesses globally. Find out how were helping organizations adopt an AI mindset here."
What is next for technology? Top Tech Trends in 2023,"How can companies and industries excel in the era of technological disruption?Every year, Globant launches a report with the most relevant trends in the world of technology. Trends that enable industries and organizations to navigate market demands and reinvent themselves through innovation.This year, our Tech Trends Report explores four trends that will help companies navigate the waters of a new technology era.What is next for technology?Discovering embedded AI in the every dayAI is so ubiquitous that people might not realize how it already plays a role in their everyday roles: more than 4 billion devices already work on AI-powered voice assistants, and 40% of people use the voice search function at least once daily. Some of AIs most common use cases are product recommendations, automated financial investing, and digital assistants. Successful companies will bare in mind the publics usage of AI.Metaverse burns brighter in 2023 or burns outAfter all that has been said about the metaverse, people expect to see it in action.One field where the metaverse has promised to change the game is customer experience. People are not just users anymore; now, they are players interacting with brands and products in a 3D shared space. Companies harnessing the metaverse successfully will create immersive experiences with a cohesive strategy across channels.Organizations are also leveraging the potential of the metaverse when it comes to training and education. Personalization, gamification, and new educational environments are changing how we learn. Unsurprisingly, education is the second leading sector investing in the metaverse, after IT.Brands are exploring blockchain-based experiences to access younger generationsBlockchain has existed for some time but is now fundamental for providing phygital experiences. As the Web 3 ecosystem grows, businesses and brands increasingly find ways to leverage decentralization while connecting the physical and digital worlds.Use cases of NFTs in 2023 will go beyond digital art. Non-Fungible Tokens will capture the attention of younger generations while providing new engagement models for older generations. NFTs trends in 2023 include ticketing, virtual events, DeFi, luxury goods, and music.The global crisis raises a new hero: smart efficiencyHow can companies keep pushing forward their reinvention and digital transformation?Low-code and no-code approaches are the new paradigms of software development. With visual editors, low codefacilitates workflows and accelerates application delivery.Likewise, approaches like Super Apps will provide more personalized user experiences and allow companies to diversify their audience. By 2027, more than 50% of the global population will be active users of multiple Super Apps daily.Tangible technology for consumers and organizationsTo dig deeper into how emerging technologies impact businesses and the day-to-day life of customers and employees, Globants experts got together in a live webinar, Trends Applied: How AI, metaverse, blockchain, and low code will revolutionize business and our work.JJ Lpez Murphy, Head of Artificial Intelligence, Camren Daly, Technical Director, and Isa Gosku  CTO, UK, discussed how top organizations tackle new technology challenges and how you can leverage top trends to navigate a new technology ecosystem.Learn how to leverage the latest trends in the tech sector and discover what our experts say about the reinvention era. Read our Tech Trends Report 2023 and watch our webinar today."
Autonomous supply chain via AI: Understanding main areas for improvement and use cases,"With over 300 million in potential value for artificial intelligence in the supply chain management (SCM) field, organizations globally seek to leverage the technology to improve processes and realize cost savings throughout their SCM systems.Main areas in an SCM system where AI can reduce costs and improve outputsAs mentioned in our recent blog The power of AI to enable autonomous supply chain, there are three crucial areas for organizations to focus on when it comes to autonomous supply chain: demand forecasting, logistics network & warehouse optimization, and sourcing & procurement.Diverse results and potential value vary by industry, company, and use case. They are estimated as follows:Demand forecasting: Predicting demand across multiple product segments and geographies, as well as ensuring that plans get executed and can adapt to variability effects (such as demand shocks, production stoppages, and transportation disruption) promptly. The potential savings are generally well over 10% of supply chain operational costs and over 20% reduction in supply chain working capital. Typical impacts (non-exhaustive) are spread all over the supply chain. However, increasing forecasting accuracy can reduce average days of inventory and lead times while increasing promotion and marketing effectiveness and supplier compliance rate.Inventory management and optimization: AI can enable companies to balance trade-offs across functions and decrease operational and inventory costs by linking together multiple areas, including logistics, production, procurement, and marketing and sales. Potential savings typically range from a 10 to 50% reduction in working inventory capital and over 10% in operational inventory costs. Inventory turnover and warehousing needs can be materially reduced.Labor optimization: The time and effort required to schedule workers is a major pain point for many companies. AI-driven scheduling solutions can help alleviate some of these problems by providing a more efficient way of managing workforce data. By using machine learning algorithms, they can consider standard shifts and other relevant information about employees availability to create a more fair schedule. Overtime reduction, stable workload, and efficiency improvements are easily captured by AI.Route optimization for distribution: AI systems have been used to help optimize delivery routes for years, but they are typically limited to individual deliveries. In contrast, AI platforms can optimize an entire fleet of vehicleswhich is why they have become valuable to companies that rely on timely shipments. The algorithm can consider several variables, such as traffic, shipment data, and geographical and environmental information streams. Typically the average mileage will be reduced as well as the secondary logistics needs.For years, Globant has been advising clients on how to implement AI and assess potential impacts we see in the Supply Chain industry. Some of these client cases include:Table 1. Globants use cases for AI implementation in Supply ChainConclusionLeveraging artificial intelligence to boost supply chain capabilities allows companies to minimize costs while increasing the outputs of their systems. With years of experience analyzing, implementing, and evaluating supply chain systems, the Business Hacking Studio at Globant creates opportunities for organizations to build autonomous supply chains. Learn more about the Business Hacking studio here."
The power of AI: Enabling autonomous supply chain management,"What is supply chain management combined with artificial intelligence? The Artificial Intelligence (AI) annual potential value for Supply Chain Management (SCM) is estimated at over $300 billion, most of which is driven by traditional AI and analytics. If we break down this bold figure by component, three main areas explain the potential value:Demand forecasting is estimated to represent ~60% of the value potentialLogistics network & warehouse optimization are estimated to represent ~35% of the value potentialSourcing and procurement are estimated to represent ~5% of the value potentialAlthough the value to capture is certain, roughly half of the companies are investing in AI in some form or another, only 10% are investing in AI applications for SCM. Whats even more intriguing is that when looking at how the efforts are distributed across SCM, it seems that there is equal focus on demand forecasting and logistics network & warehouse optimization when, as aforementioned, there is more value at stake in demand forecasting.Hence, why are so many companies lagging in AI deployment? Are there any roadblocks we havent yet sorted out? What could potentially be done to solve this? In this article, we address these questions, outline the primary considerations a company needs to take for a successful AI implementation, summarize how to take full advantage of AI and automation in SCM, and detail some compelling applications and use cases that we believe are game-changers in the AI SCM industry.Why are companies failing to implement AI successfully?We have experienced while serving our clients, and validated with our industry experts, that somewhere between 60-80% of all AI projects are falling short of their original goal.Common mistakes include lack of or insufficient alignment among business & AI strategy, processes, data maturity, and infrastructure.Moreover, companies are confusing AI projects as application development or functionality-driven projects. They often perceive them as data projects or sometimes data products. To achieve the desired outcome, AI projects require a focus on data iteration and data-centric methods once the correct coding and algorithm have been implemented.Another common misconception is that the Agile methodology, which is a fast and non-linear approach to implementing a project, works for AI projects  especially projects related to SCM. Although Agile is an excellent methodology in most product development projects, it might fail when dealing with AI because it does not define how to deal with data, the fundamental asset of AI algorithms. The same happens with other methodologies, such as waterfall. A more appropriate approach for AI implementation is the Cognitive Project Management for AI (CPMAI), which blends data-centric approaches with agile methodologies to produce optimal methods for AI projects highly data-centric, variable nature.Successful AI implementation: What can be done to streamline efforts efficiently?At Globant, and especially for AI in Supply Chains, we tackle the problem from 3 angles: Change Management & Culture, Data & Enablement, Processes Standardization, and IT InfrastructureWhen it comes to SCM, the panorama can become more challenging due to the number of systems, uneven data distribution, and complex processes that must ensure collaboration across different teams for smooth operations. Therefore, our vision suggests an end-to-end Supply Chain approach where each of the three pillars mentioned before is evaluated at every step, empowering companies to achieve a competitive advantage through AI automation.Figure 1. Globants framework for AI AutomationFirst, AI implementation is not successful when the culture and change management strategy is not aligned correctly. It is critical to understand the companys readiness, define a change management roadmap & vision, and identify the key stakeholders in the process. This must be followed by plans, artifacts, actions, guidelines, and training material. Simultaneously, there is a need to be aligned between this plan and the evolution of the main project implementation roadmap, allowing alignment between the different business units.We also need to understand that this journey isnt just about technologyits about people. We cant forget about the human element of our business model as we automate processes and move toward a more digital supply chain. That means defining business process scope by prioritizing those critical processes/tasks with high impact over the entire supply chain and ensuring clear KPIs and OKRs to define full accountability and measure improvements. It also means reviewing our operating model so that we can scale faster.Secondly, it is crucial to understand your current data management foundation. This should start with a data accessibility assessment, which will help you understand the requirements, scope, and use cases you need to address. Once youve identified these needs, its time to develop a strategy for using the right data to improve your companys customer value, operations, and decision-making process. After defining this strategy and identifying any gaps in your tech stack, its time to develop and maintain the new systems aligned with your new requirements.As we continue to automate our supply chain and move toward a digital supply network, we must keep in mind the core values that keep us moving forward. First and foremost, it needs to remember that this journey is not just about automating processes but about creating value. We want to be sure that our processes are robust and standardized, but they should also be optimized for maximum efficiency and effectiveness. This means we must take on a value-driven approach rather than simply focusing on the technology itself. At Globant, we developed a decision platform for data-driven companies called Navigate using process mining and machine learning technology. The tool is an AI/ML-based learning process that creates and evolves predictions and prescriptions. It enables companies to make intelligent data-driven decisions by anticipating multiple possible future scenarios and simulating their results. We want to ensure that we take advantage of short-term RPA opportunities while laying out a long-term vision for automation (and everything in between).All the elements described before are the drivers required to build a clear implementation roadmap that shows how the company can progressively reach a higher maturity level that aligns with its business strategy and provide actionable recommendations to successfully achieve an AI automation implementation in Supply Chain, allowing to become a leading innovative organization by tapping into new technologies to be the most cost-efficient, most reliable, faster go-to-Market faster player, along with a more responsible supply chain.Throughout the years, Globant has been advising clients on implementing AI to realize process improvements and efficiencies in the Supply Chain space. As outlined in this Stay Relevant blog post, weve identified the main areas in SCM where AI can minimize costs and increase outputs: Autonomous supply chain via AI: Understanding main areas for improvement and use cases.Key takeawaysThere is a considerable value at stake when implementing AI in SCM, especially in demand forecasting, but capturing the potential value demands a holistic approach to planning and implementing AI.Three main areas explain the potential value of AI in SCM:Demand forecastingLogistics network & warehouse optimizationSourcing and procurementCompanies fall short when implementing AI because:Their lack of alignment between business & AI strategiesProjects are usually siloedIncorrect methodologies are not accurately leveragedCompanies would approach this from three angles:Change Management & Culture,Processes standardization, andIT Infrastructure"
How artificial intelligence is reinventing the way we learn,"Artificial intelligence (AI) is changing how we live in many ways. Efficiency is improving in many industries, such as manufacturing, logistics, and financial services. It is also helping to improve healthcare by enabling the analysis of large amounts of data and earlier diagnosis of diseases or by enabling greater automation at home and work.How we consume information or learn is familiar to this. Since the advent of the Internet and search engines, the way we find and process information has changed. And that was increased with the implementation of artificial intelligence in these engines. For example, AI-based recommender systems are helping users discover content that interests them.There is also the appearance of tools that generate content with AI in audio, image, or text format, where users can consume information in a much more personalized, interactive, and friendly way.The old debate about how the educational system adapts to the changes starts over. The traditional educational system focused on memorizing information, which was very useful when finding information took a long time and a lot of work. Now, with tools like chatbots, where I can find out the answer to complex questions in a way quick and straightforward, memorizing doesnt make much sense anymore. So much so that, for example, during January of this year, schools in New York prohibited using chatGPT since students could cheat on assignments or assessments.Another of the challenges that this new paradigm brings is that of data literacy. Nowadays, we are constantly exposed to artificial intelligence engines. Why do we need to understand how they work or what we told them? Data literacy allows people to make informed decisions and use data ethically and responsibly. In this context, it becomes increasingly crucial to know how it works to understand if the answer is correct or presents biases.All of these challenges are present for those in decision-making positions. That is why it is necessary to have agile leaders with a high capacity for adaptation, where rethinking and re-adapting are increasingly common. For this, organizations must have leaders capable of inspiring and promoting learning, specific and relevant knowledge for the established objectives, and that accompany the company in its development or growth phases.Many challenges could continue to be listed, but this type of technology brings many more opportunities.Using these tools to teach allows for more interactive, immersive, and personalized instances so that everyone can take ownership of the information and create new knowledge. In addition, they enable people to focus on the purpose and objective to continue looking for different solutions to the significant problems of humanity.Adopting these types of tools in education is a fact, and there is only one question: how to use them to enhance peoples learning experiences and skills?We dedicate ourselves to developing technology to learn from the Diversity, Equity & Inclusion areas of Globant. Our focus is on strengthening the existing knowledge in the company and the community based on exponential technologies such as artificial intelligence.During 2022, we developed a search and recommendation engine to customize Globers experience on ourLearning Management System, Campus Globant University. We also created another AI function to boost learning experiences one by one. We discovered that the Globers find one-on-one meetings as one of the most effective learning formats and that 50% of them need to learn from expert people within the company about the issues that interest them.We also found that 87% of Globers are willing to spend more than an hour weekly in one-on-one meetings to learn and teach. This knowledge about the learning needs and expectations of Globers, combined with technology, gave rise to a new tool: Learning Match, an artificial intelligence engine that, based on users historical data on what they want to teach and learn, recommends -matches- with other people in the company to facilitate one-on-one meetings and foster a learning community.These are just a few examples of how we can use technology to enhance learning experiences, foster creativity, and reinvent industry and humanity."
Meet Augoor: an AI product that smartens up software development,"Todays businesses run on software, whether they produce life-saving drugs, develop residential complexes, or manage a supply chain. The 2020 global pandemic accelerated many industries already ongoing digital transformation process. The new normal is demanding that we speed up software delivery. With this in mind, theres no wonder why companies and investors are constantly looking for ways to boost productivity, shorten time-to-market, and improve coding experiences. Focusing on devs and industry pain points seems to be a perfect strategy to get the right insights and make it happen.One of the main pain points for programmers is that they spend about 70% of their time understanding code. Feeling lost in their code is one of the reasons this happens since large, cluttered codebases seem to be the rule. Whats more, the industry is in continuous movement. This entails projects takeover challenges and job switches, which means lots of time invested in team members onboardings. Other decisive factors that cause this disheartening percentage are fast-scaling teams, tribal knowledge, and technical debt or legacy code.Now lets look at one of the most prominent industry pain points, the shortage of professionals. Reveals Top Software Development Challenges for 2022 survey stated, Software Developer will be one of the top four in-demand professions over the next ten years with almost 200,000 difficult-to-replace openings yearly. This statistic is why every day, more and more software development companies are going global with their teams, and the main reason why counting on an AI product that improves remote collaboration is critical for any tech organization.How can we solve these coding challenges? We can count on the usual suspect: AI technology. This field has more than 50 years of evolution, but since 2017 and mainly the last five years, the impact of Natural Language Processing models has been huge. These models understand natural language and comprehend code, and this is what were doing at Augoor, states Tiburcio de la Crcova, Augoors CEO.Demystifying AI as a coding allyIn recent years, artificial intelligence solutions have been present in almost every industry to replace routine tasks or augment peoples experiences. And the software development sector is one of the biggest beneficiaries of AI technology, as it can improve practically any process and boost team collaboration.Initially, coding enhancing programs for engineers start by following rigorous guidelines (also known as heuristics), and usually, it has no AI at all. As these guidelines become more intricate, eventually, it becomes nearly impossible for a human to comprehend everything, and this is where AI enters the equation.AI has a vital role in software development in the long run since it aims to reduce repetitive tasks, allowing devs to focus on more creative tasks and become more productive. And this is why Augoor is taking advantage of machine learning algorithms,deep learning (neural networks), and natural language processings evolution, plus Open AIs GPT-3 training data, to smarten up software development processes.Augoor makes code more accessible by helping people understand and document codebases from multiple repositories. How? As a developer would document code, but automatically and in a natural way to keep code healthy. Also, different layers of information are generated with code auto-tagging and intelligent query suggestions for the best code exploration experience, boosting teams with AI technology to foster a healthier collaborative culture.Augoor is like a navigation tool, but for devs: we get them to their destination within codebases from multiple repositories. And we do it fast and simple, added Tiburcio. With Augoor, engineers can spend much more time creating rather than understanding and fixing code, giving us millions of opportunities to witness more innovative AI products.To learn more about how Augoor is smartening up software development, visit augoor.com and if youre interested in joining the team, go to Careers at Augoor.Keep up with Augoors updates on LinkedIn | Twitter | YouTube."
How to get your organization AI-ready? Deriving value from your data strategy (Part 2),"Deriving value from your data strategyWhat does data strategy mean for businesses, and what value can we derive from it? Lets narrow it down to what strategy means in practical terms by leveraging Mintzbergs 5 Ps of Strategy and how it translates to Data and AI. These examples do not convey their entirety but as a high-level overview.As we are addressing strategy as a deliberate and purposeful choice with intended beneficial outcomes, lets discuss strategy as a plan. What are the steps we need to take in which direction, the resources available, and the required capabilities? Within that idea, we can take good inspiration from Dan Wagners statement on strategy:Strategy to me, when you boil it down to first principles, is three things. Number one, its an assessment of what you think the truth is today. Number two, its a prediction of what you think the truth is tomorrow. And number three, its a decision of how youre going to place your resources amongst any number of different alternatives based on your prediction of truth. Data-Analytics Chief of the 2012 Obama US presidential campaign, Dan WagnerThrough his statement, we see that there is an assessment, a vision, and a decision  these 3 stages define the following aspects, which we will go into detail about:VisionPeoples capabilities and distribution of responsibilitiesUse cases and relevanceTechnology, platforms, and governanceTimelineVisionLets start with a vision  what kind of role would data have in your organization? Today, companies strive to be data-led, so what must we do to ensure that we rely on evidence to make optimal decisions without being blinded by immense volumes of data? We cant limit our decisions based on the data we currently have or make a decision first while looking for data to justify our hunch. It is equally risky to say that data is the center of everything that we do if we cannot make that a reality.Turning data into a capital asset is the ultimate goal for every organization  this means that we expect a continuous return over data assets that can be reshaped or reused in different ways to drive value for consumers. We have to ask ourselves if our business model can enable us to produce and capture such distinctive data. Does our value offering create novelty that we can capture in data that leads to capitalization by generating a non-fungible dataset?There are other aspirations for a vision, such as working to drive efficiencies and product refinement or aiming at breadth over depth. We can also empower decision-makers across the organization to have the tools and access to the data they need and build the specialized capabilities from the most demanded areas later. It could even catch up to the digital world and vanquish manual processes for accessing operational-critical information at the front line of our business!There is no one-size-fits-all as aspiration should harmonize with the other factors we consider- consuming data, product refinement, or empowering decision-makers to have available tools and access to data.Peoples capabilities and distribution of responsibilitiesWhat is data fluency, and what level should you expect from your team? From the basic ability to consume info appropriately to challenging assumptions behind the data and exploiting data to find new insights, it may be difficult to ensure that we have highly data-fluent people who are positively engaged with the business functions where they are placed. Should the organization concentrate the core data capabilities on a team of specialists, which may keep them in silos from what the business needs?The degree to which we distribute or concentrate expertise, ownership, development, and data usage will impact our capacity to hire and retain people. There are various considerations such as the organizations operational efficiencies, practices around governance, and motivations. We aim to define the depth and distribution of data-related expertise in practical terms  am I able to keep them engaged while encouraging growth within the company?Use cases and relevanceAlthough the vision sets the tone, contact points between people and data drive the application of a successful data-driven strategy. While an evolving data strategy finds new use cases, there must be clarity on a set of use cases and products that act as a litmus test on whether the strategy is sound.It is important to establish the seeds, identify priorities, and create the framework and the workflow of how data will be used. From there, we can overview what we need to tackle, who will be responsible, and what information is required. We define data products as digital products with data as their core functionality. We are considering AI models, dashboards, data streams, data-consumption APIs, and more!Use cases are about enabling the effective use and adoption of data for users in a manner that will make them feel compelled to leverage capabilities based on their merits. So how should we build this list? We can focus on decisions by looking at Marrs framing of possible objectives in these three main aspects, better business decisions, improved operations, and data as an asset. There are nuances in how we consider improving processes as a case ofby applying decisions, such as showcasingdesigning products customers want best or even choosing the best logistics route, and data is an asset that enables businesses to offer actual value when we define appropriate data products that leverage data genuinely. By focusing on who and how business decisions are made and the information required, we can pull from a single thread to align the rest of the objectives.We would want to consider these factors:Technical viabilityBusiness viabilityUsabilityCost to try/ BuildKlout/ Political cost to failClarity of success criteriaOverall vision alignmentRequired conditions and the likelihood of achievementBusiness/Adoption gain from successWe can prioritize the remaining cases by eliminating options based on meeting the required conditions. The most relevant thing, to begin with, is the dependency relations between these initiatives, as the ordering of these factors will affect the overall likelihood of success and adoption.Technology, platforms, and governanceData strategy requires a digital substrate to operate, a set of processes to govern its operations, and decisions on building blocks of technologies, languages, and services. A data platform is a central component that addresses many of these concerns. It is the cornerstone of a good data strategy that enables data products to operate.Large-scale decisions such as leveraging open-source technologies or services like PaaS/SaaS enable us to scale quicker but hinder differentiating factors or proprietary ecosystems. We can also consider typologies of data platforms, data lakes, data warehouses, data lakehouse, data fabric, and mesh. These sets of decisions may impact the technological capabilities available to the business. The type of data processes affects storage like SQL, NoSQL, HDFS, individual binary files, etc.Identifying the right data platform to manage resources is particularly important for MLOps. We should decouple this decision to have a layer of control for budgeting  a thoughtfully designed data platform considers the interplay of use cases, people, visions, and technologies.TimelineRoadmaps are essential to provide the team with a framework for success to help define the desired outcome. The question revolves around the dependencies between elements and our capacities to develop them. We can apply agile methodologies where we deliver value in small increments. The main goal is to provide value as quickly as possible with iterations based on feedback and adoption. As we find that some assumptions do not hold, we would need to adjust the course.Building the timeline needs to consider these checkpoints and define, based on our current knowledge, whether the milestone has provided value. During each stage, we need to evaluate different value tracks. Are we trying to transform the whole organization to provide a network effect on how to work leveraging data? Are we focusing on a minor test case to drive adoption by example? Are we onboarding new team members with different skill sets needed for the developments? We will consider breadth vs. depth for a healthy roadmap with checkpoints.Companies with a smaller risk appetite may not move fast enough and lose momentum or support with their stakeholders as the incentive for transformation is not enough.A fluid plan is hard to actualize, but paying attention to your roadmap, helps set conditions for that spark.Closing the loop and ongoing effortsOne contentious aspect of the strategy is no matter how many times we interpret it, it is treated as a one-off effort whose outcome is no more than a heavy slide deck without continued execution. A proper functional strategy needs to be a constant ongoing transformation track that can be intense at times, requiring monitoring and assessing.Some aspects need to have a higher frequency, particularly at high seniority levels within the organization. The data strategy revisions could cover the following:What has changed in technology in terms of evolution and deprecationsWhat has changed in needs and use casesWhat has changed in business strategy, and how does it impact the data strategyHow well are we managing our current roadmap, and what are the pain pointsGetting enough buy-in from stakeholders and evangelizing across the organizationCritical business questions list  which will become the next batch of use casesFuture-scaping the organization to have a longer-term input to the visionHow to establish effective feedback loops within the organization, data usage, and the aspects of the strategyA good data strategy is a live endeavor  a continuous transformation program. It may have a starting point with much more focus and intensity, but the only way to maintain a strategic advantage is to consider it as the backbone of the way we operate in the data world.ReferencesMintzberg  Strategy SafariBernard Marr  Data Strategy"
AI adoption in banking transformation,"The rise of artificial intelligence has heavily influenced the financial sector. The adoption of this technology has proposed new solutions by solving data extraction problems, integrating information, or analyzing data and new knowledge to improve decision-making.In turn, artificial intelligence improves customer service by changing how users interact, answering their questions, and improving the customer experience.To achieve this goal, they have started to bet on innovations such as natural language processing, emotion analysis, machine learning, or the ability to learn from their interactions.Banks and fintechs focus on artificial intelligence to increase their competitiveness and generate new revenues. Automating processes through AI offers greater efficiency at a lower cost.Therefore, although this digital innovation is still premature, more and more companies are jumping on the bandwagon of digitization through learning machines and natural language processing in decision-making processes.Why must banks embrace AI?Over the decades, banks have been adapting to emerging technologies, from card payments to online banking to mobile banking.Today, banks are in the era of AI. This technology allows them to achieve maximum efficiency, increase differentiation, and deal with risk and customer needs by improving the user experience.As a result, companies in the financial sector have begun to incorporate cloud, big data platforms, and data applications into their processes, eliminating unnecessary costs and face-to-face processing.Thanks to the automation offered by artificial intelligence, banks can improve the decision-making process by accelerating time and using machines that can simulate human behavior.In addition, AI technology can help banks achieve higher profits, increase personalization, and create omnichannel experiences and fast innovation cycles.The main reasons for banks to adopt AI comes from the risk of being overtaken by competitors who implement this technology and the current threats.Increased customer expectationsDue to the COVID-19 crisis, banks had to adapt to the new situation, using online methods to communicate with customers and execute their actions. Not having face-to-face contact forced them to develop applications to replace in-person visits.As a result, customers have become accustomed to online services, and the more they grow, the more they demand innovations to improve their user experience.Thus, banks must embrace AI to take the user experience to a new level through personalization and adapting to customers needs before knowing they have them.Financial institutions are already using AIAccording to McKinseys Global AI Survey report, 60% of the financial institutions surveyed have incorporated at least one AI technology.The most popular technology is Robotic Process Automation (RPA_ (36%), followed by virtual agents or conversational interfaces (32%), and natural language text understanding (28%).However, other adopted technologies have also achieved great importance, such as computer vision, learning machines, natural language speech understanding, or natural language generation.Although some companies have occasionally incorporated some AI technology for specific uses, many financial institutions are already incorporating artificial intelligence into their business model.The digitization of ecosystemsDigitization has reached all sectors within the market, making products or services available to the user through digital platforms.Fintechs have taken the same initiative to offer services or products through mobile applications or websitess, particularly financial one.As a result, traditional financial services are becoming obsolete and far from being able to meet the needs of consumers.That is why banks need to redesign their participation in the digital environment using artificial intelligence as a driver of change.The entry of technology giants into the financial sectorTech giants have brought many advantages to all the markets they have entered. They are currently increasing their interest in financial services, already gaining a foothold in specific areas such as payment platforms, lending services, insurance, or real estate sales and purchases.Technology giants tend to enter businesses to gain new sources of revenue and offer new and more innovative services to their customers.The growing interest of these companies poses a danger for financial institutions unless they use AI to avoid customer churn and compete in the market alongside these giants.What are the benefits of using AI in banking?AI offers improvements in financial services at different levels:Improvements in customer serviceAI aims to enhance customer service departments through improved chat or voice services by advancing natural language processing or machine learning. These improvements offer greater agility in the communication process between banks and customers.Thanks to consumer behavior analysis, artificial intelligence can also bring advances in marketing departments and product development.This technology enables the company to understand users in-depth to improve their customer experience by offering those products that best suit their needs. One example of this in action is robo-advisors, automatic financial advisors that provide financial products and services based on the users personal information.One of the greatest strengths of AI at the financial level is the possibility of assessing the risk of users actions. Thanks to all the information perceived through big data, banks can evaluate the customer in a personalized way when offering a loan or preventing defaults.Improvements in security and internal processesFinancial institutions and fintech companies have also turned to AI to improve operational efficiency, increase security, and reduce related costs.Thanks to technologies such as digitization, biometric authentication, or document auditing, companies can automate processes.In addition, another of the big bets in AI has been fraud control. For this reason, companies develop applications to learn about customer behavior patterns, i.e., how customers interact with their banks, in order to identify possible fraudulent patterns.AI has also been used in the valuation of real estate since advanced technology makes it possible to calculate real estate prices through its characteristics, location, etc.Finally, AI systems will be incorporated into financial supervision to monitor compliance with all regulations and develop real-time predictions to improve monetary policies.Improvements in claim processesOne of the most valuable applications of AI is claims processing.Claims are one of the major weaknesses of any entity. Customers look for agility in these procedures so that organizations can favorably resolve claims in the shortest period.That is why offering a good claims service has to be one of the main objectives of the banking sector.One of the potential applications of artificial intelligence is to predict possible transactions that are susceptible to rejection or reversal.Predicting the probability of a claim provides information on the factors that trigger it, which is essential for improving user service and advising customers on the safest transactions.Another benefit of AI is the optimization of resources,achieved by investigating the most critical claims and those requiring less effort to resolve. When filing a claim, the customer is looking for speed and agility, which can be achieved through chatbots allowing shorter waiting times on the phone and collecting all the data needed to process the claim. In turn, the use of AI-enhanced search can optimize customer information collection without contacting the customer repeatedly.In the end, AI can be the solution to complaints and a starting point for improving customer service and building customer loyalty.What might the AI bank of the future look like?Across all industries, artificial intelligence is a significant driver in accessing real-time data, improving efficiencies and connectivity across enterprises.AI is expanding its horizons in financial services by tackling the challenges brought about by the digitalization of the banking sector and offering numerous benefits.The future of artificial intelligence in banking focuses on becoming more intelligent, anticipating customer demands, automating tasks, and participating in decision making; more personalized, offering unique attention to each customer, and adapting to their needs through their previous behavior.How AI will transform the financial sector for individual users.The use of artificial intelligence in the bank of the future impacts customers daily. Banks will be able to recognize customer behavior patterns even in non-financial applications. They will also be able to assist in payment processes through facial recognition and offer them customized products or services based on the analytics obtained.The bank will be able to develop personalized applications for its portfolios, such as payment or savings management.The customer will receive a summary of his financial activity through augmented reality and will receive reminders about the payment of his bills.Ultimately, the customer will be able to access applications that recommend the investments most suitable for the customers profile.How AI will transform the financial sector in small and medium-sized enterprisesIn small to medium-sized companies, the bank of the future will be able to receive personalized loans based on their cash flows and the companys statistics and needs.Through questionnaires, AI applications will be able to recognize facial expressions or rely on biometric sensors to approve the loan.Banks will also be able to develop applications that advise companies about business management, such as orders, stocks, or inventories, and reminders about outstanding payments.Another task for banks will be to select the best suppliers thanks to the vast amounts of data they will be able to access and thus locate the best suppliers and buyers.Through banking platforms, the company will be able to upload all financial information by sharing invoices and documents for review and approval.Lastly, the company will have access to a virtual assistant to resolve all doubts and help in the process.As we have seen, banks increasing integration of AI looks set to have a lasting effect on the industry. Today, artificial intelligence is not just a reality but is becoming one of the leading development paths for companies.Although there is still a lot of development needed to overcome existing challenges, financial institutions are effectively embracing them. Banks are discovering the advances that technology can bring, so they must stay the course to reap the benefits that AI can offer.Learn more about our Data & AI Studio."
6 easy steps to stay relevant even if you work full-time,"In this ultra-competitive, changing world, staying relevant and continuously learning is vital. However, its challenging to stay ahead of the latest trends and technologies when working 8 hours a day.The first time that I faced this problem was when I was only 20 years old. I was a software engineering student who focused only on studying. But, different circumstances forced me to work 8+ hours a day as a programmer.Ten years have passed since that moment. These days, Im a Technical Director leading a group of 100+ employees. Also, during that period, I got my degree and two other certifications.This article will discuss 6 different tips to stay relevant while keeping a healthy social life. Trust me, its entirely workable and anyone can do it!We are living through countless transformations and the only way for no one to be left behind is to be one step ahead: to prepare young people to reinvent their careers with global projection and to foster an attitude of constant learning.Martin Migoya (Globants CEO) 6 steps to stay relevantStaying relevant does not imply getting a Ph.D., hosting a YouTube channel, or being a speaker on a TED Talk. Staying relevant means being aware of the latest trends in the area where you are passionate . When you understand the present and can imagine a realistic future, it means that you are staying relevant.Every time I succeed in an endeavor that allows me to stay relevant, Ive always taken the same 6 steps. The order is crucial, and each step contributes to making me feel aligned with my goal, gain momentum, or recharge my energies.Let me introduce them to you.1. Goal definition.Staying relevant feels like a never-ending process. However, we can hack our perception by setting short-term goals. Its easier when you have a purpose, a SMART goal, and you put a finish date in your calendar. Those factors will make you tune your pace so that you can reach the end line.In my experience, someone who works full-time should set goals than can be accomplished within one to two quarters. That reduces the chances of abandonment due to a new task in the job or personal matters.Thats the reason I like certifications. Certification typically takes 3 to 6 months to accomplish, has a clearly defined set of topics to cover, and rewards you at the end.Let me give you a personal example. When I was working on the AWS certification, I took one month to define a plan, and then I signed up for the exam. After I had a plan and money invested, I felt energized and eager to start! Thats the energy that you need at the beginning of your endeavor. I wrote a Medium post about it; maybe, it will be interesting for you to read it:The 5-STEP program to get the AWS Data Analytics Certification in three months2. Create a habitYou need to set aside some time each week for your endeavor. Imagine that you need at least 5 hours a week. It seems like a lot but, think about it this way; you can take 3 hours during weekdays and 2 hours during the weekend. It doesnt sound so bad.I believe that the best time for studying is in the morning. Generally, there is less activity at work in the early morning, its easier to concentrate, and your body is full of energy. Im not suggesting joining the 5 AM club. I recommend taking from 8-9 am to read or study something related to your plan.An excellent alternative to start would be to choose the days you are working from home and use the commute time to stay relevant.3. Create a mood to foster momentumTo learn, you need to focus. But, how do you focus during working hours in this modern world where there are distractions everywhere? Many books, videos on YouTube, and articles in papers have been written about it. After reading and investigating, I could get what helps me stay focused. Let me give you a list of the 4 tips that I use:Place a slot in my work calendar (even during non-working hours). I need to give the same value to my challenge as my work.Put my mobile phone in Do not disturb mode and remove it from my working area.Put some lo-fi music to set the study mood (thats really personal, I have friends who listen to Trance Music and others who choose Heavy Metal).Use a timer. The Pomodoro technique is great. Its motivating to say, I have 40 minutes to read this and ,after that, I will take a break .4. Try to find alliesThe context affects each one of us. If we have a group of people around us that foster procrastination every day, it will be hard to focus. Tell people about your plans and new goals. Involve friends in your endeavors so that they push you in the correct direction.In my case, its really powerful when someone asks me, Hey, how are things going with the fresh course you are doing? Most times, I start the next day with more energy.Why dont you tell your boss about it? Dont you think that they would appreciate your desire to progress? Good leaders value employees who strive for progress and stay relevant. It could even help you get a higher score in your evaluation!5. Have a notebook (digital or paper) always close to you.Imagine that you are at work and you finish early. Why dont you use that time for your endeavor?If you have your notes ready, you always have the chance to read and stay relevant. I used to have a professor at university that stated, If you are on the bus, you can use the commute time to read and keep new information! I was lucky enough to meet that professor when I started at the university. That suggestion allowed me to progress a lot in my free time. During long commutes from work to university, I remember that I was always reviewing and changing my notes.However, there is something that I regret about my life at university. At that moment, I used paper notebooks to take notes. Now, that information is not accessible to me!My recommendation is to use a digital notebook over a paper notebook. The concepts that you are going to use are valuable. You will need them for your life. Digital notebooks allow you to have your notes always ready wherever you are. Many digital notebooks even sync automatically with a mobile app. So, no matter where you are, you will always have access to your source of knowledge.These days I use Notion.io. I can tell you the reasons why it is a great app, but there is a famous YouTuber that can explain it better:The Most Powerful Productivity App I Use  Notion6. Reward yourself every time you finish a stepChallenges take time. If you are going to wait till you have your badge, you may get stressed (as you probably have felt at university many times, right?).So, why dont we try something new? Define a plan and divide it into steps. Before starting a step, think about a reward that you will give yourself after completing it. A pleasant dinner, a new t-shirt, whatever makes you feel great. Once you finish that step, give yourself that reward.Dont forget to congratulate and thank yourself when you get to the finish line! Many people quit after the first three weeks, but you could achieve a new goal if you stick with it. You will feel good about it! Why dont you write about it? Post it on your social networks (your allies will be happy to see your achievements) or write a Medium post.Closing thoughtsI want to close this article by asking for your thoughts and feedback. Have you followed some of these steps during one of your recent challenges? If not, would your results have been different if you had?Many thanks for reading, and best wishes on your new endeavor!Eng. Leandro Jorge Mora  Subject Matter Expert. Globants Data & AI Studio"
MLOps: An important building block for your AI-readiness,"Ignoring artificial intelligence (AI) is no longer an option. IT leaders have to understand the opportunities and limitations of AI and decide where and how AI makes sense to create business value. To deepen and share ideas on this topic, CIO WaterCooler, in partnership with Globant, joined a Digital Boardroom with a high-level panel of specialists. The event was held in late March and included the participation of Juan Jose Lopez Murphy, Head of Artificial Intelligence at Globant UK.Artificial intelligence is rapidly becoming mainstream, but poorly implemented AI can lead to a poor customer experience and reputational damage. The organization has to be AI-ready.The first steps to achieve this are to clean up the data infrastructure and hire the required data science talent. Machine Learning Operations (MLOps) handles, automates, and simplifies the complexity of embedding AI in the organization.It seems that we have finally passed the hype wave and are facing an exciting phase of building real, useful solutions. Exploring is not enough anymore.Juan Jose Lopez Murphy, Global Head of AI, GlobantMost organizations use AI in some wayAmong the participants in the session, 47% already had AI in their organization, either in production in specific business units or subsidiaries, centrally managed across the business, or in an AI proof-of-concept project. 18% used applications that supposedly have AI under the hood, while 35% had not used AI yet. The Head of IT at a hospitality brand said, Were looking at using AI in the booking engine, but before that, we have some catching up to do and get the fundamentals right about our IT systems.What we are seeing in AI today is about reaching a competitive advantage, but we are not that far away from the time when AI will be embedded in every aspect of the digital world. AI is going to be a necessity to be competitive and relevant.What MLOps is and why we need itMessy data (59%) and lack of data skills (41%) were reasons not to deploy more AI. MLOps extends DevOps with data science and puts processes around the machine learning lifecycle, according to the main speaker. These should be continuous processes, repeatable, and with good governance practices. MLOps should manage versioning, data storage and retrieval, computing resources, access, and process flows. The platform should be extensible because there are always new models, technologies, algorithms, and ways to put things into production.MLOps is still nascent. We see many offerings with varying degrees of maturity; as that evolves, MLOps will enable more cross-functional teams of specialists  easing up on the current requirements of a cloud + data engineer + ml engineer + data scientist type unicorn we see many organizations trying to find.AI is part of the data platform evolutionVendors are busy releasing MLOps platform products. The technology is exploding at the moment, according to the main speaker. There is no clear market leader yet. Many will eventually get acquired or go out of business, so make sure you have an exit strategy. The latest MLOps platforms include a feature store and build on the concepts of a data warehouse or data lake house, which another speaker used in production. MLOps platforms are the next iteration in the evolution from SQL to batch MapReduce processes to streaming data platforms and near-real-time processing.As platforms with the evolution in AI turn into the data-centric AI, we find a synergy that will enable us to combine large-scale foundational models and low-data expert models that will permeate previously hard-to-get industries.AI is a team sport coached by a business.The CIO of an international investment management company asked whether we should prefer technical or business skills when hiring a data analyst. The speakers generally agreed that business curiosity is more critical to avoid the trap of people falling in love with algorithms. What question are you trying to solve? What decision are you trying to make? What is the unknown that youre trying to make known? In the sessions, the chair had worked with citizen data scientists in the public sector said they could be key players.We always focus on improving decision-making, specifically when the complexity and frequency of the decisions spiral up. If a simple rule solves most of it, start with that and iterate when improvements warrant it. You are halfway there if you can adequately frame how you would use data to choose the better outcome!Regulation coming for Explainable AIThe first speaker addressed the ethics of AI but thought that term was an ivory tower thing, preferring words like explainable, responsible, transparency, and trust. Regulation is in the works from governments, standards bodies, and the World Economic Forum. The speaker had done some work for the Indian Government around this. The legislation would appear in a year or two and look similar to GDPR. Organizations that start implementing the rules of the road for AI now voluntarily would have an advantage ahead of legislation.Ethics is more valuable as a grouping factor than something solvable; hence a focus on the actionable components mentioned would enable decision-makers to be accountable and the impact of the algorithms better governed. Explainable is surrounded by controversy about viability and usage, but the other factors benefit from the start.To explore further, visit https://more.globant.com/reinvent-with-data-and-ai or write to us at globant-uk@globant.com for a free consultation."
Pharma AI: the role of artificial intelligence in the medicine of the future,"New medicines research and development process is complex and lengthy and requires significant investments. The pharmaceutical industry finds it increasingly hard to develop and market efficient medication. The entire process can last up to 12 years and cost between 1.9 and 3.2 billion dollars, according to DiMasi et al., Journal of Health Economics, January 2016.During clinical trials, doctors carry out complex biochemical and physicochemical procedures using massive data. The probability of a drug passing all tests is less than 12%, according to CSIC sources.The pharmaceutical industry has dived into computing and artificial intelligence to improve this process.Artificial intelligence in the pharmaceutical industryThe biopharmaceutical industry has been one of the key beneficiaries of artificial intelligence, mainly because it helps streamline the research and development of medicine and reduce research costs and the number of drugs undergoing clinical trials.The pharmaceutical industry usually collaborates with hospitals and tech companies. That was the case during the COVID-19 pandemic, where the sector used AI algorithms to search for information, conduct studies, and carry out test simulations faster.These algorithms helped identify the gene coding for target proteins that could favorably bind to drugs, reducing the initial drug development phase from around five years to only a couple of months.These AI-powered techniques have played a vital role in developing COVID-19 vaccines with companies such as AstraZeneca or Janssen. Likewise, AI has made it easier to predict how the pandemic might continue to spread and which mistakes we can avoid if we encounter a similar situation in the future.The use of Big Data and AI in the healthcare industry is quite positive, and it impacts both pharma companies and society as a whole. Thats why many organizations have begun to bet on this technology.The use of AI in drug researchArtificial intelligence in the biopharmaceutical industry has revolutionized the sector by facilitating access to biometric data in an agile way and helping identify hidden patterns.AI may also prove helpful in these cases:Patient segmentationProper patient segmentation is one of the biggest challenges in clinical trials. Choosing patients according to eligibility, suitability, motivation, and empowerment is crucial, and these processes usually involve significant delays that can be solved with AI.Thanks to these mechanisms, a more efficient patient classification is possible, and appropriate treatments can be promptly administered, anticipating potential risks and getting more favorable results.Disease predictionIts possible to automate medical record screening or predict behavior patterns to understand how patients react to specific treatments or identify the reasons behind their hospital readmission.Resource optimizationWith artificial intelligence, hospitals and healthcare institutions can reduce the number of in-person visits and replace them with online appointments. AI also allows healthcare professionals to monitor patients, regardless of whether they visit the healthcare center in person or not.Artificial visionRadiologists can also benefit from these innovations. Artificial intelligence facilitates faster image diagnoses, streamlining the initial stage of treatments.Pharmaceutical compound researchAI can automate complex tasks and save time.During the initial stages of medicine research, AI significantly facilitates cellular trial analysis and molecular structure modeling, as well as the prediction of physicochemical properties of compounds, among other tasks.Artificial intelligence also helps understand the possible interaction between proteins and ligands, allowing for the creation of molecules with more significant potential.Its also helpful when it comes to identifying and differentiating images, probably two of the least efficient processes in the development of medicine. Learning machines can spot the tiny differences in cellular structures from microscopic images.AI not only streamlines the initial stages of the process. It also proves valuable for quality control and optimizing medication administration.Drug reuse or repositioningDrug reuse is a strategy that seeks to discover new uses for medication that has already been approved.Thats the case with Botox, for instance. It was initially created as a treatment for strabismus, but it was later discovered that it could also help with migraines and remove wrinkles.Reusing drugs involves fewer risks and streamlines the development process. However, effectively combining clinical trials can be expensive and time-consuming.In this context, artificial intelligence can develop hypotheses faster, accelerating clinical trials.Advantages of implementing artificial intelligenceArtificial intelligence benefits significantly by predicting how molecules might react to certain medications.It reduces the failure rate of clinical trialsDrugs in clinical trials have a success rate of under 12%. The reasons behind that figure include the lack of efficiency, adverse effects on humans, and the high costs of resources, all of which hampers the commercialization of the product.Thanks to advanced artificial intelligence technology, the success rate of molecules can be increased, thus reducing the medicines adverse effects on humans. Machine learning algorithms can also improve efficiency without compromising security.It accelerates the research of new medicines while reducing costsRight now, the drug development process can last up to 12 years, including the preclinical and clinical stages, and it costs more than 2 billion dollars.As new issues arise in different stages of the development process, the price continues to go up, especially during phase 3, where biomarkers differentiate drug responses.Only the search for the compound which involves target and lead selection, validation, and optimization can last between 4 and 6 years. And at this point, only 1% of compounds reach the following phase.Thats why the use of artificial intelligence and machine learning at the beginning of the process is crucial to reduce the risk of failure. A streamlined research and development process helps save time and resources.It helps identify innovative treatmentsThanks to machine learning, technological advances and innovation in the pharmaceutical field have skyrocketed.One of the critical developments powered by artificial intelligence involves personalizing treatments to adjust to each patients specific needs.How companies are using AIMany companies are already implementing artificial intelligence to save time and money in drug development.Although research professionals are still vital in these processes, machine learning can significantly benefit pharma companies.ModernaModerna is an excellent example of artificial intelligence implementation. During the COVID-19 crisis, many wondered how the company manufactured the vaccine so much faster than usual.This achievement was due to Modernas use of state-of-the-art software and algorithms.Exscientia and Sumitomo Dainippon PharmaThis case is one of the most striking ones, as its the first AI-created drug tested by humans. This is the case of molecule DSP-1181, which doctors developed with the help of AI as a long-acting serotonin 5-HT1A receptor antagonist, a drug used for the treatment of obsessive-compulsive disorder.The development of this molecule was possible thanks to automated learning processes that allowed algorithms to generate millions of molecules and filter them to get the right one.Deep GenomicsThis Canadian company used artificial intelligence to identify the genetic cause of some diseases. Thanks to AI, its possible to develop drugs that regulate the defective genes causing these diseases.Many pharmaceutical companies have already joined the digitalization era and are betting on artificial intelligence. While its still too early to know its future impact on drugs, the current results are satisfactory."
How to get your organization AI-ready? (Part 1),"The big picture of MLOps  why does it matter, and where do you need it?In recent years, weve had tons of academic breakthroughs in artificial intelligence and tremendous levels of development on the industrial side of things. Companies have been implementing AI into their business models. While some struggle to make these models work, the speed of advancing technologies and uncertainties in the marketplace make this a challenging endeavor.AI lives embedded within software development, so it has adopted many of its practices with various degrees of success. Many AI developers shudder at the terms AI and agile put together, yet they must coexist.One term that has gained momentum is Machine Learning Ops (MLOps). Lets dive into an introduction of MLOps, why it has appeared, and our approach to harnessing it.What does MLOps look like in practice?The development of AI has had a great deal of experimental or lab culture. Many highly skilled practitioners come from academia, making it common for data scientists to focus too much on science and research. Some organizations were not ready for large-scale integrations leading to many proofs of concepts and failed scale-up attempts. They did not have enough skilled people, amongst other factors, that could hamper AIs success in their business, especially compared to technically advanced companies like Meta, Apple, Microsoft, Amazon, and Googles parent company, Alphabet.What we have learned from complex software and data-heavy developments is that there are some critical project elements:GovernanceGovernance mechanisms become key when we are exposed to several kinds of risks with these models, requiring accountability and responsibility for the potential costs of utilizing heavy computer infrastructure beyond control. It is about who has access to what resources, such as data, assets, models, and deployment.RepeatabilityRepeatability is a critical concept in understanding the quality of any AI model. We can consider models are in control only if we can generate them again from the same conditions. Repeatability enables auditing and improvement of the models and ensures that whatever we are using in production has functionality and is not just a spur-of-the-moment decision. In the context of AI models, repeatability implies tracking data sets, experimenting with configurations, analysis, codes used, etc.Continuous processContinuous process is one of the biggest learnings in the software world. The impact that CI/CD has on software development, its quality assurances, short iterations, versioning, and capability for distributed teams are all invaluable for something as complex as AI models.There has been a confluence of technologies powering the platform side of MLOps transformation, in the same concept that many principles of CI/CD have solidified in different platforms and practices, such as git flow, build managers, and code pipelines.MLOps as a platform  enabling seamlessness at scaleIn business, AI dwells within the data space  whether you call it BI, analytics, data platform, or big data. Several generations of data technologies have been developed to overcome the limitations of previous versions. It is not entirely accurate to define generations as newer versions superseding the previous ones. It is better to describe them as speciations akin to biological processes, where a technological approach has evolved to be superior in a bounded domain. They may get overextended temporarily, but they will eventually get pushed to their niche environment.When the map-reduce paradigm overcame the file storage systems and processing limitations of SQL, mainly known and developed within the Apache Hadoop ecosystem, it enabled processing vast amounts of complex data in a batch mode, spurring the moniker and buzz over big data. However, the batch processes were not a one-size-fits-all application. Other approaches were developed to handle data streams  from messaging queues to micro-batches, and more notably, with the Apache Spark Ecosystem, a current king of its hill. Spark is not suited for every use case, and for AI platforms, there is currently an explosion of approaches to tackle its specific needs.Most of the platforms developed for AI are rooted in at least one core concern of MLOps. There are a host of specific necessities within these platforms, and they aim to tackle the following:Data versioning and data lineage for source data, processing code, trained models, deployment code, and usage logsStorage different modalities for source data used in training vs. data used in productionCompute provisioning permissions, costs, configurations, and environmentsApproval processes, particularly when triggering deployment to production of models, but also for some data processesDecoupling of different process streams ingesting data and processing it into a reusable form, training models, evaluating them, and tracking performance in production and configurations across the boardExperimentation sandbox to prevent constraining innovation while preventing affected critical systemsOne salient feature of the MLOps world is the ascent of a particular structure named the feature store, bringing back the idea of a data warehouse where being a single source of processed truth and valid data to be consumed downstream. In this case, it does not need to be structured. Still, it needs to be an agreed-upon process of data that acts as a reusable representation that can power any model built using the source data, which can be fed and enriched while in production.Not all MLOps platforms will consider that feature, but its become a familiar pattern and a good practice in many cases.The benefit of a good MLOps platform is that it enables companies to have specialized talents in different platform dimensions. It untangles the unicorn that may be needed to otherwise effectively handle core concerns of data engineering, cloud engineering, data science, data architecture, DevOps, and many other disciplines. A well-rounded profile is a good thing, even better with a T-shaped knowledge. Can you imagine how difficult and expensive it would be to find such an expert in everything? It is more than what most companies are willing to handle.ConclusionWe can summarize the approach of MLOps as shifting the requirements from people to platforms, enabling fast iteration cycles on decoupled teams of experts in a modern data platform best suited for developing and serving evolving AI applications in a governable manner. It is a field undergoing a Cambrian explosion of alternatives, with its final product still being experimental. They represent a base level of maturity for any company serious about leveraging AI in the real world."
"Meet Juan Jos Lpez Murphy, Global Head of Artificial Intelligence and Data Science in Globant UK","Many companies consider AI as an enabler of reinvention for their businesses. Juan Jos Lpez Murphy shares his vision, discusses the critical challenges around developing a data strategy, and explains how Globant helps its clients thrive with data and AI.Juan, tell us a little about yourself, and what do you think the benefits of data and artificial intelligence are to business transformation?Id say that while Ive always enjoyed the possibility of modeling a process or situation with maths, the first time I saw a linear regression, I felt a sudden click in me  it made sense. I saw a way to get a level of insight out of facts that you cant get from anywhere else. It sparked this idea about data as the source of hidden insights and understanding and the capacity to act on dimensions that have the most impact potential.Ever since I found my calling, it became clear that I would need to act as an evangelist or analytical translator to make these capabilities accessible and connect to the business world to make these projects successful.I joined Globant in 2014 as a data scientist and grew into my current role today. Eight years later, I am leading the team from our London office. Besides working on exciting projects and clients, Ive been a speaker at many events and even published two books, Embracing the Power of AI and Big Data Engineering (in Spanish).Through these experiences, I came to see data and AI through a lens that grounds our decisions while paying attention to subtle trends, exploring possibilities, and assessing their impact that allows us to make smarter decisions to reshape our industry.Nowadays, I understand that AI is a strategic and operational capability that deals with data, uncertainty, and a constantly evolving market. At the same time, data is also a primary resource and, when treated intelligently, can be capitalized into a valuable asset.Data and AI are about knowing and making decisions at an unprecedented scale and sophistication, freeing up and leveraging our creativity, agency, and vision for the future.During your time in Globant, the Data & AI Studio grew in offering products and services based on data strategies to create solutions for various industries. What was it like to witness the growth?Its an ever-evolving landscape, and it is always so exciting! Technology moves at such a record-breaking pace, and data has grown exponentially. Companies are becoming more sophisticated in their understanding and needs. Over the years, our team has grown 40 times in size. We have also evolved our value offering to our clients and their businesses. We constantly ask ourselves what we want to bring forth through data and AI to improve our decision-making process. We need to adapt to stay relevant and be at the forefront of the reinventive drive.No one gets left behind in Globant, and I am grateful for the opportunity to be a part of this team that takes a step further to nurture new skills to ensure the growth of each Glober.Globants approach is multidisciplinary, combining engineering, innovation, and design to promote the reinvention of companies. What value will you bring to our customers in Europe from your new role?Currently, I am focused on strengthening our capabilities locally and regionally, tapping into local talent while retaining our Globant culture. It is essential to build a solid presence while nurturing a closer relationship with our clients and prospects. Its all about profoundly understanding them to provide the best value in reinventing their businesses. We must continue identifying new opportunities and collaborating with different institutions to build fantastic data products to accomplish this.We need to be active thought leaders in the industry to help bridge the gap between assisting companies in understanding the potential of data and AI while fostering new talents and providing guidance in the subject matter. I enjoy thinking in depth about the world of data and how that relates to different industries.The commitment to launch projects based on data and AI is more significant than ever in Europe. What are the main challenges in the region?Europe is very advanced in the requirements and expectations of data usage and privacy. It is also a very mature market where companies are very refined in their uses which makes them very sophisticated, and they expect frictionless customized digital experiences.Companies are constantly on the lookout to value add. They look to AI as an enabler to create new business models and generate new revenue streams. There is a high volume of investments towards AI, filling the market with platforms and products to leverage. There needs to be a perfect case on building their own customized AI according to their needs versus using something off the shelf. They are all vying for highly sought-after talent  keeping them engaged and committed is no small feat, so seeking external help while retaining ownership is a sweet spot.Since the pandemic, companies have reevaluated their business models, prioritizing digital transformation. Why is data strategy an important aspect for companies to embrace more than ever?Attempting to be digital without good data support is impossible. You might get part of the user experience or a specific target, but you might lose everything that binds everything together to make it stronger. Today, data is both a driver and a consequence of digital transformation. It enables a host of digital experiences such as hyper-personalization, predictions, interconnections with other services, and more.Leveraging data is an operational necessity to provide best-in-class experiences. The next step is mining data. There is much more value to be uncovered, and most businesses are still trying to reach this stage.Success is when data becomes a capital asset, and your ability to deal with data and AI is strategic. At this point, data can be composed, enriched, and consumed to generate new value. It is not a straightforward journey, but it is necessary to develop these capabilities to innovate into the future.Even if it is a 5-year implementation program, you can still see value throughout the journey. A sensible, well-thought data strategy involves a value road map at each stage. Companies need to be agile and adaptable to market movements, consumers, and organizational changes at todays pace. Data is fluid; therefore, its strategy has to be as well.What is the Studios approach to a company that wants to start its digital transformation journey with data? What are the challenges they have to overcome to succeed?The approach needs to be tailored according to its needs to ensure that they make the most of their resources, data, and people capabilities. They would need to understand what they are willing to do and the steps to utilize data to build their scope.We start looking for suitable use cases and define the requirements to get results. During this time, we look into developing talent to ensure that they are evolving together  to develop practical data literacy at the company.Every company is unique, and there are many possibilities. Data strategy, data platform, data products, and supporting structures will need to be aligned. Cultural change is also an essential factor in learning to use data to improve our expertise. People need support in adopting those data products tailored to their use cases to become advocates themselves. When it becomes a self-sustaining effort, only then, a company has successfully transformed.What are your professional aspirations, and why do you think Globant is the ideal place to continue your career?Globant is a place that constantly innovates and encourages reinvention. There are no rigid structures or bureaucratic chains of command ripe for constant disruption. In my role, I learn from different people and their visions daily. We value effective learning more than not making mistakes, and I feel inspired to do more!As a data and AI professional, I have been very interested in how the Chief Data and Analytics Officer (CDAO) role evolves at top companies. The vision required and nuances it takes to be a true disruptor in an empowering manner motivates me. My ultimate goal is to become a thought (and pragmatic) leader in the world of data and AI."
The balancing act of AI in the enterprise,"Prominent business visionaries have touted artificial intelligence (AI) as being a bigger revolution than electricity. And weve seen AI diffuse into almost every aspect of our businesses and industries. It is, undoubtedly, effecting change. But is it the kind of change that we want? How can we ensure this AI-fueled change is both meaningful and aligned with our values and objectives?Designing and implementing AI products is an endeavor that requires harmonizing antagonistic dimensions, like trying to collect astonishing amounts of data while respecting privacy. It also applies to leveraging automation to provide better, more engaging experiences, without alienating consumers when something goes wrong, or rendering the workforce obsolete.What we have learned, through trials and hardship, is that we cant treat AI as a standalone technology. Instead, its an interdisciplinary domain that requires a diversity of viewpoints and objectives.Lets consider the first viewpoint, the coming of age of AI. This means focusing on how technology is transferred from academia into industrial and enterprise settings. When we think of academia, its related to constantly challenging the state of the art of what AI models can do. When we turn to the enterprise, it becomes about engineering. As we have learned from software development, frequent improvement is better than potentially cutting edge in the future. There are many practices converging into what is coming to be named MLOps, from agile data science, all-around versioning  data, pipelines, models, training, products, everything in the form of code-, automated monitoring, continuous integration and delivery, and so forth. Availability, feedback, and iterative improvement are all hallmarks of a mature MLOps system. MLOps, in turn, acts as a litmus test to gauge how healthy are the development and operations of AI systems in the company.Another important perspective is data product management and what constitutes relevant experiences. They need to be designed taking into account that, in the end, there will be humans enjoying -or suffering- the outcomes of the AI models we put in place. What is the value that we are adding? What decisions are we making easier for users? How are we anticipating their needs and adapting? We have learned that if the selling point for our product is that it has AI, then its missing the point. -AI can be an extremely powerful enabler, but not an end in itself. It opens up the doors to a whole new set of possibilities to innovate new digital experiences that we couldnt think of before. So its not only about replacing a custom component with AI, but rather designing around the capabilities that AI enables.Finally, the issues of fairness, security, bias, and sustainability represent an ongoing concern, so the purpose and consequences of using AI really need to be evaluated objectively. Using AI in a manner that helps vulnerable populations, augments our working capacity and ensures equal opportunity for all, becomes more important as these bigger and more powerful AI models are developed. We also need to be mindful that training these models is a power-intensive activity, and we need to balance the value in training one more humongous model versus reusing and adapting the existing ones at the product definition stage.AI is not just a very powerful and exciting technological toy. It can indeed be the biggest revolution of our lifetime. And given that the enterprise is the place where it will come alive and change the world, we must make sure we are playing this balancing act very well, considering an interdisciplinary perspective with modern best practices in a humane way.This article was originally published on AI Business. Globant will be at the AI Summit in London, from 22-23 September to discuss all the latest developments in the world of AI. We look forward to seeing you there!"
3 things Data Science discovered that led to the creation of Augmented Coding,"Data science is one of the most valuable resources to review patterns, reach precise conclusions, and make data-driven decisions for the business. It is a multidisciplinary approach to extracting actionable insights from the large and ever-increasing volumes of data.Augmented Coding, Globants software development solution, is composed of a set of tools powered by AI and was designed based on the discoveries of their data scientists. The following findings gather top insights on how AI software development models are built, granting a behind the scenes on how the Augmented Coding team created the tool.1. Machine learning models run better when they are built from scratchFour years ago, AI had a breakthrough known as Transformers (yes, named after the movies and which represent the evolution of machine translation), which inspired Augmented Coding and other tools. Augmented Coding is also inspired by machine translation architecture, working similarly to Googles translator. On one side you have an encoder and on the other a decoder, transforming from one language to another, from natural language to code.This is what Semantic Code Search, one of the Augmented Coding tools, does. It finds code across all repositories through natural language, receiving questions in English and transforming them into code. Two other examples of these tools are Code Autocompletion, which completes lines of code based on existing ones, and Automatic Code Documentation, which allows developers to easily generate inline documentation by ciphering code and returning natural language (English).One of the most important insights Augmented Codings Data Scientists found was that there is no single model that can learn all programming languages by itself. This model needed to be created. Although there are existing models in the market that understand code, they are pre-trained, meaning someone already experimented with them and trained them with their code and data, making it available to others so they may use it and practice. It was a big learning experience to discover that pre-trained models do not run as well as creating one from scratch.With algorithms built and trained from scratch, the models ability to generalize different coding scenarios is widened, making them useful for a larger variety of cases.2. While natural language and code language differ greatly, they do share the concept of semanticsTheres a relationship between these two languages,code and natural. They both have rules and follow a logic that allows each language to attain their own aspects of meaning. Semantics can be learned and captured by Transformer-based architectures.A machine learning model generally has to learn through supervision; you teach a task, give a data input, and set a target. To learn natural language, there are several tasks involved to teach a machine a learning algorithm.As an example, word masking involves masking part of the input, then training a model to predict the missing tokens  essentially reconstructing the non-masked input. You give a sentence to the model and randomly hide a word or leave it blank, the idea is that the algorithm tries to generate that word. When the algorithm fails you show it the correct word and point out that the one unmasked by it was the wrong one, so they need to correct it. Supervised methods, such as this one, need to be created by examples that you grant to the model assigning a specific objective. It needs to recreate all the tasks that already exist in natural language but in code.The challenge with Automatic Code Documentation was to teach the model not to find text but to generate documentation in natural language instead. One of the most interesting discoveries was that English is intrinsically present through the use of technical terms across different languages. For example, if you document a function and describe what it does while you are programming in Spanish, there can be certain technical terms that are in English. The challenge was to identify the language of those documentations, since not all were in English. The easiest ones to recognize were those with a different alphabet, for example, Mandarin or Japanese.3. One single model can be universalized but it needs to be fed with good dataIn the pre-production phase of Augmented Coding, one of the most important findings came from the generation of models, since the model fabrication was based on each of the programming languages individually. They were more robust this way since they were specialized and would not have as many faults as a model that would learn from all of the languages at the same time.Afterward, in the production stage, the goal was to reduce technical difficulty by using only one model and not several different ones. Through the creation of Augmented Coding, it was discovered that you can generalize more than one programming language in the same machine learning model and it wont unlearn what it has learned from one language to another, its possible to have unified models for all languages. One single model can understand code, generate, seek, and produce the documentation. In regards to state-of-the-art deep learning processing coding language, this is where the trend is leading.Models learn depending on what you teach them, so you need to give them good code to train them. If you give them bad code, for example, it will also learn bad practices, instead of good ones. You have to separate the good practices from the bad, taking care of the data quality through which it is taught.GPT-3, for example, is a known reference for deep learning enthusiasts, designed for general purposes and trained without many filters, mostly with web scraping. However, the bigger the model the harder it is to add the filter you need to identify if its good quality data, so its recommended to do simpler models that learn from one particular context in order to preserve the best quality. One big lesson from Augmented Coding was that its better to develop models that arent so complex, but also improve the data quality from which it learns.Final thoughtsAugmented Coding is just getting started. There are projects on the data science roadmap that are being explored, such as certain functionalities where it is still unknown if they will have the desired impact. This search is a mixture of ideas that emerge from intuition, brainstorming, experience, and experimentation of trial and error. Theres still a long way to go but the goal is to work on improving the whole developers experience."
Why investing in AI is not enough.,"This article was originally published on InfluencersArtificial intelligence and machine learning promise to fundamentally change how businesses deliver services to their customers. Greater personalization, customization, and predictive ability are all part of this promise, which is leading executives to dramatically increase spending on their AI capabilities resulting in market analysts predicting an AI software market worth $37 billion by 2025. Other forecasts predict that revenues for AI services, software and hardware will grow 16.4% year-over-year in 2021.However, here at Globant, we believe that simply investing in AI skills and capabilities is not sufficient. The increase in customer expectations has risen to such an extent that people now expect what were calling human-centered AI. For too long, businesses have focused on using technology to create more efficient products and services, automating everything possible, with the objective of providing fast and efficient services but have ignored the human element.Human-centered AI experiences are becoming important due to two opposing forces:Research shows that people respond to and treat technology as if they are interacting with another human being. This is based on the media equation theory, where for example people describe robots in human terms, or provide polite answers to computers. People want technology to help them in their everyday lives, and when it does, quickly becomes an integral part of them.However, 59% of consumers feel that companies have lost the human factor in the customer experience. This is generally due to poor or ineffective use of technology. For example, while speech recognition and language processing technology is highly advanced today, weve all had frustrating experiences interacting with a chatbot, virtual assistant, or voice recommendation system. By not designing a conversational flow similar to the interaction between two people, properly handling possible errors of interpretation and understanding, the human factor is lost and the experience is impoverished. That happens with many AI-based systems.More human AI-powered experiences mean learning to blend different elements and technologies. For example, rapid advances in natural language processing (NLP) mean that its now possible to provide people with highly customized answers in real-time, for example when interacting with a bot or other conversational interface.Human-centric AI means getting away from the use of the standardized applications that we have become so used to. A banks application looks the same to you, as it does to your retired parents, or teenage children, despite very different financial needs and objectives. Similarly, a retailers application will often have the same bland look and feel, as it is used by all the different demographics that use the retailer. However, with the use of AI we can customize the user interfaces of these applications depending on the characteristics of the client or the use that will be given to the application.Human-centric AI also means understanding the so-called black box of AI, that is what happens behind the scenes, and how the algorithm is reaching decisions. In order to trust AI-based systems to make more and more complex, yet vital, decisions its essential that all stakeholders understand at least at a high level how the system is working. This will be important for everyone from business executives using the inputs to make strategic decisions, to regulators who will need to understand inputs in order to trust results. Were seeing approaches, such as LIME (local interpretable model-agnostic explanations), which provide ways to better understand your machine learning models.There is no doubt that experiences based on human-centric AI will be essential to build an emotional bond between brands and their customers. They will also play a key role in the wider adoption of AI-powered products and services. Achieving this means keeping people in the game, focusing on creating a customer journey that creates an intimate connection between people and AI."
What are the keys to repurposing legacy code?,"One of the key features of software is the fact that it never stops growing. For each product or service that a company creates there will soon be legacy code. Its very common to see companies struggling to deal and manage this  and it can quickly have a significant business impact, as companies wrestle to adapt their products to changing market and customer needs.Why do software systems tend to become obsolete? In our webinar, The repurposed legacy system Michael Feathers, Chief Architect at Globant and author of the book Working effectively with legacy code illustrates the mechanism of software by comparing a watch and a table. On the one hand, the mechanism of a watch is very complex and has multiple pieces and gears that perfectly match each other. This means its a very specialized object that at the end of the day, does one main thing, which is to tell time. On the other hand, when you look at a table, its mechanism is rather simple but it has many uses  you can sit at the table and eat, have a conversation, do some reading or writing.The clock has a very specialized mechanism but only one use, while the table is undifferentiated in terms of design but has multiple uses.When we build software, we usually want it to be more multi-use, but it tends to become more specialized like the watch. This is because we have to meet very specific business and technical objectives to provide specific solutions for clients. The reason why companies find themselves in a tight spot when it comes to legacy code is that businesses are never static and customer demands change each day. As a result, software products need to evolve and accommodate to meet business and customer demands. So there is a conflict between those very specialized pieces of code as its difficult to reuse them to provide new functionalities and services without causing friction.Here its important to highlight two ideas. Firstly, the concept of innovation for most companies involves the creation of something new. New products mean new code. However, innovation also comes from observing and analyzing what already exists and using its potential to enhance a product or create a new product. Its essentially a creative process. Also, more code does not necessarily mean good code.Secondly, often people think about legacy systems in a negative way, like code that is holding them back from modernizing their products. This causes companies to miss the potential of using existing systems to address new needs.How can companies start using legacy code as an asset to pivot their businesses?Michael Feathers provides three steps you can follow to use legacy code as a tool to pursue new opportunities in the market:Examine existing products. Analyze not only your technical assets but your systems in general. What capabilities and key competencies do you have? What makes this particular capability different from the new one that you need?Discern capabilities. Take assets you identified and ask yourself if you can separate them in order to repurpose them for new products or markets.Re-purpose. Use your revised legacy software to enter adjacent markets.Explore in-depth the best practices behind repurposing legacy code and watch here the complete webinar with Michael Feathers.https://www.youtube.com/watch?v=zhwQsvxVJkw"
"AI, the key to succeeding in todays business world","Our White paper exposes how Augmented Coding helps companies integrate artificial intelligence into their daily operations and their software development strategies.Artificial intelligence has made giant leaps during the last few years presenting an array of new options for businesses. AI is increasingly transforming the way we develop software. Through Augmented Coding, Globants newest patented AI-powered solution, developers are coding faster, more effectively, and with fewer errors, by harnessing the power of AI.Decoding the developers worldThe easiest way for a developer to find code is to search for it in natural language. Augmented Coding makes this possible because Natural Language Processing (NLP) is at the core of the solution. Using semantic search, the tool provides the code the developer needed, even if the developer didnt use specific keywords to find it. This means they can then use the code quickly and easily.1 tool with 3 game-changing featuresThrough 3 key elements, Augmented Coding will grant companies a solution that accelerates the software creation process, promotes best practices in daily tasks, and enables engineers to code in a more agile, efficient, and creative way.1. Semantic searchDevelopers can easily search code repositories in natural language. In order to search within these codebases, the programmer doesnt need to use keywords or specific terms, as they might do if they were using other tools. Thus Augmented Coding provides a much-enhanced search option which in turn accelerates the software creation process.2. Automatic code documentationAnother feature of Augmented Coding will be its capability to automatically document code. Developers will write code and the tool will describe it in natural language. It will be able to do this because it also works as a natural language generator. At the core of Augmented Coding is Natural Language Processing (NLP), a branch of AI that helps computers understand human language and communication. In order to build this system capable of understanding both natural language and code, we trained a model from GitHub written in a variety of languages. We used several repositories with nearly one billion lines of documented code.3. Code auto-completionCode auto-completion is possible because the tool will use a semantic representation of the context where the code is being written. It will therefore predict what the developer is likely to code. This will be adapted to Integrated Development Environments (IDEs).Globant is unleashing the potential of teams with the use of Augmented Coding. Saving time, increasing productivity, and improved software development are only some of the game-changing results seen so far, allowing developers to work in a smarter, collaborative way.Augmented Coding = An AI-driven businessWe are harnessing the power of AI to increase capabilities and help reinvent the technology industry. Augmented Coding shortens the learning curve when developers start working with a new programming language, empowering them to enhance their capabilities.For organizations, Augmented Coding will offer to improve developers hard and soft skills while lowering risk, and improving code reuse. Ultimately, Augmented Coding can help organizations create better software and better digital experiences for their customers. In short, its a win-win, that helps teams and businesses.Read more about Augmented Coding and discover how far this tool can take your business and your teams.Download our whitepaper here"
Data-driven marketing in the new world,"The current landscape that businesses find themselves in demands that companies go back to the drawing board and rehash their marketing strategy, brand messaging and manage their brand image to address the needs and expectations of customers in the new world. It is essential that companies tread carefully during these hard times. I have noticed two kinds of brands in crisis: those who are waiting and watching, and those who are viewing the scenario and realigning their strategies to reach their audience.Traditional media and face-to-face interactions with customers are not an option any more. Research shows that more than 50% of marketing budgets are now focused on digital channels. Lets explore how digitization has changed with the crisis and what brands can do to build and sustain customer loyalty and a strong brand image.The role of social mediaSocial media is no longer an exclusive field of entertainment or simply to track photos posted by the Kardashians. Forward-thinking brands had already been using social channels as an extension of physical interactions with customers prior to the crisis. In the past few months however, it has taken on much greater importance. I can hear you thinking, nothing will take the place of face-to-face customer interaction in the long-run, but I believe social will be a close second. Simply put, if a channel has a captive audience, dont just spend your ad dollars on it, but use it to reimagine your brand communication strategy and to build new revenue channels.Data-driven marketing in the new world 4So what do you do with all the ad dollars? According to IAB, 70% of buyers have already adjusted or paused their planned ad spend, while 16% are still determining their next steps. Part of these budgets will move to audience targeting and over-the-top (OTT) platforms for the third and fourth quarters of 2020 and even some part of 2021. Also, marketing messaging is moving to cause-related and mission-based marketing.What organizations need more than ever is to stay connected and sustain customer confidence. Marketing automation will create the difference between the haves and the have nots in the times to come. Bands that have a customer nurturing process in place without overheating their marketing engine will be well positioned to win.Make your content stand outData-driven marketing in the new world 5Next, look at your brands content strategy. This is not just a tactic for crisis management, this is hygiene. In the UK, people were spending an average of 4 hours a day online following the crisis, a significant increase on 2019 levels. We saw similar trends in the US and other countries. Mobile engagement across the globe shows no sign of slowing down. It has become more important than ever that brands create a content plan and can detect and measure what works. For those organizations that dont have the in-house resources or expertise to create compelling content, third parties can provide an attractive option. In the era where there is an ocean of content on every subject, focus on interactive content versus static content to stay relevant and outperform the competition. With ever-changing search engine algorithms, its time to tirelessly work on SEO to keep your content ranking high.Data-driven marketing in the new world 6Lastly, but most importantly, let data drive your decision-making. Track, track, and track how your audience is interacting with your brand. Companies that dont know their customer behavior are shooting in the dark  and risk losing market share to those who have data-driven marketing strategies in place. Let data show the strengths and weaknesses of your brand and predict the future behavior of your audience.Many businesses will experience difficult times as a result of the pandemic. The more you do to retain your brand presence now, the quicker your brand will recover and bounce back as the world adjusts to our new normal. Spend this time to build and sustain long-term relationships with your customers."
The challenges of building human-like conversational experiences,"In my earlier articles on building conversational experiences, I explored how they have become highly attractive to businesses, whether they are text-based (a chatbot) or voice based (a voice assistant). I also provided some ideas as to how organizations can use them to better connect with their customers. In this concluding article, I want to focus on the challenges you might encounter in building your conversational interface.Providing customer feedbackAll graphical interfaces have the capability to provide feedback to the user after a specific action. But with voice assistants or chatbots we are not able to use animations, colors or raw data as you would in a website or app.If you have tried a conversational interface, you might have noticed that interactions provide feedback by using part of the original message in every response. For example, if I ask Alexa to set a timer for 15 minutes, the assistant will reply 15 minutes, starting now.While we try to imitate human interactions as much as possible, when using conversational interfaces there is a greater need for some kind of confirmation. This is why, while repeating part of the message as a confirmation might seem odd to people, the alternative of following an interaction without providing any confirmation or giving some sort of feedback about whether it has been correctly understood, can leave people with the sensation of an incomplete transaction.As conversation user interfaces (CUIs) improve and people gain confidence in using them, we will probably see less of this approach but, for now at least, it is a necessary constraint.The issue of contextWhile lots of our human interactions are simplified or shortened by omitting some information, CUIs are not that advanced. We need to provide them with context. When you are asking about the weather for example, you will need to provide at least the context of the location you are referring to.The workaround for these types of interactions is to set up a profile that gives the initial context of the user, and the information we might need to know, in advance to correctly solve their requests.Building context may not necessarily mean creating a profile. Sometimes you will need to prompt with follow up questions to complete the intention.Complexity of simple conversationsDesigning conversational interactions does not mean passing the Turing Test. But, to successfully create a functional product, you do need to provide human-like responses, not just confirmations and errors.Yes, this may mean going out of scope of your actual application: you need to invest in providing smalltalk feedback, adding a certain personality to responses, and using past interactions to predict future behaviour.Complexity then means eliminating the robotic or automated feel when converting visual feedback into more robust textual responses for our users.Mixing languagesDifferent regions with different languages and dialects might use words that are not necessarily the same, and this can lead our assistants to fail with their translation. This is mainly due to the fact that the first action that an assistant performs is speech to text translation. When a pronounced word is outside of the language dictionary it might lead to it being mapped to something else. After that, when the text reaches the point of natural language understanding, there can be trouble matching the pattern which results in the assistant not understanding a command.Those issues might not surface in a chatbot for example, since that speech to text translation never happens and that particular word can be added as a training example to our models.Next steps for conversational experiencesWhile CUIs are far from the chatbots of 10 years ago, which could only reply to a specific set of words, theres still a way to go to achieving fully linguistic comprehensive AI.With the spread of many voice enabled devices, and their increasing popularity, the amount of interactions and available apps will increase, bringing together major improvements over the different platforms.Hopefully by now, if you have followed this series, you will have a better understanding of conversational interfaces, how they work and how they can be applied to your business solution. Please dont hesitate to get in touch with us to find out more about the work we are doing and how we are helping organizations."
The Content Dilemma: The balance of collecting peoples data,"Data, hyper-personalization, and value generation: Insights from the most recent Netflix documentaryBowling in Columbine. Super Size Me. An Inconvenient Truth. Every now and then a new revealing documentary emerges from niche interests and gains popularity and the attention of a broader audience. Quickly, your LinkedIn feed is full of passionate reviews, it becomes the hottest subject for your Whatsapp groups, and the publications you follow start to publish articles and editorials about it.The newbie on this list is The Social Dilemma (Netflix, September 2020) which shows the influence of social media platforms, and the landscape of data collection and behavior manipulation through refined algorithms.It comes with huge viral potential. The movie was launched in a particularly complex panorama: countries are still dealing with the COVID-19 pandemic and many are facing a second wave of lockdowns. According to Digital 2020, a report published by We Are Social and Hootsuite, the pandemic has led to 70% of people spending more time on their smartphones, and 43% report increasing their time on social media. The report also showed that more than half of the worlds population uses social media, a number estimated at 3.96 billion in July 2020.The narrative of The Social Dilemma targets those who are less familiar with the intricacies of technology. And it does well on that perspective, even if sometimes there is a lack of more structured technical information or a better understanding of how things had developed themselves until the current scenario.Even if it is always helpful to hear from those who were deeply involved in the creation of many of the social media platforms used today, for those who are more familiar with digital marketing initiatives, ethical usage of sensitive data is something that has been widely discussed for a long time. And more recently, digital marketers have understood the implications of the General Data Protection Regulation, and how it impacts the data we collect, use, and store.From the perspective of a content strategist who has worked closely with large brands in the last decade  and after watching The Social Dilemma  a few thoughts popped into my mind on how companies can improve their experience using content without transforming their brand into Orwells Big Brother.Make it worthy: Deliver value to your audienceWhether your content goals are raising awareness, walking your user to a conversion journey, or launching a new product, your brand should provide valuable information in return for the insights it collects. Your prospect, your customer, they want to be sure they are making a smart decision when relying on your products or services. Feed them with relevant content and become an authoritative figure in your industry. Recommended content includes How-to guides, highly detailed technical information, decision making checklists, trustworthy reviews. You know better than anyone else what your brand has to offer.Make it honest: Dont over accumulate sensitive but irrelevant dataI remember hearing it from a client: we dont wanna sound creepy. The first step is: do not ask embarrassing questions. Why do you want to know the food preferences of your real estate client? Why does the marital status of your prospective client matter? In the case of doubt, dont ask for the information. If it becomes necessary, you will find an occasion  and a good trade-off  to ask for it.Make it accurate: Make it hyper-personalized to improve peoples livesThere are a few industries that have been well-known for offering hyper-personalized experiences  these include health tech, insurance, and fintech. But take the example of real estate. Knowing where a tenant works, which university they attended, their hobbies and interests, how they commute, and who their flatmates are. All this information may help you to offer more personalized services. The data you gather can help create actionable recommendations, and provide a better experience, such as improved property recommendations. But this is also sensitive, personal information, so firms need to find the balance of offering more personalized services without becoming creepy.Transparency and balance need to be at the heart of todays digital marketing strategiesThe crises presented on The Social Dilemma are not just the result of new technologies. Rather they are based on an asymmetric and undisclosed relationship between companies and their customers. To navigate this complex environment, leading organizations are focusing on building those bridges between personalized services and respect for peoples data, with transparency and balance."
"Haldo Sponton: With Augmented Coding, we are going to really disrupt the software industry","Have you ever asked yourself what is the limit of artificial intelligence (AI)? Can a machine learn to program all the innovative solutions that we use? Imagine if there existed software capable of proposing to programmers the exact code they need. We have great news: it is no longer necessary to imagine it, it exists, and it is called Augmented Coding, and you will be able to find out about its potential on September 17 at Converge+.As the event date approaches, we invite you to get to know how this innovative solution, that promises to transform the future of programming, arose and became a reality. Discover the history of Augmented Coding told first-hand by Haldo Sponton, VP of Technology and Head of AI Development at Globant.How was Augmented Coding born?Augmented Coding arose from experimentation. At the beginning of 2019, we were playing with linking natural language and programming language, for example English and Python. After a month and a half, we arrived at the first result, a small engine which could ask questions in natural language and it would return the relevant code as a response to that question. We had managed to link data between two worlds!On February 5, 2019, in New York, I showed the engine to Martn Migoya, Globants CEO and Co-Founder. I am never going to forget that day. We spoke about its potential and he told me this tool could perfectly understand programming languages, help programmers in a lot of ways, and completely transform the way in which we make code. That was a very powerful sign that we were on the right path.How did Augmented Coding evolve and develop?Starting with Martins vision and validation, I had all the support to set up a great team for research and applied development. Everything was very new and needed a lot of work before we could transform it into a product.Within three or four months, we had our first MVP or solid proof of concept, which supported more than one programming language. During the second half of 2019, we dedicated ourselves to developing it, improving its interfaces and trainings, and developing its functionalities.At that time, we realized something fundamental: the MVP not only responded to questions with code fragments, but it was also capable of performing the inverse operation. It could receive a code fragment and explain the operation to us. In other words, it automatically generated documentation for the code. So those clients with enormous undocumented code bases now have the possibility of having them transformed with original documentation immediately added to them. With that the code becomes understandable and maintainable. Its amazing!What challenges have you had and how did you overcome them?One of the greatest challenges of the project was getting training data. AI algorithms need to learn, and for that, you have to give them large amounts of data and hours of training. We had to go out and search for open, quality data that could be used for training. It was not as easy as it sounds!In addition, we constantly had to train the team to work on these models and artificial intelligence approaches, because we also needed to specialize in state-of-the-art natural language processing technology. Today, the team has 30 people, is constantly doing research, improving the products functionalities, and looking for new functionalities.What impact does Augmented Coding have on how programming is done? And what opportunities does it provide for companies?We are revolutionizing the way in which we develop software. This will help developers program in a faster and more agile way and code with fewer errors. This will help our clients be significantly more productive.Today, AI is accelerating software development. With Augmented Coding, we are compiling this intelligence to place it at the service of developers working for clients.To our clients, code is gold. For this reason, the product has to be installed and the code processed in a secure manner. No complications can occur, and it has to be able to provide developers with fluid and intuitive access to the tool.In addition, each functionality of Augmented Coding (self-completion, automatic documentation, semantic search), depending on the clients use, can be trained, and may evolve for what each client needs. The potential that Augmented Coding has is tremendous.Why did it become more relevant during the pandemic?When confronted with a distributed work scenario, it was essential to support our developers, who no longer had the in-person support of a leader or technical support staff.We achieved this with Augmented Coding, which worked as an assistant who automatically responded with the code that individuals needed. The tool quickly became the first point of consultation.In the future, remote work will be more and more common. For this reason, its important to have technology that helps teams collaborate.Augmented Coding is based on collaboration, it learns from the operations that everyone performs with the tool. Whenever valid relationships between questions and codes are established, they are saved as useful links. When these relationships multiply, a knowledge map is created with an extremely large amount of available information. This ends up impacting speed, efficiency, and productivity.What expectations do you have for the Converge+ event?First of all, the most important objective is to introduce the tool and communicate its immense potential. With Augmented Coding, we are really going to disrupt the software industry. We want to share this message with everyone attending, and show them this new vision of programming.We also are extremely excited by the incredible speakers that are coming to the event, and with whom we hope to talk about these ideas.CONVERGE+On September 17, Globant invites you to participate in Converge+ Reinvent the Future: Organizations Strengthened by AI At the event, Augmented Coding will be introduced, an AI-driven solution that will forever change the software development industry. Some of the top speakers who will participate in the event are: Steve Wozniak (Top Technology Speaker, Innovation Expert, Cult Icon, Co-Founder of Apple Computer, and Bestselling Author) Martn Migoya (CEO & Co-founder, Globant) Haldo Sponton (Head of AI Development) Michael Feathers (Founder and Director of R7K and author of Working Effectively with Legacy Code) James Taylor (Creativity Speaker, Award-winning Entrepreneur, Founder of C.SCHOOL)Come get to know this new form of programming and discover how it can change the present and future of businesses!HALDO SPONTONVP of Technology Head of AI DevelopmentHaldo is an eager technology and math lover, with vast experience as a teacher and researcher in signal processing and machine learning in universities in Uruguay and France. His primary focus is the strategy of digital and cultural transformation of companies based on a data driven mindset. He is a VP of Technology and Head of AI Development at Globant. Hes the leader of the Augmented Globant team with the objective of implementing and realizing the idea of Augmented Organizations in order to transform Globant and organizations. This team develops tools based on Artificial Intelligence that transform the way we code, the way we find the best talent, the way we design, the way we organize anda access all our information, and the way we keep our culture alive. Haldo has an Electrical Engineering degree, a Master in Sciences degree in Applied Mathematics and a Tech MBA degree. He has been keynote speaker in different tech events, and hes the co-author of the book Embracing the power of AI: a gentle CxO guide to Artificial Intelligence, in which the impact of AI is analyzed both from a technical and business perspective, among other scientific publications."
The role of artificial intelligence in improving recruitment processes,"Job candidates who decide not to take up a position, after receiving a job offer, can cause headaches for human resource departments. For large scale companies that hire thousands of people every year, this can be a major issue. It can affect everything from talent management plans to revenue.In this article, we examine how artificial intelligence and machine learning techniques can manage this problem, while improving the employee onboarding experience. We also analyze the challenges that organizations need to address if they want to implement such a solution.The problems with current recruitment assessmentsMany organisations implement a candidate engagement program to engage people after providing an offer to help with their onboarding. Such a program tracks candidates engagement with details such as their contact details, communication dates, attendance at company events, previous employment details, travel time from home, and responsiveness. This information helps recruiters provide a more personalized experience for candidates  but hiring managers and recruiters also perform a continuous assessment of the risk that the person wont finally take up the offer, and use this assessment to manage their hiring pipeline.However, these cancel hire risk assessments can be unreliable making it difficult for hiring managers to plan for contingency. Factors such as subjective assessments and poor use of data reduce their reliability. In particular we see:Subjective judgements are often inaccurate. In many organisations, the assessment of cancel hire risk is performed through judgement calls by individual recruiters and hiring managers. The quality of these assessments naturally vary. For instance, a department head can probably make a better judgement call than a new manager because of their prior understanding of the market.Assessments ignore past data. Many of these assessments do not leverage knowledge and insights from past data, nor the collective intelligence of recruiters and hiring managers. The usage of assessment data becomes limited to simplistic and past-data oriented dashboards.These inefficiencies in risk estimation can impact talent management plans.Artificial intelligence and machine learning have the potential to improve the reliability of these assessments. We have worked with organizations to build machine learning models that use past data and leading indicators to generate a predictive score for each offered candidate.Such models help drive objective assessments that use an organizations collective intelligence, rather than relying on a few senior managers. Hiring managers can then use this score to increase engagement activities with candidates to mitigate risk. These scores can also generate an enterprise view of risks that will help central capacity teams.AI solutions have significant potential, but challenges remainAI-based solutions can transform the way organisations manage their talent pipelines. However, building such a solution has its own set of challenges that HR teams need to address to realize their potential. In our experience building solutions for our clients, we have seen the following:Prior experiences can impact acceptance. First and foremost, gaining acceptance of the idea is not always as easy as you expect. This can be because of the hype about AI and ML. In some cases organisations have tried a AI/ML solution in the past and experienced poor results. Such approaches might have been unrealisticand driven more by hype around the technology. It is also possible that earlier solutions failed due to the wrong skill mix. Setting the right expectations is key for success.Technology teams often continue to work in isolation from the business. Initiatives that are too technology focused or theoretical tend to fail in delivering a business solution. Teams building the solution often fail to enlist recruiters or HR personnel whose contribution and observations are equally, if not more, important than the algorithms being discussed in the sprint calls.Talent acquisition processes have to be revised.For example , many of the leading indicators about hires such ascandidate responsiveness, job portal updates and communication dates, that are critical indicators of the outcome, may not be captured in the current process. This implies that processes and spreadsheets have to be revised to capture the data elements that are required for the models.The typical challenges of building data based solutions have to be addressed.Examples are missing data, multiple date formats, variable reduction, de-duplication of data, encoding of the categorical data, model selection and cross-validation. Proven techniques usually exist for these tasks, but considerable time often goes into addressing these issues and requires inputs from business users.We recommend following standard practices like basic descriptive statistics, evaluation of ML models for predictive and cognitive analytics, hypothesis testing, and application of multiple ML techniques. It is then necessary to continuously improve the accuracy of your models by gathering better quality data and learning from historical data.The more data, the better, for algorithms. This means you may have to spend time collecting and munging the data once the processes and templates are revised.Solutions are best built using an iterative process rather than treating it as a one-time exercise.Earlier iterations tend to set up a base and accelerate outcomes in future iterations.AI has significant potential to help organizations manage talentTo summarize, there is immense opportunity for organisations to become better at estimating and managing their new hires by applying machine learning. Recruiters and hiring managers can use these technologies to benefit from the collective intelligence of their team and organization. Doing so means they become better at managing the risk of cancel hires and the resulting business impact.We published this article with the help of our colleagues, Hrishekesh Shinde , Surender Singh and Aishwaraya Upadhyay."
SageMaker Studio: The IDE for machine learning development,"IntroductionSageMaker Studio was made available in April 2020 as a new addition to the SageMaker family of products. Its aim is to create a cohesive experience for everyone that works to solve machine learning (ML) problems, including tools for every part of the process in an easily accessible IDE-like UI.This article is for people new to SageMaker Studio and the JupyterLab environment who want to approach it using a tool that provides a cohesive, integrated experience. We have included links to official documentation and tutorials, with the goal of grouping the most essential information regarding SageMaker studio in one place for the curious novice.Well describe the main features that make working with data easier, covering the following topics:Onboarding and sharingPerforming common tasks in StudioTraining and experimentingMonitoring your modelsCutting costsBut first: what is SageMaker? SageMaker is a group of services that lets us solve ML problems such as data exploration, model training, model deployment, and monitoring. SageMaker Studio is Amazon Web Services interface that brings together the best parts of the service in a single integrated experience. Together with the SageMaker SDK, we can perform the most important parts of a machine learning operation with very little effort.Related reading:Announcing General Availability of Amazon SageMaker Notebooks and expansion of Amazon SageMaker Studio to additional AWS regionsOnboarding and sharingSageMaker Studio is designed to onboard new users and set up an environment suitable to work with data in minutes. It also provides a means of sharing notebooks between users.SageMaker Studio users are assigned to a single domain, are assigned user profiles, and have isolated storage spaces where they can store their user files. Well explore each of these concepts now, and get a few tips on how to get started today.What is a SageMaker Studio domain?From the AWS documentation:A domain consists of an associated directory, a list of authorized users, and a variety of security, application, policy, and Amazon Virtual Private Cloud (VPC) configurations. An AWS account is limited to one domain per region. Users within a domain can share notebook files and other artifacts with each other.When creating a domain through the console, only a few options are available. Using the AWS CLI provides a broader scope of options, particularly regarding default user configurations. This can be particularly interesting for users that are looking to customize default instance size for both the Jupyter server and the machines running the kernels.Domains are the central configuration object in SageMaker Studio. Among other things, they determine how user authentication and authorization work. We have two choices for this:IAM allows us to access SageMaker Studio using regular IAM accounts.SSO allows the use of AWS Single Sign-On, which is a separate auth service from IAM. Users do not need to have an IAM account to have an SSO account. Please note that SSO is only available on an AWS account which has it enabled.SageMaker Studio requires an execution role to interact on your behalf with other AWS services. The role must have the AmazonSageMakerFullAccess policy. You can easily create this from the domain creation dialog.To allow SageMaker Studio access to resources that reside inside of your VPC, you can set your VPC, subnet, and security group here.Read more about domains:create-domain  AWS CLI 2.0.14 Command ReferenceCreateDomain  Amazon SageMaker ServiceWhat are user profiles?Regarding user profiles, the official documentation defines them as:A user profile represents a single user within a domain, and is the main way to reference a person for the purposes of sharing, reporting, and other user-oriented features.In other words, domains are simply collections of users. Its only possible to have one domain at a time per account per region.SageMaker Studio user profiles are isolated from one another. However, they may need to share files or data at some point. SageMaker Studio allows users to share notebooks.As soon as you create a SageMaker Studio domain user, you will create an Elastic File System (EFS) volume. This EFS volume will be used to store that users files. Each SageMaker Studio user will have their private home folder hosted in this volume, and each time they spin up a Jupyter server instance, their home folder will be mounted seamlessly.Read more about user profiles:CreateUserProfile  Amazon SageMaker ServiceShare and Use a Studio Notebook  Amazon SageMakerCreating a SageMaker Studio userTo use SageMaker Studio, you need to create a user for the domain. This user will get assigned compute and storage resources. Depending on which identity provider you pick  AWS Identity and access management (IAM) or AWS Single Sign-on (SSO), youll need to follow different steps.IAM userTo create a IAM user, in the domain dashboard, click Add User, then pick a username and an execution role.Sagemaker studio: the ide for machine learning development 27Sagemaker studio: the ide for machine learning development 28Sagemaker studio: the ide for machine learning development 29Accessing SageMaker notebooks with pre-signed URLsHaving an IAM account for each of your SageMaker Studio users might introduce administrative overhead that is not compatible with your current workflow. You can still give your users access to all SageMaker Studios features by using pre-signed access URLs.An easy way to do this for the first time is to do so through the AWS CLI tool. However, manually generating URLs should be automatized by some other tool. When you reach that stage, be sure to use one of the many AWS SDKs for the language of your choice. For this example, were going to be focusing on the CLI.A pre-signed URL is a limited time use link that allows you to access SageMaker Studio IDE for a specific user. To generate one, we need a domain ID and a user profile name.Before you begin remember to be logged in to your AWS account with SageMaker Studio full access permissions.Step 1: Get domain name and user profile name by running the following command: aws sagemaker list-user-profilesSagemaker studio: the ide for machine learning development 30Step 2: Create the URLaws sagemaker create-presigned-domain-url domain-id <your-domain-id> user-profile-name <your-user-profile-name-here>Sagemaker studio: the ide for machine learning development 31Try the pre-signed URL by using an incognito browser window.Read more on pre-signed domain urlsSingle Sign-OnIf the intended users for SageMaker Studio do not have an IAM account, other auth methods can be used by enabling AWS SSO. Microsoft Active Directory or SAML 2.0 can be used instead. To use this, make sure the account youre using SageMaker Studio has SSO enabled. You can read more about the service here.Enabling SSOLogin to your AWS account and visit this pageClick on Enable AWS SSO3. Once SSO is enabled, add a new user by clicking User > Add UserSagemaker studio: the ide for machine learning development 324. Create a user, then make sure you can login with that user in the generated user portalSagemaker studio: the ide for machine learning development 33Creating a SageMaker Studio domain with SSOMake sure you dont have any previously created SageMaker Studio domainsPick Standard Setup when creating the new studio, then AWS SSO as the authentication method.Create an execution role or reuse one from a previous scenarioCreate the Studio domain.Sagemaker studio: the ide for machine learning development 345. Once the domain is created, assign the previously created user to it.Sagemaker studio: the ide for machine learning development 356. To test that the user is currently working, go to the SSO user portal and login. Remember that the user portal URL is generated for you when you enable SSO.Sagemaker studio: the ide for machine learning development 368. You should see that the created user now has the SageMaker Studio application enabled. Click on it to open the SageMaker Studio IDE.Sagemaker studio: the ide for machine learning development 379. This starts the SageMaker Studio Jupyter server. To shut down the server and quit, use File-> Shutdown-> Shutdown All.Performing common tasks in StudioSageMaker Studio provides a customized JupyterLabs environment that offers a full IDE experience for data tasks. Well now go through some of the UI features and conveniences introduced by this tool.The sidebarThe sidebar allows quick access to SageMaker studio tools.File explorer: shows you the content of your Studio users home folder.Git: allows you to graphically perform git operations.Running terminals and kernels: shows you terminal, kernels and images that are currently open. Handy for closing them.Command Palette: searchable command list to customize your IDE and perform common operations.SageMaker Experiments: lets you track visually your SageMaker Experiments.SageMaker Endpoints: lets you visually monitor your deployed models.Tabs: a list of open tabs in your Studio IDE, handy for navigation.Read more about SageMaker Studios UI:Amazon SageMaker Studio UI Overview  Amazon SageMakerLaunchersA launcher is a page in the IDE that lets us launch an activity.To create a new launcher, use the menu option File -> New LauncherSagemaker studio: the ide for machine learning development 38In the Launcher UI, activities are the most common tasks in day-to-day notebook use. Well do a brief tour of what is available out of the box.Starting a notebook  Youll pick an instance type and image.Sagemaker studio: the ide for machine learning development 39After waiting a little while, youll see the new notebook file. Top right youll see the instance type and the kernel, which default to ml-t3-medium and Python3 respectively.Sagemaker studio: the ide for machine learning development 40You can click on either to change them to one that better suits your execution needs.However, be mindful that changing instance types does not terminate the old instance, so when moving to a different instance make sure to kill the previous one.An easy way of stopping an image is through the sidebar. Clicking shut down in the image in the Running terminals and kernels section will stop the image and all kernels running on it.Sagemaker studio: the ide for machine learning development 41You can stop kernels and terminals independently, too, and reuse the images if you want.Starting an interactive shell  Youll pick an instance type, an image, and a kernel. Once launched, a new interactive shell (outside of a notebook) will be available in the IDE. By default this is Python3.Sagemaker studio: the ide for machine learning development 42Starting an image terminal  If you need to connect directly to the instance that is running your code to install a library or configure something, you can open an image terminal.Sagemaker studio: the ide for machine learning development 43Starting a system terminal  This opens a terminal in the instance hosting your Jupyter notebook. A use case for this is, for example, interacting with your home folder through the command line, or creating custom kernel configurations.Sagemaker studio: the ide for machine learning development 44Create a new text file  If you want to edit text or markdown files, Studio gives you a handy way of creating them. By default, all files are created in the home directory.Sagemaker studio: the ide for machine learning development 45Customizing your environmentBesides the Jupyter server running for each user, each user will be able to spin up ML-optimized instances that come pre-installed with different purpose-specific software.When working with notebooks, well be able to pick and choose:Instance typesSageMaker imagesKernelsInstance types are the size of the virtual machine that will execute our notebook code. There are many sizes to suit all your ML development needs.Fast launch instance types are particularly interesting since they start up and shut down several times faster than regular instance types.SageMaker images are Docker images that come prepackaged with the SageMaker SDK. Available images come with different tools to help with Data Scientists tasks. Images run on instances.Kernels are runtime configurations, usable from notebooks. AWS provides different kernels to take advantage of the optimized image and hardware that kernels run on, so keep this in mind when picking yours. Kernels can also be customized.To create your own custom kernel, simply start an instance. Then, go to File -> New Launcher -> Image terminal.Once in the image terminal, run conda create name <your kernel name> ipykernelPlease note that once you kill your instance, your kernel configuration is lost. So its better to save it to a file and recreate it whenever you need it again.Read more about instance types, images, and kernels:Use Amazon SageMaker Studio Notebooks  Amazon SageMakerAvailable Instance Types  Amazon SageMakerAvailable SageMaker Images  Amazon SageMakerAvailable SageMaker Kernels  Amazon SageMakerCreate a Custom Kernel  Amazon SageMakerGit integrationYou can clone a git repository using a system terminal through the CLI and run a regular git clone command since git comes pre-installed in system images.You can also use the UI to clone your repository: go to Git -> Clone and paste your git projects URL. If its password-protected, youll be prompted for your credentials.To take advantage of the UI that Studio provides for Git, you need to register your repository first. To do this, click in the Git icon in the sidebar, and then Find a repository. Click on the repo you want to register, then come back to the sidebar. Youll be able to visually stage, reset, commit, and push changes with the convenience of a UI.Sagemaker studio: the ide for machine learning development 46Sagemaker studio: the ide for machine learning development 47Sharing your workWere stronger together, and more productive as a team. When working on a data problem with notebooks, youll sometimes want to share your notebook file with a colleague. SageMaker Studio provides two easy ways to do so.Sharing a notebook with another SageMaker Studio UserSharing a notebook with another user is simple: click the Share button in the top-right corner.Follow the dialogue and you will create a shared notebook. You can grant access to shared notebooks to your colleagues by giving them the shared notebook link.Sagemaker studio: the ide for machine learning development 48Shared notebooks are read-only. They may or may not include cell content. If the person that received the link wants to modify them, they need to create a copy of the shared notebook.Sharing also includes the option of including the git repositories being used in development. This way, if youre working with a colleague and need them to make a change to your code and commit it, you can provide the entire repository instead of just the files.Exporting notebookYou can also share notebooks by exporting them to one of the supported file formats.Sagemaker studio: the ide for machine learning development 49Training and experimentingTraining ML models is a highly iterative process. You need to tweak and train them several times to get a well-performing model. Keeping track of which hyperparameters, data, and code produced which model can be a daunting task. Also, at some point, well need to choose the best performing model.SageMaker Studio builds on SageMakers original offer to solve these two problems (tracking experiments and picking the best performing one) with modern UI tools.As a side note: hyperparameters are parameters that are set before the model begins training and determines training behavior. Some common algorithm hyperparameters include epoch and learning rate.Running experimentsSageMaker Studio provides an easy way to track different code, parameters, and configuration combinations. It groups training jobs, algorithms, parameters, input data output, and other metadata in a trial.Trials are organized into named components, so you can better understand the performance of each trial. Trials are not only training. They can also have pre-processing steps. This is why trials are subdivided into trial components.You can find an exercise that lets you train a model and use experiments while doing so here: Track and compare trials in Amazon SageMaker Studio  Amazon SageMakerThis experiment uses the smexperiments library to track training and preprocessing performance. However, that experiment takes quite a bit of time and some money since it runs several training jobs. So heres a conceptual summary of whats going on related with experiments in the exercise:First, an Experiment is createdFor each run of a training job, it creates a new Trial object.When running a pre-processing step, create a new Tracker object and some hard-coded preprocessing information.This particular experiment is using Pytorchs estimators. After configuring estimator metrics, the name of the trial is passed as a parameter to estimator.fit() and its metrics will get automatically registered as part of that trial. Its useful to call this trial component training or something similar.Experiments with SageMaker Autopilot and automatic model tuningExperiments are great when doing iterative model building. But what happens when you have a range of hyperparameters that you want to test out? Automatic model tuning, in a nutshell, works as follows:You define a training jobYou configure your hyperparameters and the ranges that you want to testSageMaker will run randomly chosen combinations of values inside those ranges and run training jobs with it.SageMaker will then use the results of the training job to pick which set of values to use next as hyperparameters.Sagemaker studio: the ide for machine learning development 50Read more about automatic model tuning:How Hyperparameter Tuning Works  Amazon SageMakerPerform Automatic Model Tuning  Amazon SageMakerMonitoring you modelsOne of the biggest advantages of using SageMaker is the ability to deploy trained models in fully managed environments. You can monitor deployed endpoints from SageMaker Studio itself. One of the advantages of this feature is that it helps detect if our models performance degrades over time.Many factors may affect a models accuracy, but two of the most common ones are data drift and concept drift. Data drift occurs when the training data is not representative of the population, which introduces bias to the model. Concept drift is slightly different: training data might be representative at the time of training, but the way the features are interpreted can change over time.Read more on drift:A Primer on Data Drift. When Machine Learning models are not | by Du Phan | data from the trenchesHow monitoring worksThe first step is to start capturing prediction input and output, meaning both the data provided to run inference and the inference result. Both are stored in the S3. This is enabled by creating a data capture configuration and applying it to the deployed endpoint. These results are compared to a baseline, to have a benchmark of the endpoints accuracy. This baseline is a dataset. Usually, the training dataset is recommended to create the baseline. Creating and running a baselining job is needed once. Then, a monitoring job is created to run periodically. This compares the captured inference data with the expectations from the baseline.The result of these monitoring jobs is accessible from the SageMaker Studio endpoint monitoring tab.Sagemaker studio: the ide for machine learning development 51How to enable monitoring with SageMaker StudioIf you have already deployed endpoint and you want to start monitoring it from SageMaker Studio, follow these steps:Go to the Endpoints item in the sidebar and double click on the deployed endpoint you want to start monitoring.Click on the button Enable monitoring.A shared notebook will be opened. Click on the top-right corner button Import notebook.In the executable copy, follow the instructions in each cell.Read more about monitoring deployed endpoints:Deploy Models for Inference  Amazon SageMakerMonitor Amazon SageMaker  Amazon SageMakerCutting costsOne big difference between SageMaker Studio and other notebook environments is that its designed to decouple code, compute and storage. You spin up and shut down compute instances easily and seamlessly, without the need for infrastructure as code or admin privileges.When a notebook user no longer needs it, it can simply shut down every computing resource without losing data or code.Shutting down unused machinesIf youve stopped using SageMaker Studio and you dont have any long-running jobs in any backing instance, the best way to stop all compute resources is to use the following dialogue:File -> Shutdown -> Shutdown AllOther tips to control costsIf you have access to the studio Dashboard, you can also stop instances from there. Just go into a users details, then click on Delete app. Delete is a bit ambiguous in this context though. If deleting the Jupyter server, the users data wont be deleted since its stored independently from the instance in EFS. Everything outside of the home folder will be lost.To monitor your users AWS EFS volume size, you can check your EFS dashboard and see how much its costing you using the EFS calculator.ConclusionWeve explored the main features and core concepts of SageMaker Studio, and provided a quick look of what an integrated development environment for data looks like. Weve also talked about how it can help us in the build stage by keeping track of experiments and improving the training experience. It can also help us maintain deployed models by providing monitoring, and keep costs down by provisioning only the infrastructure that we need at the moment we need it.For further information please visit SageMaker Studio official documentation."
Building a near real-time data warehouse using AWS technologies,"Introduction: The importance of having access to real-time dataIncreasingly organizations need to have access to real-time, or near real-time, data. However, in many cases there are technological challenges in achieving this, such as issues with data quality and the prevalence of legacy platforms. In many technology environments, data is sent to a data warehouse in a batch process that is executed hourly, daily, or executed in some other periodic fashion. This approach is often accepted because most reports dont depend on recent or real-time data to provide useful information. But this isnt the case in all scenarios.For example, here at Globant we worked with an organization in the educational sector. They faced a scenario where students who used their tools were answering quizzes based on single option questions. As soon as they finished the quiz, the teacher needed to see their performance. It wasnt useful for the teacher to wait until the batch process had finished at some later time to see the students results.To help our client obtain real-time data, we set about complementing their data warehouse tool, as part of their AWS infrastructure, with other technologies from Amazon, to define an architecture that enabled them to make data available almost immediately. This meant they did not have to have a scheduled process or a job that had to check if theres new data to send to the data warehouse. So in this article, well explain how we created this architecture, using the example of the educational organization and student quizzes. Its an approach that requires very little coding.The technologies involved in the architectureLets first examine the technologies involved:API Gateway. According to the online documentation, Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the front door for applications to access data, business logic, or functionality from your backend services. API Gateway allows you to easily connect the API to different AWS services such as Kinesis Firehose, Lambda Functions.Kinesis Firehose. According to its documentation, Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics tools. Kinesis Firehose makes ingestion of streaming data into storage systems such as Amazon S3, AWS Redshift, and Amazon Elasticsearch easy.AWS Lambda. Based on the AWS official documentation, AWS Lambda lets you run code without provisioning or managing servers  You can set up your code to automatically trigger from other AWS services or call it directly from any web or mobile app. Basically, you can upload your own code to AWS infrastructure and you dont have to worry about setting up the servers to execute this code.Amazon S3. The documentation says: Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. S3 is used to store any kind of file.Amazon Redshift. According to the Amazon Redshift Cluster Management Guide: Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. The data warehouse is the place used to do reporting and analytics.Kinesis Data Streams. Based on the official documentation: Amazon Kinesis Data Streams is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources. KDS is used to transport data in real-time and can be configured to transport large amounts of information.Building a near real-time data warehouse using aws technologies 6The quiz micro-app first saves the transactional data in its database for application usage. Then, the application sends the data to a REST endpoint using API Gateway that contains a proxy to send the information to Kinesis Firehose. We recognized it was possible for the volume of data generated by the micro application to increase over time, and thats why the usage of API Gateway made sense because the API configuration can be modified to make the switch between calling to Kinesis Firehose or Kinesis Data Streams.Kinesis Firehose allows producers to send streaming data to destinations such as S3, Elasticsearch, and Redshift. Firehose has a limit of 5,000 records/second and 5 MB/second. If the applications volume is higher than Firehoses limit, the API integration can be changed to call Kinesis Data Streams instead of calling Firehose directly. If the API is configured to integrate with Kinesis Data Streams, this technology has to call Kinesis Firehose anyway to save the data in its final destination.Any configuration made to the API Gateway is transparent to the quiz micro-app since the application only cares to call the endpoint and get a successful response.In our example, at the early stage of the project, the volume of data was not very high, so using Kinesis Firehose was a good option. We didnt expect that the volume of data would increase quickly.If you need to transform the data records, Kinesis Firehose can do that by using Lambda functions. Once the Lambda function executes its logic, transformed data is sent back to Firehose. In our project example, we didnt need to use AWS Lambda to make transformations since the data from the micro app was saved as-it-is in Redshifts destination table. However, the usage of Lambda functions helps to enrich and transform data in case its not possible to do so in the data source.Configuration for Kinesis Firehose in the AWS web consoleBuilding a near real-time data warehouse using aws technologies 7Firehose saves the data into a file that is located in an intermediate S3 bucket. The reason for the intermediate bucket is to use the COPY command from Redshift. It performs better using COPY command rather than doing several INSERT commands.One configuration item defines when the data must be sent to Redshift. The data will be saved either when the buffer size reaches a certain size (1-128 MiB) or the buffer interval reaches a certain time (60-900 seconds).Building a near real-time data warehouse using aws technologies 8Other configuration items that are related to Redshift are the cluster name, user name, destination database, and table. You can define which tables columns are going to receive the information. As mentioned previously, data is saved in Redshift via the COPY command and you can define the format of the file. In our case, the format of the payload is JSON and the file stored in S3 is compressed. As long as you define COPY options and columns, the COPY command is generated automatically, so you dont have to worry about writing the command from scratch.Configuration for API Gateway in the AWS web consoleOnce you have created the path, method, and API URL, the integration of the API with Firehose has to be done in the section Integration Request as shown in the following image.Building a near real-time data warehouse using aws technologies 9Since the micro-app needs to deliver several records in the same request, the Action item for Firehose is PutRecordBatch. In case the payload only has one record, the value is PutRecord.Building a near real-time data warehouse using aws technologies 10The next step is to create the request to send the payload to Firehose. On the same page, subsection Mapping Templates, the header Content-Type whose value is application/x-amz-json-1.0 is configured to take the micro-app payload, iterate over the records, and create the request.The API should receive a parameter called delivery-stream which is the name of the Firehose stream. Its mandatory to make the request to Firehose.The template which is written in Velocity iterates over the records and encodes them in base64.You can find the documentation for PutRecordBatch request in this link.Step to read the data from RedshiftFinally, a reporting application such as AWS QuickSight, Tableau, or Power BI can query the results using the connectors that are available for Redshift.Results based on executionsBased on the pipelines previous executions, the data takes around 70 seconds to be saved in Redshift when its sent from the micro application. For higher volumes (2,000 records per second), the data is saved after 110 seconds on average. Redshift can receive the files content in parallel with the COPY command so volume is not a problem to store large amounts to the data warehouse.Conclusions and considerationsWe successfully implemented this architecture with our client in the educational sector. The organization was then able to provide their users with reports containing real-time data. This made a significant difference, enabling teachers to provide better feedback to their students.Based on our experience, I want to share some of our key learnings:The coding effort is very low to implement this architecture. On one side the micro app needs to send the data to the endpoint. On the other side, a Lambda function could be used to transform the data. But if no transformation is needed, the coding effort is even lower.You can do all the configuration for this architecture using the AWS web console or AWS Command Line Interface (CLI).Its key to know the data volume in advance based on Firehose limits (5,000 records/second and 5 MB/second) to determine if the API Gateway can call Firehose directly or Kinesis Data Streams has to be used in the middle to support higher volumes. As mentioned above, the projects initial volume didnt surpass Firehose limits, so we havent yet tested integration with Kinesis Data Streams, but its mentioned in the reference section.The data sent by the micro-app is mapped to a destination table in Redshift. This means the payload has to fit the table structure. If the micro app cant have all fields to be inserted into the table, the optional Lambda function triggered by Firehose can be created to add missing information to fit the destination structure.Buffer conditions in Firehose are very useful to determine how quickly the data is going to be sent to S3, and is determined by the datas volume or buffer time. The minimum buffer size is 1 MB. If your applications volume is equal or greater than 1 MB/sec, you wont wait one minute to have this data stored. If your applications volume is less than 1 MB/sec, the buffer time determines when the data is saved. However, although the buffer configuration depends on volume or time, application volume can make the pipeline save data in a shorter amount of time.ReferencesAmazon ElasticsearchAmazon QuickSightAmazon RedshiftAmazon Redshift ArchitectureAmazon Redshift Management GuideAmazon S3Apache VelocityAPI GatewayAWS Command Line InterfaceAWS LambdaKinesis Data StreamsKinesis FirehosePower BIPutRecordBatch requestReference of COPY command in RedshiftTableauWriting to Kinesis Data Firehose using Kinesis Data Streams"
Building a cognitive machine: AI and cognitive computing,"Change in our world continues to accelerate, most recently driven by cognitive machines.1 Machines are learning to do tasks once thought innately human. With AI and cognitive computing, they beat us at our games, turn photos into paintings, and write passable poetry.Source: https://www. Ibmbigdatahub. Com/blog/cognitive-computing-new-frontier-machine-intelligenceToday, unlike ever before, we assume a machine can do anything a human can. Once weve imagined a new what, we quickly think about how. Before we understand precisely what were trying to solve, we dive head first into solving it.The human tendency to let the how get before the what is well known to software developers. During a job interview, a key skill measured is requirements gathering. When the interviewer poses a problem, she intentionally leaves out the information necessary to answer the question. The interviewee needs to remind themselves to check their assumptions and ask clarifying questions before they start to solve the problem.In traditional software development, algorithms are expected to be 100% accurate.2Consequently, the what is the focal point and drives the how.Machine cognition is completely different. When we employ machine learning, we dont actually understand how the problem is solved. We may have a guess  and we embed our prior assumptions into a models architecture  but ultimately, the parameters of these models are trained on data. And because the problem is difficult and we dont know how its actually solved, we expect the accuracy to never be exactly 100%. 3So what is the right response when facing a problem suited to artificial intelligence? Just like with software development, we shouldnt jump straight into how. We need to focus on requirements gathering. However, the difference is that with machine cognition, the what is less important, and answering why is more important.To illustrate this, lets look at an example that recently crossed my Slack feed:Who has experience with algorithms to identify the chorus within a song?This is undeniably a cognitive problem. Whats more human than recognizing and responding to a songs catchy refrain?4Source: https://imgur. Com/gallery/cu477rsHearing a question like this, I almost cant help but jump directly into technical approaches. Digital signal processing, audio transcription, autocorrelation, sequence-to-sequence models, and so on! I have to wipe away the saliva from my lips and remind myself that I dont really understand why Im trying to solve this yet.First, we need to understand why we need a chorus detection system: What will this system provide to our ultimate business goals?What is this being used for?Does this need to work for a known, finite list of songs? Or against any song that exists now or is written in the future?What is the data form of the input to our system? For example, is the data composed of lyrics in plain text format, audio from studio recordings, or audio captured in a noisy environment?What is the negative consequence of a false positive? What is the negative consequence of a false negative?Where should the computation take place? Should this occur on a smartphone device, a desktop, or in the cloud?The answers to these questions should help our solution take shape. In particular, we should consider what answers allow us to simplify or completely short-circuit the problem.Short circuitrySource: https://media3. Giphy. Com/media/gty2odyq1fih2/giphy. GifIf we need to detect the chorus in 10 songs, the easiest thing to do is label those songs ourselves. This removes all artificial intelligence from the equation and all avenues of failure. Even if this needs to work for all 2,000 songs in a karaoke playlist, its likely to be less expensive and more accurate to label the music with Amazons Mechanical Turk than to pay an engineer to build a detector.SimplificationSource: https://i. Gifer. Com/2eao. GifLets assume that the problem cant be completely short-circuited. It must be a system that automatically works against new music that doesnt exist today.The next step is to think about the data source and how that could simplify the problem. My assumption is that chorus detection is done by evaluating an audio stream. However, that assumption needs to be checked. Can we, instead, ingest the songs written lyrics? If so, that greatly reduces the data scale and removes most noise. In this case, wed be looking for lyrics that repeat (autocorrelation if you want to impress a business stakeholder).Even if the input is audio, a studio recording differs from capturing the music in a crowded bar. We know that audio can be programmatically identified even if there are high background noise levels. 5 However, hearing repeated audio patterns in a noisy background may be challenging.Planning to failSource: https://media. Giphy. Com/media/kjsjlzcvoy8nk/giphy. GifWe believe that our solution will make mistakes. Now, we need to understand the consequences of those mistakes. By understanding what chorus detection is used for, we can optimize our algorithm to our higher-level goals.If this seems abstract, it may be useful to think about a high-stakes prediction, such as testing for a medical condition, such as high cholesterol. Obviously, we want the test to reflect reality perfectly. But if the test produces an incorrect result for you, which would you prefer? It could flag you as having dangerously high cholesterol when you actually have no underlying condition (false positive), or it could tell you that you have a healthy cholesterol level when you actually have a high one. In the case of medical tests, you would likely prefer a false positive (which could be corrected on a follow-up test) to a false negative (which might go unverified, leaving your health at risk).When building our classification models, we can tune for more false positives or negatives. Economists use so-called indifference curves to understand the trade-off between the two outcomes. Data analysts will mention ROC AUC 6 when angling for a promotion.ConclusionsMachine learning is about stirring data into a pile of linear algebra. Machine cognition (artificial intelligence) has a higher goal: to give a machine the same cognitive abilities as humans. In order to deliver value to our customers, we must build machines that reason as we do, solve problems as we do, and fail gracefully as we try to do. To succeed, we must remember the core business problem and constantly reason how our pile of data and linear algebra is one step toward the solution."
The importance of voice in the post-screen era,"Globant released its latest Sentinel Report, digging into the challenges and opportunities of shaping a brands voice. The report examines the driving force and trends behind the rapid adoption of conversational interfaces. Here we delve into some of the context and insights.  The very way that customers interact with brands is changing. Between options of ordering pizza by chatbot to placing buttonless, clickless orders by Alexa, the days of dial now are being eclipsed. What makes the difference in customers experiences with brands nowadays? Their voice.Wait,you might askwhich Voice are we talking about? Voice interfaces or my voice as a brand? Thats the cool thing about Conversational Design: it concerns building a brands voice, in large part by way of voice interfaces. So, we mean Voice in both ways. One serves to build the other, in the post-screen era.Whats this about the post-screen era?Its the era were now in, and theres precedence for this trend. The mouse and the touch-screen, in their respective moments, revolutionized peoples way of interacting with computers. Now, voice technology is altering customers interactions with brands.The post-screen era is where customers are interacting with brands in ways that dont rely onscreens. This includes speculation and research into holographic technologies as well as the more currently available immersive experience by Voice technologies. Voice is forming new patterns in the way we communicate with devices, and poorly designed interactions leave consumers frustrated not just with the device but more devastatingly with the brands behind it. This new era requires a shifting focus, on the behalf of designers and brands alike. Customers have already been shifting; staying relevant as brands depends on our learning from the new patterns and designing even better conversations.But why does this era point to voice?This era heralds voice technology precisely because the user has invoked it. By 2022, research says that 55% of all US homes will own a smart speaker. Thats up from 13% in 2017. As brands that seek to center around the consumer, we have to pay attention to the ways that voice can improve the users overall experience with our digital solutions. It will end up being one of the surest ways to build trust and solve their problems effectively. The distinguishing factor lies in how brands make use of voice.Voice is capable not only of replacing current functionsactions that people already execute by other means, like turning on and off lights, and scheduling a reminder. Voice applications must start with the goal of solving a problem, a need that the user has. But the true value of this technology lies in altering the way we solve that problem for the user, making it even more intuitive and organic than previous technologies allowed. Thats where we confirm the users trust in our brandon the four fronts of integrity, governance, metrics, and evidenceand confidence that we can be the ones to solve their problem.What started as a voice is now a conversationOne of the first notable (and incredibly complex) electronically-powered attempts at replicating the human voice was called the VODER. During its debut at the New York Worlds Fair in 1939, the VODER and its highly trained operator, Ms. Harper, exhibited the ability to reproduce sounds that had been considered so uniquely human. The voice could now be produced without recording human sounds. Fast forward to 2011, when Siri meets the iPhone. A mere 73 years later, the idea of reproducing the human voice found a daily, or even hourly, use. But now it was contextualized within the bounds of a conversation. This added another layer of complexity, and one that still needs to be addressed by designers today. Thats the impetus for conversational design. For more insights, read on: the  latest Sentinel Report focuses on Conversational Design. Worth a read."
Presenting Globants Artificial Intelligence Manifesto,"At Globant our mission is totransform the world, one step at the time. With our capabilities and knowledge around Artificial Intelligence (AI) and digital trends, we can help organizations thrive and change the way they relate to users and employees. In order toincorporate AI in the organization, we must unitewith the client andembrace a common vision, overcoming ethical challenges and social risks. Therefore we needed to form our Artificial Intelligence Manifesto.Artificial Intelligence ManifestoGlobant has defined this AI Manifesto, a set of principles thatstateswhat we believe and encourage. Moreover our goal is to definewhat we doandwhat we dont do with AI. We encourage you to read them, embrace them, and share them.What we will do with AI:Augmented Intelligence:AI should exist to cooperate with humans and to improve humanity. Collaborate with humans on complex tasks and facilitate their work, relieving them from tedious tasks, and elevating them.Respectful data:A data-driven culture means having clean and accurate data, compliant with laws and regulations, guaranteeing privacy and intimacy of all individuals. Meet strict reliability, security, and integrity standards.Fairness:We are responsible for the data and how its use may affect society. We will promote data-driven outcomes that are unbiased in terms of race, ethnicity, gender, nationality, income, sexual orientation, ability, and political or religious beliefs. Avoid biased results as much as possible to eliminate unfair outcomes.Transparent:We are committed to pursue algorithmic accountability. Thus AI products must preempt the risks of user data misuse and protect from imprudent use. Create products that are transparent and open in their purposes and results. Exercise caution by anticipating adverse consequences.Social Contribution:Ensure access to relevant forms of knowledge, promoting fundamental skills and critical thinking among the community. Open, promote, and facilitate access to AI researchfor the community.We will not pursue any AI applications which contravene or may contravene any law or regulation, the public order and good morals, which includes, among any others:1) AI systems developed to spread untrustworthy information.2) The dissemination or misuse of algorithms.3) Misuse of sensitive characteristics such as race, ethnicity, gender, nationality, income, sexual orientation, ability, and political or religious beliefs.4) Implementations that may cause or directly facilitate injury to people.5) Projects thatimplycontravening widely accepted principles of international law and human rights, such as Information security and others.AI is a field that is constantly evolving. Likewise,this Manifesto is a guideline, to be revised and adapted according to the evolution of knowledge and techniques. We will roll with the technology, but remain constant in the ethical standards of international law and human rights.If you are interested in learning more, email us at: AI@globant.com."
Fraud Protection: AI solutions in healthcare,"As ifthe US healthcare system werentcomplex enough these days, fraud is also on the rise. The National Health Care Anti-Fraud Association (NHCAA) estimates that the financial losses due to healthcare fraud are in the tens of billions yearly. This may account for up to ten percent of US healthcare spending, says HealthLeaders Media. Thats why it is crucial that healthcare providers take steps to prevent fraud and malpractice.In an earlier post,we discussed how artificial intelligence (AI) will help healthcare institutions provide value-based care. In this post, we will discuss AI solutions in healthcare and tackle the question of fraud protection by leveraging AI technology.The cost of fraudFraud and counterfeits critically impact healthcare companies costs of operation, liability,and brand image. These extra financial and quality-of-life costs detract time and attention from more critical aspects of healthcare. This is even more so when related to products and services; it could negatively impact patients physical health.Common examples of fraud and abuse in healthcare include the following:Treatment is paid for but not providedFalsified claimsMultiple claims by different providers for the same patient/treatmentStolen patient identities are used for reimbursing medical services never providedMultiple treatments and diagnostics requested but not requiredHow can machine learninghelp?The classical way of dealing with these behaviors requires highly trained people with acute audit senses. They need to catch patterns in distributed, incomplete data. Predictive analytics and big data systems can help prevent fraud. These technologies allow us to identify inaccurate claims, unwarranted procedures, occluded duplications, etc.. Using machine learning, we can begin to leverage these professionals expertise and scale up. Analyzing historical claims, especially when some prove fraudulent, can teach us. We learn what a normal claim looks like, when abnormal claims happen, what conditions they have in common, and plausibly how early we can tellthe likelihood ofa fraudulent claim.Furthermore, we can assign claims and providers a trustworthiness score to avoid costly manual checks. We can also flag low-scoring claims or providers, anomalies, and suspicious billing patterns for detailed revision. Daily, field agents are constantly making repeated decisions with limited information. Machine learning strikes gold precisely in those two areas. It leaves long tail decisions to the algorithm, including previously impossible-to-process sources like social media, news articles, etc.Some AI solutions in healthcare involving machine learning to prevent fraud and reduce costsare:Indicating whether a given claim is a fakePredicting the total cost of following through with a fake claimFinding common factors missing in fraud cases in order to devise a new policyPredictive filtering of the claims validityupon filing to avoidwasting time on suspicious onesIncluding an interactive helper when filing a claim to minimize misfilingRevealing many large-scale trends that are otherwise invisible to local teams because, on the individual level, the signals are soft or undetectableAn example of whatAIcan doInitiatives involving AI, big data analytics, and smart packaging can be game changers for protecting against healthcare fraud. In terms of consumer items, smart packaging is a great option. It allows field agents to check the authenticity of products by scanning certain details and markings on the package. In this way, they can ensure the authenticity of the products and minimize the chances of product tampering. Also, this technology allows for tracking their origin. This helps with compliance, product recall, information for end users, and quality management.The digital journey aheadAs far as artificial intelligence technology and healthcare are concerned, many areas remainleft to explore. Leveraging AI and big data, healthcare providers can take fraud protection to a new level. And if done successfully, this reducesfraud losses andimproves the brand image and overall user experience. Check out Globants Data & AI Studio to learn how we can help you mitigate fraud and implement AI solutions in healthcare.This post is the third in a healthcare series we are working on. Lookherefor the second post."
Globot AI: How Does it Work?,"A few days before the start of the 2018 FIFA World Cup in Russia, Globants Artificial Intelligence and Big Data studios decided to use proprietary technology to predict match results. Weve already developed these technologies, so why not have a little fun with them?We developed Globot AI, an Artificial Intelligence engine that used statistical simulation to predict the results of all 64 matches of the World Cup, based on the data of the players that make up each team. Lets explore how Globot AI worksEach teams data was generated using the FIFA 2018 game database, which is widely available online. This dataset covers 75 characteristics, such as acceleration, agility, ball control, dribbling, and penalties, of almost 18,000 players. These characteristics were used to calculate different offensive and defensive scores for each of the 32 teams that played the World Cup.Once we characterized the teams, we used statistical simulation techniques to obtain the result, for example, of a thousand matches between two given teams. The conditions of each match were simulated by slightly varying the characteristics of the teams in each simulated game, using normal distributions. Once we established the conditions of each game that would be simulated, we simulated the score of each team using Poissons distributions, which are optimal to describe the occurrence of certain events in certain periods of time, such as the goals of team A, since it plays against team B, in a football game.As a result of this experiment, we obtained, for each match between team A and team B, a thousand possible scores that team A will score against team B, and vice versa, which also makes up the same number of possible results for the match. The most common result in that scenario was the result that the Globot AI put forward as a prediction (with its respective percentage over the total number of simulated matches).For the final match between France and Croatia, Globot AI posted a 0-1 prediction, naming Croatia the World Cup winner. According to the simulation, France would end with a score of 0 goals in 392 of 1000 matches, and Croatia would end with a score of 1 goal in 378 of 1000 matches. Analyzing the results in more detail, we see the following graphic representation of the possible results for this match (the x marks the most common result).The most interesting thing about this experiment is not the method used to simulate the matches or whether the predicted result for each match happened. Rather, it was learning what it means to make a statistical inference and how this kind of technique can even help us in the generation of synthetic datasets to feed the training of a certain algorithm.According to Wikipedia says, statistical inference is the process of using data analysis to deduce properties of an underlying probability distribution. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates. This concept differs from that of descriptive statistics, which is solely concerned with properties of the observed data.The problem with statistical inference is that it is only mostly wrong (which can lead to a very deep discussion about philosophical interpretations of probability*). In a nutshell, any statistical inference requires several assumptions, or models. A statistical model is a set of assumptions about the generation of the observed data and similar data, so any limitation of the model is a problem in the inference (and we know that models are inherently less complex than the reality they are trying to explain). In our case, we generated observed data (results of matches) using models that simulate each games conditions and the probability distribution of the number of goals a team can make. We need to understand then the implications of statistical inference to know how to evaluate any prediction based on these techniques.*If anyone wants to engage in a deeper this discussion, I recommend a lecture by the Computation & Cognition Lab of Stanford University, called Probabilistic reasoning and statistical inference.Haldo SpontnTech Manager  AI & Big Data Studios @Globant"
The world cup at Globant: Betting Game and Globot,"In honor of the competitive spirit of the World Cup, Globant Labs developed an internal betting game in which Globers could enter their predictions to every World Cup game and compete among their colleagues.Even though the World Cup was, for most of us, unpredictable, Globers were brave enough to take their chances. A very special player even joined the competition: the AI Globot! So, how did we introduce a bot in the betting game? That is to say, how were there were two machines playing against each other at the same time? Globant is well known for its innovation, and at Globant, we recognize great ideas and the Globers that make them happen. This is how the Globot AI (Glober + Robot + Artificial Intelligence) came to be developed by the Big Data Studio and the AI Studio.Globot AI is an Artificial Intelligence engine that uses statistical simulation to predict the results of the 64 matches of the World Cup, based on the data of 18,000 players who are part of FIFA 2018, as well as almost 75 features of each player. Globot AI made predictions about each game and got 58 points, climbing the leaderboard and getting to second place among all those betting at Globant.With the same number of points, Globers Mara Jose Basavilbaso, Javier Miranda, Ignacio Escudero, and David Palacio Vargas followed the Globot on the leaderboard.The winner was Glober Juan David Marin, from Medelln, Colombia, with a difference of 1 point. More than 2,000 Globers participated in the betting game, but none of them imagined competing against a robot. But at Globant, everythings possible!Congratulations to the Glober Juan David Marin, who won and bet the AI Bot, and to the whole team that developed the AI Globot!"
"Data Driven Companies: Skills, Tools and Culture","By Ana Maloberti, Data Architect @ Big Data StudioIt has been said many times that data is the new oil. However, it is not something you have to dig deep to get, it flows naturally as a result of any business regardless of what product or service a company offers. Getting quality data and its insight is a natural continuous process. It is not an occasional effort, and it should be recognized as such, especially considering the payback this processes entail. A recent study by Boston Consulting Group (BCG) found that data driven organizations can expect to see 20% to 30% EBITDA(1) gains over companies that have not made the transformation. However, many companies are not prepared, or think they are data driven because they have data and a few reports and dashboards but dont see the complete picture. They generally have a piece of the reality without context or meaning. The transformation involves data quality and sharing, IT skills to select the correct tools, analytical skills to get insight, organizational structure, metric design, decision-making processes, and much more, but, most crucially, it is about a culture that acts on data. To really be data-driven an organization must have the right culture and processes in place to drive critical business decisions with these insights and therefore have a direct impact on the business. Analytics is not data-driven if its findings are never seriously considered or acted upon.These initiatives can succeed only if they start small, evolve and are sustainable in time without much further costs. The end result is a unified, complete shared view of the companys reality. If not everyone is on board sharing data and acting upon this shared and complete view, then it is not complete and they are probably making risky decisions. This is why having support from top management is important.In short, data undoubtedly is a key ingredient, but the key is the right people with the right skills and tools to generate impactful insights and a culture to democratize data and drive the business based upon the analysis.https://www.bcg.com/en-au/publications/2017/digital-transformation-transformation-data-driven-transformation.aspx"
Future of Content Generation,"Writing is One of The Most Vital Contributions of The Human Race Starting from documenting our history, to passing on our teachings and information, our intellectual society has been found on the basis of human-written data. Our society has evolved on the foundation of millions of written scriptures.The same humans, with brilliance at par, have now created algorithms, which can create content, without human intervention. The question that comes to mind, is whether machine generated writings will have the same essence of human writing.Will it be biased or honest? But the real question is, are we ready for it?Yes, we are indeed, at least in some cases. Take the example of business operations and intelligence, of any industry. Business reporting requires documenting down thousands of data and analysis. Now, we have BI tools with all sorts of dashboard and visualization features, but still the data popping up, needs to be given a language to be understood and analyzed. Natural Language Generator (NLG) swoops in to fill up this vital gap. It can digest large volumes of data in question and generate reports automatically, thus augmenting the job of business users so they can focus more on high-value tasks and less on menial work.Healthcare companies can use it to document patient data on a real time basis, by analyzing data sent through connected patient monitoring devices.Financial institutions can use it to flash real time stock updates and generate analytical reports.Educational organizations can use it to create assignments for students.Every marketing department can have a NLG tool, to create quick short contents for digital as well as physical media.Customer representation is already using this tool to make bots communicate with customers and is seeing an exponential rise in every sector as days go by.Some popular tools are Narrative Sciences Quill, Arrias Recount and Yseops Savvy. Most of these tools enable sales representatives with information on consumers so that they can sell the right product to the respective customer. Some tools have an inbuilt learning system featured. Companies who use them can train their NLG tools about a certain style or format of writing by feeding it examples.But how about AI as an author or a journalist?Do we dare to let them write an entire news article? How does it work?The Post has used Heliograph, an AI that has to be just connected with a structured data source, and a template with keywords on which the article has to be based. It then mines the relevant data, matches it with the available keywords, maps and arranges the keywords in an order that creates understandable messages. It compiles and structures phrases, paragraphs or articles for publishing in various platform-relevant formats. The question of accountability of data or information can be nullified, as AI tools can verify and authenticate data in numerous ways. Its most useful for journalists and writers in their secondary research about any topic. They can shake off the headache of data mining, data validation and analysis, which can be easily drawn out by AI. AI can be also used to dig and publish local and niche news aimed towards personalized journalism.Does this mean it will replace journalists and writers?Though AIs can give a complete unbiased and honest picture of an event or news, but we will still need the human touch to display the bigger picture derived from current state of events. AI can help a writer with a popular plot line, or a unique story line idea by mining through all the stories ever written online in past or at present, but we still need human writers to give certain philosophies to characters, insightful dialogues and natural elements to draw interpretations and reflections that races, ignites or lulls the mind and heart of readers.Who knows?Now when AI is generating film scripts, and many companies are investing in AI tools for content generation than recruiting new talents, human writing will become history, in a sense."
