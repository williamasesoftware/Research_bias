,article_name,content,file_id
0,Enhance medicine using AI,"During the last decades, AI transformed multiple fields of knowledge; medicine is not out of this transformation. There are many different ways in which we can enhance medicine using AI. In this article, I will introduce you to some of how AI can help discover new drugs, understand the mysteries of cancer, and learn up to one billion relations between different research resources. 
 

 
The first time AI helped humans research was in 2007 when Adam (a robot) generated hypotheses about which genes code for critical enzymes that catalyze reactions in the yeast Saccharomyces cerevisiae. Adam also used robotics to test its predictions in a lab physically. Researchers at the UK universities of Aberystwyth and Cambridge then independently tested Adam'sAdam's hypotheses about the functions of 19 genes; 9 were new and accurate, and only one was wrong. [1] This is only one example of the multiple applications of AI in this field; get ready to learn more! 
 
From understanding cancer to discovering new drugs 
 
AI is turning the drug-discovery paradigm upside down by using patient-driven biology and data to derive more-predictive hypotheses rather than the traditional trial-and-error approach. For example, Boston's Berg biotechnology company developed a model to identify previously unknown cancer mechanisms using tests on more than 1,000 cancerous and healthy human cell samples.  

Another contribution of AI to this field was made by BenevolentBio when they created a platform that took information from multiple sources such as research papers, patents, clinical trials, and patient records.  
Then this information is used to create a representation, based in the cloud, of more than one billion known and inferred relationships between biological entities such as genes, symptoms, diseases, proteins, tissues, species, and candidate drugs. The inference of those relationships uses Natural Language Processing (NLP) techniques. 
Deepchem is a neural network model based on python used to find a suitable candidate in drug discovery aimed to democratize drug discovery, material science, quantum chemistry, and biology.[2] The Neural Network model used is a multilayer perceptron, a neural network where the mapping between inputs and outputs is non-linear. A Multilayer Perceptron has input and output layers, and one or more hidden layers with many neurons stacked together[3].

 
Uses and advantages 
 
Only in the drug development field there are many applications of AI. In this article [4], the authors mapped the uses of AI in drug discovery: 

As you can see, there are a lot of applications in different fields, only in the drug discovery area. The prediction task has a central role in most of the uses. It is one of the preferred tasks of Artificial Intelligence!  
Developing a new treatment can take over a decade and have an estimated cost of about 2.6-billion dollars. Much of that effectively goes down the drain because it includes money spent on the nine out of ten candidate therapies that fail somewhere between phase-1 trials and regulatory approval. With the right effort, AI and machine learning will usher in an era of optimizing quicker, cheaper, and more effective drug discovery. AI can recognize hit and lead compounds and provide a quicker validation of the drug target and optimize the drug structure design.[4] 

 
 
Some challenges 
 
Despite these promising applications, many scientists are unaware of the capabilities of AI. A survey published in February by BenchSci, a start-up in Toronto, Canada, that provides a machine-learning tool for scientists searching for antibodies, found that 41% of the 330 drug-discovery researchers who took part were unfamiliar with the uses of AI [5].   
AI faces some significant data challenges, such as the data's scale, growth, diversity, and uncertainty. The data sets available for drug development in pharmaceutical companies can involve millions of compounds, and traditional ML tools might not be able to deal with these types of data. In addition, access to data from various database providers can incur extra costs to a company. The data should also be reliable and high quality to ensure accurate result prediction.   
Other challenges that prevent full-fledged adoption of AI in the pharmaceutical industry include the lack of skilled personnel to operate AI-based platforms, limited budget for small organizations, apprehension of replacing humans leading to job loss, skepticism about the data generated by AI, and the black box phenomenon (i.e., how the AI platform reaches the conclusions)[4].
 
To conclude 
 
In spite of all the previously listed challenges, Artificial Intelligence will transform the world in the coming years. If we want to prepare, we have to endure each of these problems. Fortunately, AI is not stopping us from finding solutions and answers to different problems. If we look forward to the future, new tools, such as Quantum Computing, will be available to develop chemical simulations [6]. If you are looking to enhance some processes in the pharmaceutical industry with AI help, don't hesitate to contact us! 
 
References 
 
[1]  
[2]  
[3]  
[4]  
[5] go.nature.com/2xarpt3 
[6]",0
1,La inteligencia artificial en beneficio a la humanidad,"Todos tenemos una opinión frente a lo que es inteligencia artificial (en este blog, lo llamaremos AI). El cine y la televisión, como medios de comunicación masiva, nos ha empapado de historias y personajes relacionados a esta y han construido todo un imaginario frente al tema. Así, podemos tomar como ejemplo el universo cinematográfico de Marvel Studios en el cual nos presenta, por un lado, inteligencias artificiales como Visión, superhéroe cuya mente posee los restos de Jarvis, el asistente personal virtual de Tony Stark, y su contraparte infame Ultrón, robot cuyo objetivo es exterminar la raza humana.
En contraste, dentro de nuestra realidad, podemos pensar en sistemas de AI tales como lo son Siri o Alexa, presentes primordialmente en nuestros celulares para asistirnos, o las controversiales cámaras de reconocimiento facial implementadas en china, cuyo uso generó revueltas en Hong Kong por sentir que este tipo de tecnologías pueden llegar a vulnerar la privacidad de la población.
Con esto en mente, junto a nuestro  nos dimos la tarea de preguntarle a nuestros clientes, amigos y hasta familiares su opinión frente a esta tecnología. Si bien, muchas de las opiniones recalcaban ventajas como agilizar procesos, disminuir errores o procesar grandes cantidades de datos, de igual manera hacían énfasis en que esta tecnología competía, desplazaba y reemplazaba al ser humano, dejándonos a merced de esta sin ningún control. Debido a esto, me parece pertinente que nos preguntemos: ¿Está la Inteligencia artificial en beneficio a la humanidad? Antes de dar una respuesta a esta incógnita, debemos conocer un poco a nuestro sujeto de estudio.
 
Abrebocas a la AI
 
En las ciencias de la computación, la AI es la capacidad de una máquina de tener facultades cognitivas semejantes las de un ser vivo, especialmente a las del ser humano. Según ,  que lleva décadas trabajando en esta área, lo define como la simulación del pensamiento humano a través de modelos computarizados, los cuales le permiten entender, razonar, aprender e interactuar a través del uso de datos. Sin embargo, aunque las máquinas tengan estas facultades cognitivas, ellas realmente no pueden igualar en su totalidad a las de un ser humano. Asimismo, según l, el término AI podemos clasificarlo en 3 tipos: Débil, general, y fuerte.
La inteligencia artificial débil (weak AI o Narrow AI en inglés), es una AI que lleva a cabo tareas totalmente específicas como jugar , dar recomendaciones como lo hace Netflix o poder hablar y dictarle comandos a nuestro celular a través de Siri. La inteligencia artificial general (AGI por sus siglas en inglés), es capaz de tener facultades cognitivas al mismo nivel que un ser humano, por lo que podría tener emociones y llevar relaciones.
Y, finalmente, una inteligencia artificial fuerte (ASI por sus siglas en inglés), es capaz de sobrepasar en todo sentido las capacidades de un ser humano. Por lo tanto, y a pesar de que en los últimos años hemos tenido grandes avances dentro de esta área, es claro que seguimos desarrollando AI débiles.
No obstante, ya hemos comprobado, para bien o para mal, el impacto que puede tener esta tecnología en nuestra sociedad. Por un lado, en el año 2014, la compañía estadounidense  desplegó una AI encargada de llevar a cabo el proceso de reclutamiento y contratación de la empresa con el fin de automatizar y optimizar recursos dentro del proceso. Sin embargo, un año más tarde, el equipo se dio cuenta que esta AI tenía  a la hora de seleccionar los aspirantes, pues castigaba aquellas hojas de vida de mujeres y premiaba al de hombres. Otro ejemplo podemos encontrarlo en el documental Coded Bias, la cual expone que esta tecnología puede llegar a ser sexista, racista y opresora si no se entrena y desarrolla de manera correcta.
Por otro lado, tenemos los casos en los que los coches de la marca , los cuales cuentan con un sistema AI de piloto automático, se han visto envueltos en graves accidentes debido al mal uso de este sistema ya que se ha demostrado que estos sistemas no son capaces de responder eficientemente a las condiciones normales necesarias para una conducción real. Por ende, si estas AI débiles están teniendo un impacto adverso para el ser humano, ¿por qué continuar usando esta tecnología? ¿qué mundo distópico nos espera cuando lleguen las AI fuertes?
 
La importancia del diseño en la AI
 
En el caso de Amazon, el equipo técnico y los diseñadores que construyeron esta AI debieron tener en cuenta qué partes de todo este proceso se podían automatizar, dónde intervenía el ser humano y no dejar que la máquina sea quien tome la decisión y defina quien debe ser contratado y quien no. Aun así, el equipo debió tener un entendimiento del contexto, ya que los datos con los que entrenaron a este sistema eran datos que demostraban el sesgo y la preferencia de contratación desigual en género que Amazon desde el 2014 quería empezar a combatir. Por otro lado, Josh Lovejoy en su artículo  describe los alcances actuales de la AI como “esenciales pero insuficientes por sí solas”, puesto que a las AI son una tecnología que tiene un aprendizaje similar a la de un niño. ¿Le daríamos total control a un niño para que maneje nuestros vehículos en las calles de nuestra ciudad? Esto nos lleva al caso de tesla, donde la falta de entendimiento de los usuarios hacia los alcances de esta tecnología, el poco análisis y prevención del comportamiento adverso que tendrían los clientes fue lo que ha ocasionado los graves accidentes que, en algunos casos, han causado muertes.
Repasando estos y otros casos de AI adversas se percibe levemente la falta de un componente que, de haberse tenido en cuenta, hubiesen tenido un mejor impacto: diseño centrado en el ser humano.
Si bien, los sistemas de AI han ganado popularidad abriendo las puertas a las distintas industrias de conocer a fondo a sus clientes, tener una mayor capacidad de análisis de estos para captar y mantenerlos, ¿Está la Inteligencia artificial en beneficio a la humanidad de  los clientes, usuarios, seres humanos? Como lo describió mi compañero Manuel en su blog titulado , el uso de metodologías de diseño centrado en el humano permite abordar problemas complejos y que respondan a la pregunta: ¿cómo podemos hacer del mundo un lugar mejor? A través de esta podemos desarrollar inteligencia artificial en beneficio a la humanidad, con propósito humano y que trasciende de la perspectiva simple de negocio.
Usar esta metodología abre los ojos de quienes participan en el desarrollo de sistemas de AI donde, por un lado, nos hace conscientes para discernir cuándo interviene una AI, cuándo un ser humano, o cuándo deben colaborar ambos; y, por otro lado, permite planear y prevenir los posibles impactos éticos, morales, sociales, culturales y económicos que puede llegar a tener dentro y fuera de quienes usarán la AI.
 
La Inteligencia artificial en beneficio a la humanidad
 
Dentro de , nuestro grupo de AI, promovemos la colaboración equilibrada entre el humano y la máquina, la cual tiene como resultado no solo la automatización de tareas repetitivas, peligrosas o poco agradables para las personas, sino que pretende aumentar y potenciar nuestras capacidades humanas. Wildlife Insights es un proyecto que ejemplifica estos valores. Esta es  en la que investigadores pueden monitorear, analizar y compartir datos en tiempo real sobre el estado de la vida silvestre a nivel global. Esta usa trampas y sensores para adquirir datos y usan AI para clasificar los distintos animales encontrados dentro miles de fotos, permitiendo a los científicos concentrar sus esfuerzos en planear y tomar acción para la conservación de los ecosistemas a nivel global. Otro ejemplo por destacar es , una plataforma creada por investigadores de  que proporciona análisis de radiografías al instante, indicando al usuario qué enfermedad ha detectado, en qué área concentra su predicción y con qué porcentaje de confiabilidad presenta su análisis.
 
Conclusión
 
El propósito de este blog era ir más allá de explicar o mostrar ejemplos de cómo con AI podemos captar y analizar a profundidad las dinámicas entre el cliente y nuestro negocio. Más bien, es una invitación a nuestros pares dentro de la industria y clientes que no debemos construir AI sin un sentido, o por moda. Quiero resaltar la importancia de construir AI con propósito y responsabilidad, manteniendo siempre el componente humano en mente. Finalmente, ¿Está la Inteligencia artificial en beneficio a la humanidad? Solo si desde ahora construimos nuestro futuro con responsabilidad.
 
REFERENCIAS
 
IBM Design for AI. (s. f.). Recuperado 18 de abril de 2021, de 
Education, I. C. (2021, 7 abril). Artificial Intelligence (AI). IBM. 
(2018, octubre). Artificial Intelligence Driven Design (N.o 1). Awwwards.books. 
(s. f.). IBM100 – Deep Blue. Recuperado 18 de abril de 2021, de 
India, S. (2019, 5 noviembre). How Netflix’s Recommendation Engine Works? – Springboard India. Medium. .
Saini, K. (2021, 4 febrero). Natural Language Processing ft. Siri – MyTake. .
BBC News Mundo. (2018, 11 octubre). El algoritmo de Amazon al que no le gustan las mujeres. 
Bogen, M. (2019, 15 octubre). All the Ways Hiring Algorithms Can Introduce Bias. Harvard Business Review. 
Kantayya, S. (productor) y Kantayya, S. (director). (2020). Coded Bias [Documental]. EU.: 7th Empire Media.
Kolodny, L. (2021, 19 abril). «No one was driving» in Tesla crash that killed two men in Spring, Texas, report says. 
Lovejoy, J. (2021, 22 enero). When are we going to start designing AI with purpose? 
Ortiz, M. (2021, 8 abril). Impacto del Design Thinking en las diferentes industrias – HOLISTIC – Design Lab. Holistic Design Lab. 
Wildlife Insights. (s. f.). Wildlife Insights. Recuperado 18 de abril de 2021, de 
Conservation International. (s. f.). Wildlife Insights. Recuperado 18 de abril de 2021, de 
(productor) y Wolff, T. & Yogeshwar, R. (directores). (2019). ¿De qué es capaz la inteligencia artificial? [Documental]. Alemania.: DW.",1
2,Artificial neural networks vs human brain,"Artificial neural networks vs human brain 

Material interesante:


Post information:
Artificial neural networks vs human brain, are they the same?
It's fascinating how our understanding of how the brain works have influenced the creation of AI. However, despite being historically inspired by our human brain, AI has always been designed to function differently from us. It's like the difference between a bird and an airplane - both can fly, but they do it in their own unique ways. 
4 bullet points
Natural learning is our innate mental process for acquiring knowledge and skills, facilitated by the neural architecture and function of our brain.
Natural neural networks inspire the creation of artificial intelligence entities. 
The differences in the structure and processing abilities of our brains and artificial intelligence systems result in significant differences in functionality between humans and AIs.
Despite their differences, natural thinking and AI can complement each other. 

Introduction:

Have you ever stopped to wonder how we learn? As a scientist, I find this absolutely fascinating. Every single thought and action we have ever made is controlled by one of the most incredible organs in our bodies - the brain. And what's even more mind-blowing is that all of humankind's greatest achievements throughout history are based on the simple, yet complex, process of learning. Observing recent history, one of these astonishing breakthroughs has been artificial intelligence (AI). 

We've been able to create some seriously impressive AI entities that can do everything from recreating images and videos to playing games and even having full-on conversations with us! When you consider what we can do with our own brains, it is astounding. The real question is whether or not these machines are thinking. If so, exactly how do they accomplish it? This article will investigate the interesting field of artificial intelligence while contrasting human thought processes with those of these incredible artificial entities.

 ""I think; therefore, I am.""

""I think; therefore, I am."" Rene Descartes. With this simple phrase, this philosopher was able to capture the essence of the power of our thoughts. Basically, our ability to think is what makes us who we are - it's what drives us to do the things we do, feel the way we feel, and connect with others. In other words, everything we are and everything we do starts with that one little thought in our head! It's pretty incredible when you stop to think about it. 

Although there are various ways for us to learn, natural learning is one of the most amazing. It could be interpreted as the mental mechanism we employ to pick up information and abilities without a conscious effort. It allows us to learn new abilities and reach new conclusions by letting us apply our mental abilities such as problem-solving, critical thinking, and decision-making. It also includes various cognitive skills such as perception, attention, memory, and language processing [1]. 

From a biological perspective, our unique ability to think is all thanks to how our neurons interact in our brains. Each neuron is made up of three basic parts, a cell body, where the genetic material is stored, and two extensions, an axon, and a dendrite, that work together to transmit and receive chemical signals throughout neurons by a process called the synapse (fig. 1). The aggregation of millions of neurons is what we recognize today as a natural neural network (NNN) [2]. Around the 1950´s the study of natural learning, cognition, and brain architecture brought up another interesting idea, can we mimic natural learning with an artificial approach? 

Fig 1. Synapse process and neuron architecture in the human brain. Made by Catherine Cabrera

Traveling throughout history - artificial intelligence

Many thinkers throughout history, like René Descartes and John Locke, have debated the central issue of the connection between the body and the intellect, laying the groundwork for the development of cognitive science [3]. But the field of artificial intelligence didn't start to take shape until the middle of the 20th century, with the invention of perceptons. They were designed to mimic the structure and functionality of natural neurons in order to create simple machines that could solve classification problems (Fig. 2). People were quite excited about this technology as they envisioned all of the amazing possible uses it might have. For example, when the Navy was working on a perceptron technology-based electronic computer in 1958, according to a New York Times article, it would not only be able to walk, talk, and see, but also write, reproduce, and be aware of its own existence [4].

Fig 2. Comparison between perceptrons and biological neurons. From: 

And while perceptons were able to take inputs, process them, and produce outputs, their limited application was soon revealed when they failed to solve non-linear problems like XOR functions [5]. It was then that the tantalizing question arose: if we can mimic a single neuron, could we mimic an entire neural network? The relationship between the brain and intelligence sparked a whole new era of exploration, leading to the incredible advancements in AI that we see today. Initially, artificial neural networks (ANN) were made by joining multiple layers of single perceptrons, becoming able to solve pattern recognition problems, however, two obstacles arise; training these networks was not cheap in terms of computational cost and the binary answer (0 or 1) perceptons manage, limit the problems these neural networks can solve [4].

Thankfully, we have overcome these obstacles and now we can design great artificial intelligence entities like ChatGTP, Midjourney, You, Dalle-2, and others. This accomplishment was achieved with the creation of new technologies and ground-breaking theories that have completely changed the architecture and processing abilities of artificial neural networks. For instance, recurrence, or feedback information processes were added to neural networks, which was a significant advance in this area and resulted in the creation of recurrent neural networks (RNNs) (fig. 3). The instauration of this concept in ANNs generated that these networks start to diverge from natural neural networks, as we don't have this process instaurated in our brain. Just consider that in the synapse process, the signal direction is one-way oriented [2]. 

However, thanks to these adaptations, we can now execute a variety of tasks, such as test prediction tasks and language synthesis. Nonetheless, RNNs have certain drawbacks too, such as poor short-term memory and challenging learning curves. Long Short-Term Memory Networks (LSTMs) were developed to address these issues, and they have completely changed the game by enabling RNNs to expand their memory and carry out jobs that need long-term memory (fig. 3). In a more recent story, convolutional neural networks (CNNs), a three-dimensional configuration of artificial neurons, had been created for increasingly more difficult tasks. These networks are primarily employed for activities related to computer vision analog to those you can carry out using your own eyesight (fig. 3) [5].

Fig 3. The architecture of different neural networks. A. Structures of ANN, RNN, and LSTM. Made by Catherine Cabrera. B.  CNN basic structure, from   A-B

It's fascinating how our understanding of how the brain works have influenced the creation of AI. However, despite being historically inspired by our human brain, AI has always been designed to function differently from us. It's like the difference between a bird and an airplane - both can fly, but they do it in their own unique ways. Similarly, AI has its unique way of processing information and making decisions, which sets it apart from our own thought processes. Specifically, AI development has been a data-centric approach, meaning that besides combining science and technology to develop these machines, we have to give data to these AIs in order to teach them how to “see”. For example, just like a biological neuron has dendrites to receive signals, a cell body to process them, and an axon to convey messages to other neurons, for a single perceptron to make a prediction, input channels, a processing stage, and one output channel are necessary to understand data (fig. 2). In our case, the signals we receive are stimuli from the outside world, which we later on convert into chemical signals the brain can process. For AI, data is this entry signal they need in order to function [1,6].

Can AI outthink the human brain?

Nowadays, both fear and curiosity about new AIs technologies cause a typical misconception about the strengths and limitations they have. For instance, observing an AI generating new images from plain text, resuming huge amounts of information, and creating videos, all this in seconds, is scary, basically because we ourselves are not capable of this. But, did you know that you, or even a kid, can probably win in Tic-Tac-Toe against some of the most amazing AIs? And even though artificial neural networks were inspired by the function of our brain, making them somehow similar in concept, important differences in structure and processing capacities cause significant divergence in the functionality they have, as shown in the table below. 
Table 1. Comparison between natural neural networks and artificial neural networks. Made in Canva [4, 7].
From the information exposed before, we can observe that there are some huge differences between NNN and ANN, including size, flexibility, and power efficiency among others. However, comparing just vague numbers or specific aspects isn't enough for understanding the whole picture, as learning is a process that goes beyond the sum of these aspects. 

To give an example, have you ever considered how distinct machines are from people in terms of how they perceive and interpret the world around them? It's similar to equating wisdom with knowledge. Despite having a vast quantity of knowledge in its memory, AI lacks the wisdom to evaluate tricky circumstances that we humans can handle with ease. For instance, it may be simple for us to identify a fuzzy image of an animal, but it may be difficult for an AI system to do so because of the rigid training parameters that are used to create them [8]. Hence, the next time you're in awe of AIs' incredible talents, remember that there are some things that only humans are capable of doing.

Fig 4. Image classification, performed by AI. From )

Exploring this idea, AI's are recognized for performing specific tasks with ease, but, did you know that our ability to multitask is one of the things that makes our brains so incredible? Our ability to multitask is due to the asynchronous and parallel nature of our neurons. Regrettably, artificial intelligence (AI) can't completely match our abilities in this regard because their artificial neuron layers are frequently fully connected. Artificial neurons need weights of zero to represent a lack of connections, in contrast to biological neurons, which are small-world in nature and can have fewer connections between them [2,4,7]. It just goes to show that while artificial intelligence has made great strides, nature still excels at some tasks better than even the most sophisticated machinery! Even more, the strict parameters used to build AIs and their data dependencies make them extremely vulnerable to any software or hardware malfunction. In comparison, even if a segment of our brain gets any damage, it could still function, keeping us alive!

Conclusions

Despite their differences, natural thinking and AI can complement each other in many ways. For example, AI systems can be used to help human decision-making by providing insights and predictions based on large amounts of data. In turn, human cognition can be used to validate and refine the output of AI systems, as well as to provide context and interpret results. So next time you learn something new, take a moment to marvel at the incredible power of your brain and natural learning, and don't underestimate yourself! You are not a machine and you don't need to be. 

References

Criss, E. (2008). The Natural Learning Process. Music Educators Journal, 95(2), 42–46. https://doi.org/10.1177/0027432108325071
Brain Basics: The Life and Death of a Neuron. (2022). National Institute of Neurological Disorders and Stroke. https://www.ninds.nih.gov/health-information/public-education/brain-basics/brain-basics-life-and-death-neuron
Gardner, H. (1987). The mind's new science: A history of the cognitive revolution. Basic Books.
Nagyfi, R. (2018). The differences between Artificial and Biological Neural Networks. Medium. https://towardsdatascience.com/the-differences-between-artificial-and-biological-neural-networks-a8b46db828b7
3 types of neural networks that AI uses. (2019). Artificial Intelligence|. https://www.allerin.com/blog/3-types-of-neural-networks-that-ai-uses
Sejnowski, Terrence J. (2018). . MIT Press. p. 47.  .
Thakur, A. (2021). Fundamentals of Neural Networks. International Journal for Research in Applied Science and Engineering Technology, 9(VIII), 407–426. https://doi.org/10.22214/ijraset.2021.37362
et al. (2015). Explaining and Harnessing Adversarial Examples. Published as a conference paper at ICLR 2015.  https://doi.org/10.48550/arXiv.1412.657",2
3,RPA Invest your time on valuable tasks,"ROBOTIC PROCESS AUTOMATION (RPA) 
invest your time on valuable tasks
Have you ever felt trapped in your tasks? You’re into a never-ending circle where as soon as you finish daily tasks you have to start all over with them. Those repetitive but also necessary activities have taken most daily time.

https://gifer.com/fr/7Ie5
Robotic process automation (RPA) is a technology that develops robots. These robots are able to emulate some human interactions with different digital systems. And so can help with those routinary activities.
But how is it if robots are those big and heavy machines? well, here we talk about the informatics concept of software robots. They can do things like login into applications, extract information and report it. Additionally, it could be faster and has fewer errors completing the tasks.
 How could it help? 
When people think about automation usually appears an image of machines replacing humans. But nothing could be more unrealistic. The key concept of automation is to help. This rules-based technology will take and complete all those 'boring' and high-volume activities. 
So, the less you have to stay on these tasks the more time you have. It gives people the time to use their intelligence and creativity to improve their skills and feel comfortable in their works.  
Having a robotic automation on the right processes could increase productivity and creativity and reduce mental stress. Now we will see a few tips to recognize those processes.
When to use it?
Something to keep in mind with RPA is not all processes can be automated. And some can be but are not useful. So, how to know where it is useful? According to diverse authors there are some important aspects to be considered:
It is a rule-based process (It follows a well-defined number of steps)
It repeats at regular intervals (once a month, everyday) or has a defined trigger (every time an email arrives)
It has a defined inputs and outputs (defined sources)
The RPA solutions will be a long-term solution (It will be executed several times)
And also, is important to identify if the original process needs a redesign. Because a human process with faults could also have an erroneous automation solution. 
Some uses
In recent months, Asesoftware and Equinox AI Lab have implemented this technology in marketing and human recourses processes. This has had an impact reducing the time they spend looking for new opportunities or candidates. The automation gets and organizes the information from different sites and saves the filtered information. So, they have a punctual information to check and save time.
Final thoughts and a step forward

As you may notice RPA is not necessary AI, it takes human process actions and completes them. But there is the concept of intelligent automation. This combines the best of both. RPA actions and AI processing information, thinking.
This gets a new vision having characters or speech recognition and manage unstructured data. With this in mind, future use of RPA with AI could include read emails and give a specific answer related to the client petition. Or it is necessary pass this information to a person who could give a better response. 
In conclusion, RPA is a useful tool to reduce the time we spend in repetitive jobs with defined rules that can be modeled. Not all processes could be automated but the ones which can be, have a great impact on people's lives and industry productivity.

Optional video: https://www.youtube.com/watch?v=gy-KEIK6Cyo


REFERENCES",3
4,DIY People and object detection with Jetson and Python,"Getting started with Jetson Nano and Jetson Inference


Introduction

NVIDIA’s Jetson Nano is one of the simplest prototyping environments for computer vision projects, as it includes a Graphics Processing Unit (GPU) and is designed specifically for the implementation of basic AI applications. At Equinox we have experience with this development platform, particularly in computer vision problems such as face recognition and object detection. In this article, we will show some options for remote controlling so you can start developing right away, as well as the basic installation and configuration of a deep learning library specifically developed for the Jetson Nano.
NVIDIA’s Jetson Nano is one of the most straightforward prototyping environments for Computer Vision projects. It includes a Graphics Processing Unit (GPU) and is designed specifically for implementing basic AI applications. We have experience with this development platform at Equinox, particularly in Computer Vision challenges such as face recognition and object detection. In this article, we will show some options for remote controlling so you can start developing right away, as well as the basic installation and configuration of a deep learning library, developed explicitly for the Jetson Nano. (OK)

SSH connection and VNC

The control and configuration of the Jetson Nano can be done through a remote connection, using  and the VS-code  extension. In this section, we list all the steps needed to configure both tools.
The control and configuration of the Jetson Nano can be done through a remote connection, using  and the VS-code  extension. This section lists all the steps needed to configure both tools.

Installation and Configuration of VNC-Viewer on the Jetson Nano and the main Windows computer

Download Vino (VNC server) on the Jetson Nano.
$ sudo apt update
$ sudo apt install vino
Create a new directory.
$ mkdir -p ~/.config/autostart
$ cp /usr/share/applications/vino-server.desktop ~/.config/autostart
Configure the VNC server.
$ gsettings set org.gnome.Vino prompt-enabled false
$ gsettings set org.gnome.Vino require-encryption false
Set a password
$ gsettings set org.gnome.Vino authentication-methods ""['vnc']""
$ gsettings set org.gnome.Vino vnc-password $(echo -n 'yourPassword'|base64)
'yourPassword' can be replaced with any other for example, if the desired password is abc123, then the command would be:
$ gsettings set org.gnome.Vino vnc-password $(echo -n 'abc123'|base64)
Find and save the Jetson’s IP address.
$ ifconfig
This output shows the different internet connections on the Jetson Nano where: 
eth0 is for Ethernet.
wlan0 is for Wi-Fi.
l4tbr0 is for the USB-mode connection.

Restart the Jetson Nano device.
$ sudo reboot
Download and install VNC Viewer on the main computer.

On VNC Viewer, go to File > New connection > General.
Enter the IP address of the Jetson Nano obtained in step 5, define an identifier, then click “ok”.
After the new connection is created, double-click it. A prompt box will appear, click “Continue” and enter the password that you defined in step 4.

Installation and Configuration VS Code Remote-SSH extension
Download and install VS Code.

On VS Code, go to the left sidebar, click “Extensions”, click on the search bar, and write Remote – SSH, then click “install”.
On VS Code, go to the left sidebar, click “Remote Explorer”, on the Extensions panel click “Add New” and write the following command:
ssh “jetson_nano_Username”@”jetsonNano_IP”
For example, if the username is myjetson and IP is 192.168.101.15, then the command will be:
ssh myjetson@192.168.101.154
Select the SSH configuration file to update, press enter to select the first option, which should contain “user” or “home”.
Select Linux, and enter the password for the Jetson Nano user.

Installing Jetson Inference

In order to efficiently use the Jetson Nano with a deep learning model, we need a library that offers optimized deep model implementations. One of the best options is Jetson Inference, a library that uses TensorRT to efficiently deploy neural networks using graph optimizations and kernel fusion. You can find the full installation and setup guide on their . In short, you need to have your Jetson Nano flashed with JetPack, and then run the following commands to install and build the project:
We need a library that offers optimised deep model implementations to use the Jetson Nano with a deep learning model efficiently. One of the best options is Jetson Inference, a library that uses TensorRT to deploy neural networks using graph optimisations and kernel fusion. You can find the complete installation and setup guide on their . In short, you need to have your Jetson Nano flashed with JetPack, and then run the following commands to install and build the project:
Clone the repo: make sure that git and cmake are installed, then navigate to your chosen folder and clone the project:
$ sudo apt-get update
$ sudo apt-get install git cmake
$ git clone https://github.com/dusty-nv/jetson-inference
$ cd jetson-inference
$ git submodule update --init
Install the development packages: this library builds bindings for each version of Python that is installed on the system. In order to do that, you need to install the following development packages:
$ sudo apt-get install libpython3-dev python3-numpy
Configure the build: next, create a build directory and configure the build using cmake:
$ mkdir build
$ cd build
$ cmake ../
Download the models: Jetson Inference provides a model downloader tool that provides several pre-trained networks that you can easily install:
$ cd jetson-inference/tools
$ ./download-models.sh
Compile the project: finally build the libraries, Python extension bindings and code samples:
$ cd jetson-inference/build
$ make
$ sudo make install
$ sudo ldconfig

With Jetson Inference installed, you can start testing some pretrained detection models such as MobileNet, classification models such as GoogleNet, and semantic segmentation models such as Cityscapes. An example for detectNet, the detection object used in detection models, can be accessed with the following .
With Jetson Inference installed, you can start testing some pre-trained detection models such as MobileNet, classification models such as GoogleNet, and semantic segmentation models such as Cityscapes. An example of detectNet, the detection object used in detection models, can be accessed in the following .
Conclusion

With all remote connections set up and Jetson Inference installed, you can start developing your own computer vision projects in Python. In the next articles, you will see how to create a people detection application using Jetson Inference’s detectNet object, as well as deep learning architectures such as MobileNet and Inception.
With all remote connections set up and Jetson Inference installed, you can start developing your Computer Vision projects in Python. In the following articles, you will see how to create a people detection application using Jetson Inference's detectNet object and deep learning architectures such as MobileNet and Inception.",4
5,Robotics driven by artificial intelligence,"The branch of robotics has advanced significantly over the years, having applications in the industry, domestic services, medical field, among others. However, have you ever wondered why humans are afraid of some robots? Is humanity prepared for robotics driven by artificial intelligence?
Take a look at the following images. How do you feel about them? Maybe neutral? Maybe uncomfortable?
 

 
And what about these? Perhaps the sensations are better than the previous ones.
 

 
The strange but fascinating phenomenon of the Uncanny Valley
The sensations people feel with those images, including discomfort and calm, are caused by a strange phenomenon called Uncanny Valley. This concept was first introduced in the 1970s by Mashiro Mori, who attempted to describe his observations and feelings to robots that looked and acted almost like humans. He hypothesized that as robots appear more humanlike, they become more appealing, and people have more empathy with them, but only to a certain point [1]. When we reach that point (uncanny valley), the person's response would abruptly shift from empathy to revulsion, causing a tendency to be scared of the robot or causing feelings of strangeness or a sense of unease [2]. Besides robots, the phenomenon could appear in virtually created characters, such as the new metaverse.
For more information about the metaverse, you can read this 
Figure 1 shows the uncanny valley graph. The horizontal axis represents the robot (or virtual character) human likeness, and the vertical axis represents our affinity towards it. Nevertheless, the relation between those parameters is strange as the human likeness increases and becomes more evident when the robot moves.
 

Figure 1. Uncanny Valley graph
 
Why is it important to avoid the uncanny valley?
 
Nowadays, the usage of robotics in our daily life is booming [3]; however, whole interaction with machines has not been achieved due to the low affinity and empathy between humans and robots. Their limitations lead to an inevitable rejection when interacting in terms of expressing emotions. Therefore, it is a great challenge for this industry to naturally make robots part of the environment without reaching the uncanny valley.
One of the most visionary applications of robotics is related to education and rehabilitation. These applications are achieved with robots capable of simulating human emotions, which leads to an increase in the man-machine relationship. By accomplishing this, the education or rehabilitation of disabled people is possible since the patient will be able to create a specific empathic bond with his therapist. At the same time, the robot or virtual character will be able to develop his work in a more effective way [4].
Can AI help us to avoid the uncanny valley?
 
Yes, the short answer is yes. And you may be thinking that the answer is obvious, but it's not. Many companies related to this field continue creating super technological robots with considerable sensors, complex computer vision algorithms, advanced joints, etc. But forgetting one crucial but straightforward phenomenon, the uncanny valley, where many robots fall in, even the most advanced [5].
The problem is that it is thought that the more, the better, and this only applies to the use of artificial intelligence, not to the design of the robot. So, for example, when a robot with a complex design tries to emulate emotions with many facial expressions or large movements of its joints, no matter how complex its artificial intelligence models are, the result? The famous but unwanted uncanny valley.
The best example could be Ameca, a robot released in 2022 that uses Automatic Speech Recognition to recognize people's voices, Computer Vision to recognize faces and objects, among others [6]. However, in my opinion, it is a robot that will fail in its interaction with human beings due to its way of expressing emotions.
[embed]https://www.youtube.com/watch?app=desktop&v=JEBnQKxQYtE&ab_channel=SkyhoundInternet[/embed]
 
On the other hand, let's imagine a simple robot that can express emotions with just a few joints or by simple sounds, the empathy towards the robot will increase a lot. But that's not enough.
If we want to reach maximum familiarity with the robot, we need to use artificial intelligence, how? For example, using Automatic Speech Emotion Recognition to listen and understand what people are saying, Facial Emotion Recognition, or Computer Vision.
At the same time, robots could use emotions to accompany what they are saying, giving a personalized treatment depending on that emotion.
To have good examples, we need to find robots that emulate living things' emotions (animals or humans), with minimalist details or even without any human or animal trait. One of the best-known examples could be WALL-E, an animated robot that looks like a robot and can emulate emotions with just his movements and sounds. Another good example is , a little AI-powered robot capable of emulating emotions without any joint, just an LCD screen. 
 
[embed]https://www.youtube.com/watch?v=Qy2Z2TWAt6A&ab_channel=DigitalDreamLabs[/embed]
 
What's next?
 
As mentioned before, the branch of robotics has grown in recent years; although its most considerable daily use continues to be industrial robots, the other branches of robotics have also had significant growth.
What's the problem? Before creating any robot, we need to know its purpose well. If the tasks robots have to perform are related to human interaction and its design and technology aren't focused on that, the only thing that will happen is that there is a repulsion towards the robot by the human, and therefore, robot mission will fail.
To conclude, if you think that robots in the future will look like those in Terminator, I robot, Transformers, Surrogates, or anyone else who might cause discontent, the answer, in my opinion, is no. Instead, they will look like the robots of Star Wars or WALL-E, which show emotions simply and without causing us, terror.
 
References
Caballar, R. D. (2019). What Is the Uncanny Valley? IEEE Spectrum. https://spectrum.ieee.org/what-is-the-uncanny-valley
Mori, M. (2012). The Uncanny Valley: The Original Essay by Masahiro Mori. IEEE Spectrum. https://spectrum.ieee.org/the-uncanny-valley
Patiño, P., Moreno, I., Muñoz, L., Serracín, J. R., Quintero, J., & Quiel, J. (2012). La robótica educativa, una herramienta para la enseñanza-aprendizaje de las ciencias y las tecnologías. Education in the Knowledge Society (EKS). https://www.redalyc.org/pdf/2010/201024390005.pdf
Kaspar’s journey. (2017). Kaspar the Social Robot. 
Rankings-Creepiest-Robots - ROBOTS: Your Guide to the World of Robotics.. Robots.Ieee. 
Engineered Arts Ltd. (2021). AI vs. Human Intelligence. Engineered Arts. https://www.engineeredarts.co.uk/software/artificial-intelligence-vs-human-intelligence/",5
6,NLP la magia de las máquinas que entienden,"INGLÉS


This article NLP: the magic behind machines that understand us, talks about Natural Language Processing or NLP, a branch of knowledge between Artificial Intelligence and Linguistics that aims to give machines the ability to understand the language of humans.
 
NLP involves the analysis of the following:
1. syntax: identifying the syntactic role of each word within a sentence.
Example: in the sentence ""I will buy a candy"", ""I"" is the subject, ""candy"" is the direct object and ""I will buy"" is the verb.
2. Semantics: determining the meaning of words or phrases.
Example: in the sentence ""The engineer went to the customer's office"", the customer's office refers to a place.
3. Pragmatics: establish how the communicative context affects the meaning.
Example: in the sentence, ""David drank wine at the party. It was red."" Red refers to the wine David drank.
 
We perform these three analyses every day, some almost unconsciously. For us, deriving meaning from language turns out to be very simple and is the consequence not only of a process of development and learning but of thousands of years of evolution. Now, think how difficult it can be for a machine which only understands ones and zeros to understand your emotions when writing a review of a movie or a product you have purchased.
Among the most difficult challenges in NLP are dealing with ambiguous expressions, pragmatics and the implicit use of prior knowledge. Some sentences that show this kind of problem are:
- I will be on the beach with Mary reading. Are you reading, are you both reading or is she reading? (Ambiguity).
- We saw John walking. Did you see that John was walking, or did you see John when you were walking? (Ambiguity)
- The first sprint of the project took one month. The term sprint is technical language in project development (Prior knowledge).
Think how difficult it is for a machine to derive meaning from such sentences if we cannot derive meaning clearly.
 
WHAT GOOD ARE MACHINES THAT UNDERSTAND US?
 
Language is the primary way for humans to communicate effectively. Think that you use the language daily to perform tasks, make analyses, convey emotions and perform many other actions and needs of your life. Now think, what would happen if a machine that can perform tasks and activities thousands of times faster than you, could understand you, and not only you but thousands of people at the same time.
It is clear that the implications of a machine that understands natural language are fascinating. Here I will present some applications and systems that make use of NLP.
 
DIALOG SYSTEMS
 
These systems are divided into two main groups: task-oriented systems and chatbots. The former focus on accomplishing valuable tasks, while the latter focus on maintaining a conversation with the user without implementing any specific task.
The following image shows how these types of systems are divided according to whether they can perform practical tasks and maintain a complex social conversation. 
In many business environments, these terms are often confused as all companies ask to develop chatbots when they really want a task-oriented system.
Examples of task-oriented systems are virtual assistants such as Siri, Alexa and Cortana. Chatbots, on the other hand, are characterised by having defined personalities, carrying on entire conversations, and expressing emotions such as fear, temper, and distrust. The trend is to move toward virtual assistants with characters who can handle more complex social conversations yet are knowledgeable on many topics and can perform functional tasks.
 
TEXT CLASSIFICATION
 
This is the process of assigning categories to texts according to their content. Four of the most common applications of text classification are:
- Sentiment analysis: when you want to determine whether a text is negative, positive or neutral, or when you want to determine the predominant emotion in a text. This is widely used to analyse texts in social networks or product and service reviews.
- Classification of texts by topic: the objective is to understand what a given text is talking about or its main subject.
- Language detection: you want to determine a given text's language.
- Intention detection: it is widely used in customer service to classify customer responses according to their intention (interested, not interested, unsubscribe, potential buyer...).
 
INFORMATION EXTRACTION
 
This is the process of transforming unstructured information into structured data. The objective is to find important entities within a text (people, organisations, locations, etc.), discover their relationships, and store them in a data structure such as a relational database. The purpose of this is to structure the information so that better analysis can be performed, information can be reused, and better insights can be generated.
 
Information Retrieval Systems
 
Information Retrieval is the science behind search engines. It aims to bring users relevant information from unstructured sources such as text, images, videos and audio. The quintessential example of this type of system is the Google search engine. However, consider all the benefits your company could have if it had an ""internal Google"" that could bring relevant information to expose all documents, reports, emails and even social networks, according to search criteria that can be written as you normally as you express yourself.
 
AND WHERE'S THE ""MAGIC"" BEHIND IT?
 
All these NLP systems and applications are based on Machine Learning methods that seem, at first glance, very sophisticated. But how intelligent are these systems? To answer this question, you must first understand what is behind these systems. In general, there is no convention or a defined process to attack this type of project; however, here, I will state three tasks that are usually fundamental in many NLP applications.
The first one is to do POS Tagging (Part of Speech Tagging), which aims to assign a tag to each word in a sentence according to its syntactic role.
The second is to perform Dependency Parsing, which aims to analyse the grammatical structure of a sentence by establishing relationships between words.
The third is to perform Named Entity Recognition, which aims to find all relevant entities within a sentence. These entities can be people, organisations or places.
 
HOW ARE THESE TASKS PERFORMED?
 
These three tasks are performed automatically by Machine Learning models. It seems almost ""magic"" that we only have to pass plain text to these models so that they automatically perform these analyses, and we have the labels word by word. However, the reality is that these models are not so magical. All these Machine Learning models are supervised learning, meaning they learn based on many examples we humans must give them. The following example shows how a human must tag the syntactic role of each word (POS tag) in a sentence to provide the machine with an example of how it is done (the tags are in red, and each tag has a meaning):
The/DT grand/JJ jury/NN commented/VBD on/IN a/DT number/NN of/IN other/JJ topics/NNS ./.
 
Note that there is a tag even for the period!
 
This is a terribly tedious process. Now note that the example is only one sentence. For such models to automatically perform these tasks, the machines must learn from millions of sentences that a poor human must manually tag. Still, sounds very sophisticated?
There are already vast amounts of tagged documents freely available; however, the differences in the quality of tagging documents, the number of tagged documents and the variability of type of tagged documents between English and Spanish is abysmal, which makes Machine Learning models that automatically perform these tasks mentioned above much better in English than in Spanish or other langauges.
The truth is that, despite these limitations, very interesting, functional and valuable things can be done. The challenge is to understand what works, what doesn't and why, to know how to use the best tools, optimise their use and try to improve their performance for each specific business case.
 
What is the future?
 
The future dream in this field is to arrive at methods that do not need human examples or, at least, to reduce their dependence significantly. We want machines that do not need millions of examples of labelled word-by-word sentences to learn. 
It is already known that some methods allow machines to learn without needing examples. These are the methods behind the machines that play chess better than the great chess masters or those that beat the best gamers in the world in some video games. But can these methods be extrapolated to other areas of Artificial Intelligence, such as NLP? For example, can we reach machines that just by ""observing"" us talking, can understand what we say?",6
7,Shaping the metaverse,"A few months ago, I received a  newsletter in which, as if it were a chapter in Black Mirror, it affirmed the existence of a mixed physical-virtual universe called Metaverse. With intrigue, I searched on the internet and found out that  rather than a social media one, so they were  to work in it. Another news said that the video game developer and publisher Epic Games, owner of the popular Fortnite, got a  to develop their Metaverse. These companies and many more have followed the same vision. So, what is the metaverse? Why are these tech giants so interested in it? What does AI have to do with all of this? Well, I will tell you how both humans and machines are shaping the Metaverse.
 

 
What is the Metaverse?
 
To give you an idea about Metaverse, I will give you an example with films such as  or . In both, the characters live in an earthly world, like the one we live in. However, in both, the characters are aware that their reality has two worlds or realities which share the consequences of what happens in each other. For example, in Ready Player One, there is a virtual world called Oasis in which you access through gadgets that let you to live and interact with that world. In Avatar, there is the planet Pandora in which humans can transfers their consciousness through a capsule to a humanoid being living on the planet. These films show that what happens in Oasis or Pandora has a direct effect on the reality and it is because both the Oasis and Pandora have been merged into a single entity with our world. Today there is no clear definition to Metaverse but, for me, it is every experience that can be constantly shaping and replicating on both virtual and physical world, reprogramming beings and machines.
 

Ready Player One movie
These films can give a sense about some properties and characteristics that the Metaverse will address in the future and that have been mapped in a , highlighting 9 key characteristics:

Mapping the Metaverse by Wunderman Thompson
 
All these 9 ingredients will allow us to go further in our world, from a passive participation of digital content to a totally active and inclusive one, where each action we make will affect our physical and virtual environment simultaneously.
 
A human - machine tendency
 
But why large companies have invested so much in the development of this Metaverse? It is because the increase in social networks, entertainment, and technological advances such Blockchain and AI that have being immerse in our everyday activities.


The lockdown promoted and reassured the use of digital platforms as alternative means to entertain, socialize, shop, work and more. We have made these digital means part of us. This is reflected with data collected by  of the surveyed say that their now-a-days life is technology dependent. Jon Radoff, an American entrepreneur, explains in our lives through human-machine trends:
Virtual Mainstreaming: People tend to make the digital world to be as real as the physical world. If you've ever played Farmville on Facebook, or were on Haboo making friends, you will sense the feeling of reality in these games. Or to put other examples, Amazon adopted the way of shopping, or Netflix's watching movies from home. We are unconsciously using the digital version of what we do physically in the real world, and we love it. As Beth Kindig writes in her article for Forbes: ""If you experience moments where your virtual life online feels as real as your physical life, then you 've dipped your toe into the idea of a Metaverse.""
 

Avatars at the disco in Haboo
 
Simulating Reality: As Jon Radoff mentions in , we will go beyond the Internet of Things (IoT) to an Internet of Everything. Technological advances let us think that, in the coming years, machines will be able to acquire and process huge amounts of data from our world in real time, allowing the Persistent and Reactive properties mentioned in the . In addition, with the arrival of the , digital twins will allow us to simulate and explore in much more controlled environments better ways to optimize and operate real objects. This gives us the opportunity to simulate how a factory operates like in , how it feels to, or to generate .
 


 
Machine Intelligence: AI as the mainstay of the Metaverse
 
To make real the Metaverse, we need machines to understand us and be reactive to our interaction, and AI has the potential to achieve them. In fact, AI will have a key role being facilitator of interactions, participant within this universe and support for the human being, getting involved in processes of creativity, automation, security, and privacy, becoming our best partner. Think of writers like J. K. Rowling, author of Harry Potter, who was able to construct a thrilling story in a magical world, rich in detail. By reading or watching its movies we may experience the Harry Potter universe to a certain extent. Instead, if we want to create our own universe and let people be part actively on it, AI will support us in creating compelling stories like we could do in  like tools, forming more realistic characters with , or speeding up the construction of entire spaces with .
Moreover, it will not only be enough for us to create this new universe, but it should remain attractive and immersive enough, so people want to participate and interact in it actively.  that will help us understand better our users and continue to create memorable experiences for them. However, as it is sensitive information on the playground, the Metaverse will involve Blockchain. As suggested by the authors of the , this technology mix will potentially ensure the transfer of data from each of the users to the Metaverse, to have traceability of who and why the user data has been used and maintain dynamic parameters that reduce vulnerability during encryption.
 

Illustration about the Promethean AI software
 
References
Calandra, C. (2021, 1 julio). Entering the meta realm. Wunderman Thompson. Recuperado 2 de noviembre de 2021, de https://www.wundermanthompson.com/insight/entering-the-meta-realm
Newton, C. (2021, 22 julio). Mark Zuckerberg is betting Facebook’s future on the metaverse. The Verge. Recuperado 2 de noviembre de 2021, de https://www.theverge.com/22588022/mark-zuckerberg-facebook-ceo-metaverse-interview
BBC News. (2021, 18 octubre). Facebook to hire 10,000 in EU to work on metaverse. Recuperado 2 de noviembre de 2021, de https://www.bbc.com/news/world-europe-58949867
Epic Games. (2021, 13 abril). Announcing a $1 Billion Funding Round to Support Epic’s Long-Term Vision for the Metaverse. Recuperado 2 de noviembre de 2021, de https://www.epicgames.com/site/en-US/news/announcing-a-1-billion-funding-round-to-support-epics-long-term-vision-for-the-metaverse
Epic Games. (s. f.). Fortnite and Travis Scott Present: Astronomical. Recuperado 2 de noviembre de 2021, de https://www.epicgames.com/fortnite/en-US/news/astronomical
Wunderman Thompson Intelligence. (2021, 14 septiembre). New trend report: Into the Metaverse. Wunderman Thompson. Recuperado 2 de noviembre de 2021, de https://www.wundermanthompson.com/insight/new-trend-report-into-the-metaverse?j=61174&sfmc_sub=37405083&l=65_HTML&u=4069500&mid=110005021&jb=9008
Ortiz-Ospina, E. (2019, 18 septiembre). The rise of social media. Our World in Data. Recuperado 2 de noviembre de 2021, de https://ourworldindata.org/rise-of-social-media
Radoff, J. (2021, 1 noviembre). 9 Megatrends Shaping the Metaverse - Building the Metaverse. Medium. Recuperado 2 de noviembre de 2021, de https://medium.com/building-the-metaverse/9-megatrends-shaping-the-metaverse-93b91c159375
Kindig, B. (2021, 3 septiembre). The Key To Unlocking The Metaverse Is Nvidia’s Omniverse. Forbes. Recuperado 2 de noviembre de 2021, de https://www.forbes.com/sites/bethkindig/2021/09/02/the-key-to-unlocking-the-metaverse-is-nvidias-omniverse/?sh=62ca37795e17
NVIDIA. (2021, 1 noviembre). NVIDIA OmniverseTM Platform. NVIDIA Developer. Recuperado 2 de noviembre de 2021, de https://developer.nvidia.com/nvidia-omniverse-platform
NVIDIA. (2021a, abril 13). NVIDIA Omniverse - Designing, Optimizing and Operating the Factory of the Future. YouTube. Recuperado 2 de noviembre de 2021, de https://www.youtube.com/watch?v=6-DaWgg4zF8
Safian-Demers, E. (2021, 25 marzo). Augmented living. Wunderman Thompson. Recuperado 2 de noviembre de 2021, de https://www.wundermanthompson.com/insight/augmented-living
Ankers, A. (2021, 19 octubre). The US Army Is Planning a Huge Experiment With Robot Tanks. IGN. Recuperado 2 de noviembre de 2021, de https://www.ign.com/articles/us-army-robot-tanks-experiment-planned?utm_source=twitter
Jeon, H., Youn, H., Ko, S., & Kim, T. (2021, 3 agosto). Blockchain and AI Meet in the Metaverse. IntechOpen. Recuperado 2 de noviembre de 2021, de https://www.intechopen.com/online-first/77823#B1
Fotos
Epic Records [Travis Scott]. (2020, 26 abril). Travis Scott and Fortnite Present: Astronomical (Full Event Video) [Vídeo]. YouTube. https://www.youtube.com/watch?v=wYeFAlVC8qU
Wunderman Thompson Intelligence. (2021a, septiembre 14). Mapping the Metaverse [Gráfico]. Into the Metaverse. https://www.wundermanthompson.com/insight/new-trend-report-into-the-metaverse?j=61174&sfmc_sub=37405083&l=65_HTML&u=4069500&mid=110005021&jb=9008
Our World in Data. (2019, 18 septiembre). Number of people using social media platforms, 2004 to 2018 [Gráfico]. The rise of social media. https://ourworldindata.org/rise-of-social-media
Haboo. (s. f.). [Avatars at the disco in Haboo]. Haboo. https://www.habbo.es/playing-habbo/what-is-habbo
Jeon, H., Youn, H., Ko, S., & Kim, T. (2021, 3 agosto). Relationship between the real world and the Metaverse. [Gráfico]. Blockchain and AI Meet in the Metaverse. https://www.intechopen.com/online-first/77823#B1
Promethean AI. (2021, 5 marzo). Promethean AI Keynote [Ilustración]. Promethean AI. https://www.youtube.com/watch?v=hA0MsGWvmzs",7
8,Medical utopias between design and AI,"Medical utopias between design and AI

Globally, the healthcare system changed the way users connect to different types of medical services. After COVID-19, the appropriation of telemedicine has manifested itself globally. In 2018, this medicine modality started to be priced at $38.046 million USD and is expected to increase to $103.897 million USD by 2024. (Frontiersin, 2018). 
According to the article How to Measure the Value of Virtual Health Care (2021), before COVID-19, telemedicine care only reached 1% in the United States. At those times, virtual care did not have a direct relationship with a person in the health sector behind a screen, i.e., there was a total disconnection between the virtual ecosystem and face-to-face care.
Moving on to a more specific context, in Colombia, according to figures from MinSalud, on April 30th, 2021, only 4% of the total number of health service provider institutions had enabled the telemedicine modality (MinSalud, 2021). Concerning the number of appointments performed using telemedicine before the pandemic caused by COVID-19, in Colombia, this figure did not reach 50,000 (Delgado, 2020). 
On the other hand, during the period between March 2020 and March 2021, an average of 49.5 million teleconsultations were performed under this modality per month (MinSalud, 2021). Additionally, for the same period of time, 32.5 telehealth consultations were carried out (2.5 per month on average) (MinSalud, 2021).
Due to the pandemic, medical care through teleconsultations increased considerably, undeniably generating significant challenges for the sector. As a result, its operations and technology centers of medical care centers and hospitals have been transformed, and the experience in medical health services has had to face new ways of solving the challenges and needs of users when making use of the medical service virtually.

Opportunities and new needs in telemedicine 

Benefits that telemedicine has brought with it should be emphasized, for example, shortening distances and times for users, more efficient operating processes, and a model of care based on clinical suitability. It means that it is possible to decide which patients should and can be attended by teleconsultation and which can be attended by traditional face-to-face medicine, and who will be able to have hybrid care. In addition, it is possible to decide who will have the ability to monitor and carry out remote controls of medical and surgical procedures; and which communities will be protected by reducing their exposure to hospitals and centers with high potential infectious load, among others (Márquez, 2020).
Thus, the telemedicine service has solved several of the needs initiated from the pandemic in the health service before it. But the use of this new modality has also originated new pains and conditions in the service. It has differences from the traditional care; the interaction between doctor-patient has been weakened, and some users claim that the empathy that doctors have in front of a screen is lower than usual (Márquez, 2020). 
On the other hand, patients also claim their fear of misdiagnosis because telemedicine generates gaps concerning the physical review; it can trigger that the service experience is not optimal for the patient. Many physicians were not trained in this type of care before the pandemic, which generated inconveniences when it came to humanizing a service provided in a purely digital way. 
In addition to this, another pain of this modality is that it requires the population to use smart devices. Still, the country has a population with low monetary acquisition for this type of commodity and high levels of digital illiteracy (Márquez, 2020).  

Design Thinking in healthcare

With new needs, design thinking generates value by promoting ideas that innovate and produce solutions to user pains in the health sector. However, design can intervene in any user medical care touchpoints, either from the medical closer approach, in appointment preparation, following up the treatment, and at the end of the intervention. 
Thanks to in-depth interviews, it is possible to understand the needs of the patient's health service experience to understand their context and generate value solutions consistent with their realities. After this, based on the information collected and preliminary findings, brainstorming, and co-creation sessions, creativity-based techniques are promoted to generate as many solutions as possible to choose the most valuable ones in the end. 
Finally, the aim is to convert these ideas into tangible prototypes to visualize and corroborate these opportunities and reach a successful final result.
The ultimate purpose of applying design thinking to any sector is to improve the user experience in all service touchpoints. To create new dynamics that change some moment of the journey or generate points of contact (both digital and non-digital) that improve or enhance the patient's experience.

Artificial Intelligence in the development of the user experience
 
Now, why use AI to improve the user experience? Because Artificial Intelligence (AI) also has the potential to transform the way healthcare is delivered and can lead to better outcomes and improve productivity and efficiency in service delivery.  
This technology can produce a wide range of improvements, such as better focusing a physician's efforts to create a diagnosis or early detection of the development of more severe conditions before they arise. 
Although AI is rising and its long-term implications are uncertain, its future applications in healthcare delivery and how each of us thinks about our health may be transformative.  
We can imagine a future where data obtained from wearables, portable devices, and implants change our understanding of human biology, enabling personalized, real-time treatment for all. 


Doctor-free testing

Preferential behaviors towards remote monitoring and diagnostic tests are currently trending. In addition, the relevance of algorithms for sorting and classifying stored patient data has led to a change in the dynamics of visiting the doctor (Future Today Institute, 2021).
Thanks to smartphones and smartwatches use, blood pressure readings and electrocardiograms are just a click away. In addition, data recorded and stored in the cloud daily has made it possible to monitor and diagnose people's health status in real-time. (Future Today Institute, 2021).
 ""People who wear an Apple Watch know that an abnormally high or low heart rate or rhythm may suggest atrial fibrillation"" (Future Today Institute, 2021).
Thus, the aforementioned diagnostic devices link to moments of need for contact, consultation, and follow-up/treatment. In other words, the data collected by this type of device in the home will allow quick and timely detection of health problems, generating alerts for the user to inquire more about their medical condition and approach specialists in the sector. 
On the other hand, in this first need for contact, the dynamics of taking action through these devices begin to change. However, when the patient detects atypical data in his devices generated daily, he would contact his health care provider to evaluate the problem in greater detail. 
The intervention of these devices to automatically connect patients with doctors and pharmacists generates less concern for the user to take action regarding their medical condition. Automating these processes will allow for better medical care, more assertive diagnoses, and a faster connection with the entire healthcare ecosystem related to the patient, alleviating the pains mentioned above of teleconsultation, such as the fear of misdiagnosis.
Likewise, the data generated will provide a more efficient medical consultation and a better follow-up of the treatment given; this data will help the medical staff to have an accurate and precise diagnosis of the pathology at the time of the consultation. 
The creation of these devices has generated changes in the way patients interact with their doctors and the healthcare system. However, these devices do not come out of anywhere; they are born from a rigorous process of conceptualization through Design Thinking and the technological development behind it. In short, they are born thanks to methodologies (such as AI4UX from Equinox AI Lab) that unite the best of both fields of knowledge.
AI models could be used to analyze data generated by wearables (for example, a portable heart monitor or a smartwatch) to identify among many measurements points those that could represent conditions of severity to then be analyzed by specialized medical personnel. This approach can optimize the time spent by specialists in these tasks by helping them focus their efforts on analyzing the most critical data, avoiding the review of thousands of measurements that do not represent significant values. 

Pharmacies as health allies

Pharmacies have been the most frequent point of contact for people regarding immediate and fast medical care (Pérez & Silva, 2015). Being family and community spaces, they generate a strategic role in advising and connecting patients with other health services.
If we add to this connection the use of data related to patient's health, the role of these places would be enhanced and with more significant evolution in the sector. For example, the pilot program of CVS Pharmacy, with more than 1,000 branches in the United States, automatically analyzed and detected customers with high rates of non-compliance in the control and management of their health conditions. This information enabled the prioritization of patients for training programs and individual counseling for each pharmacist to prevent or treat chronic diseases for each branch's users (CVS Health, 2021).
This pilot shows a judicious consumption of medicines by patients and a closing of gaps in medical care, an increased focus on preventive medicine, a reduction of unnecessary doctor or emergency services visits, and lower medical costs (CVS Health, 2021).
It is not just a matter of generating data just to develop it. It is essential to give this information a valuable purpose that addresses the needs of patients to provide a higher level of medical well-being. Currently, there is pain in the virtual care of patients, which requires the support of non-specialized medical personnel for the physical care of patients who cannot go to medical health centers due to mobility problems or lack of time or resources. 
At first glance, it is possible to say that this pain could be covered by non-specialized health personnel in pharmaceutical centers in the community. Still, a strategic Design Thinking process would be of great importance to design the whole experience and correctly address every need and opportunity in this situation, from the appointment request to the follow-up of the condition and closure of the process.
In this case, a possible AI solution could be analyzing the different drugs that a person consumes to identify potential diseases that could develop because of their doses and the side effects that the current drugs could create. Therefore, this approach could be a step forward in implementing preventive medicine since it is looking for ways to foresee possible medical conditions so that the user can take the necessary measures. 
 
Conclusion
 
As could be seen in the previous examples, the business opportunities are many, considering that other types of approaches can be developed for these same situations. There are many opportunities for improvement if you look closely at the whole process that a user has, from the moment the patient has a medical appointment to the completion of treatment. Identifying the user's most important pains, and adding the use of technological tools such as Artificial Intelligence, will impact user experience and will optimate resources in the health system enormously. 
In countries where the transition of using new technologies started, those who correctly identify the user's pains and locate the most successful technologies are the ones who will be able to take advantage of all the opportunities that open up by addressing problems using UX.

References 
Referencias 
 
Angela Spatharou, Solveigh Hieronimus, y Jonathan Jenkins. (10 de marzo 2020). Transforming healthcare with AI: The impact on the workforce and organizations. McKinsey & Company.  
CVS Health (2021). Health Trends Report. Recuperado de   
 
Delgado, H. (2020, julio 19). «La pandemia generó una transformación en el sistema de salud»: Presidente de Acemi. El País. Recuperado 29 Mayo, 2022, a partir de  
Fuller D, Colwell E, Low J, Orychock K, Tobin MA, Simango B, Buote R, Van Heerden D, Luan H, Cullen K, Slade L y Taylor NGA. (8 de septiembre 2020). Reliability and Validity of Commercially Available Wearable Devices for Measuring Steps, Energy Expenditure, and Heart Rate: Systematic Review.  
 
Future Tech Institute. (2021). Tech Trends.Miami, Estados Unidos.Recuperado de  
 
MinSalud. (2021a). Cifras Aseguramiento en Salud. Recuperado 29 Mayo, 2022, a partir de  
 
MinSalud. (2021b). Gasto de Salud en Colombia. Recuperado 29 Mayo, 2022, a partir de   
 
Márquez V, Juan Ricardo. (2020). Teleconsulta en la pandemia por Coronavirus: desafíos para la telemedicina pos-COVID-19. Revista colombiana de Gastroenterología, 35(Suppl. 1), 5-16.  
 
Omar Ford. (20 de septiembre 2020). 10 FDA Cleared or Approved Wearable Devices that Redefined Healthcare. Medical Device and Diagnostic Industry.  
 Revista Frontiersin (2020). Telemedicine Across the Globe-Position Paper From the COVID-19 Pandemic Health System Resilience PROGRAM (REPROGRAM) International Consortium (Part 1) . Frontiers in Public Health . Recuperado de",8
9,Random Number Generator,"Random number generators 

What makes a number random?

Pick a random number between 1 and 10. 

Was it 7?  
Humans are really bad at picking random numbers. For instance, choosing 1 or 10 doesn’t seem so random because they are the largest and smallest numbers. A number picked near the middle intuitively feels more random than one at the higher or lower end. Even numbers seem less random than odd ones (though there is no reason for this to be true). 
A true random number is equally likely to be any of the numbers and is completely independent of any previous number chosen. That means if you were to choose a large set of random numbers, each one would appear an equal number of times and it would be impossible to predict with absolute certainty the next few numbers. If you were to keep generating random numbers forever, you could produce any sequence of numbers (although this may take longer than the lifetime of the universe). 

Why do we want random numbers?

For much of human history, random numbers were only used in games of chance. Dice go back 5,000 years . During WW2, random numbers became an important statistical tool for von Neuman when he was working on the Manhattan project  and for use by the Germans in sending encrypted messages. 
From the Manhattan project  came Monte Carlo methods. These are simulations that take a large number of samples from a model using random numbers to compute something that would be difficult to solve otherwise. They are a powerful and ubiquitous tool in physics, economics and data science. It became clear that random numbers were increasingly useful in areas of science, cryptography and statistics. More recently, with the abundance of private information sent over the internet, there is considerable need to generate a large number of random numbers for various encryptions standards .

How do conventional RNGs work

Computers generate random numbers in a deterministic way by taking a number that is close enough to being random (known as the seed) and then performing some iterative process to generate a sequence of numbers that appears random. For simple processes, the seed can be digits from the computers time in milliseconds. 
Different algorithms generate the sequence in different ways. The middle square method used by von Neuman  took a three-digit number, squared it, and took the middle three digits as the next number in the sequence. For instance, starting from 123, you compute 1232 = 15129 and just take 512 as the next number and repeat as long as needed. 
Another method, the linear congruential method, generates the next term by calculating:

Where  is the next term in the sequence,  is the previous term,  is the multiplier,  is the increment and  is the modulus. This sequence will repeat with a period less than m. 
However, these methods are called Pseudorandom number generators (PRNGs) and do not actually produce random numbers. Anyone who knows the seed (initial random number) and the algorithm can generate the entire sequence with complete certainty. If left to run for long enough, both the middle square method and linear congruential method will repeat. This is fine for videogames where it is the feeling of randomness which matters, but not so great for encrypting communications . With insider knowledge of the PRNG, an attacker could decrypt the communication.

Quantum RNG

Thankfully there are ways of generating truly random numbers based on physical processes. Atmospheric noise, the cosmic microwave background (the effect that caused static on old TVs) and radioactive decay are good examples. We can measure radio wave or microwave radiation, or the number of clicks from a Geiger counter.
Quantum computers can also be used for this purpose. They are effectively controlled physical experiments leveraging quantum mechanics to perform some computation. Since randomness is an inherent part of quantum mechanics, quantum computers, unlike classical ones used by von Neuman, can serve as a True Random Number Generator (TRNG) . This is because a quantum system can exist in a superposition of possible states, and following a measurement takes on one of these states. Whilst we can know the probability of the system taking each of these states, we cannot know with absolute certainty which it will take .

Below is an example of how n random numbers (from 0 to nmax) can be generated using IBM’s quantum computer. The code creates a quantum circuit with enough qubits to represent the power of 2 greater than nmax. The qubits are then put into a superposition and measured to obtain the random number. This process is repeated 1000 times and sampled n times to produce the numbers.  


The TRNG implemented here is hardly the most practical implementation. It is rather slow, requires access to IBM’s cloud infrastructure, is vulnerable to interception, and runs on a device cooled to nearly absolute zero. A more useful implementation of this concept was done 20 years ago by the Swiss company ID Quantique, using a photonic chip. Newer models can be integrated in desktop PCs with PCIe connectivity or even USB .

What makes a number random?
 
Pick a random number between 1 and 10.
Was it 7?
Humans are really bad at picking random numbers. For instance, choosing 1 or 10 doesn’t seem so random because they are the largest and smallest numbers. A number picked near the middle intuitively feels more random than one at the higher or lower end. Even numbers seem less random than odd ones (though there is no reason for this to be true).
A true random number is equally likely to be any of the numbers and is completely independent of any previous number chosen. That means if you were to choose a large set of random numbers, each one would appear an equal number of times and it would be impossible to predict with absolute certainty the next few numbers. If you were to keep generating random numbers forever, you could produce any sequence of numbers (although this may take longer than the lifetime of the universe).
Why do we want random numbers?
 
For much of human history, random numbers were only used in games of chance. Dice go back 5,000 years (Piovano, 2011/2016). During WW2, random numbers became an important statistical tool for von Neuman when he was working on the Manhattan project (Metropolis, 1987) and for use by the Germans in sending encrypted messages.
 

From the Manhattan project (Metropolis, 1987) came Monte Carlo methods. These are simulations that take a large number of samples from a model using random numbers to compute something that would be difficult to solve otherwise. They are a powerful and ubiquitous tool in physics, economics and data science. It became clear that random numbers were increasingly useful in areas of science, cryptography and statistics. More recently, with the abundance of private information sent over the internet, there is considerable need to generate a large number of random numbers for various encryptions standards (Zhou & Tang, 2011).
 
How do conventional RNGs work
 
Computers generate random numbers in a deterministic way by taking a number that is close enough to being random (known as the seed) and then performing some iterative process to generate a sequence of numbers that appears random. For simple processes, the seed can be digits from the computers time in milliseconds.
Different algorithms generate the sequence in different ways. The middle square method used by von Neuman (Neuman, 1951) took a three-digit number, squared it, and took the middle three digits as the next number in the sequence. For instance, starting from 123, you compute 1232 = 15129 and just take 512 as the next number and repeat as long as needed.
Another method, the linear congruential method (Thomson, 1958), generates the next term by 

calculating:

Where Xi+1  is the next term in the sequence,  is the previous term,  is the multiplier,  is the increment and  is the modulus. This sequence will repeat with a period less than m.
However, these methods are called Pseudorandom number generators (PRNGs) and do not actually produce random numbers. Anyone who knows the seed (initial random number) and the algorithm can generate the entire sequence with complete certainty. If left to run for long enough, both the middle square method and linear congruential method will repeat. This is fine for videogames where it is the feeling of randomness which matters, but not so great for encrypting communications (Li, 2013). With insider knowledge of the PRNG, an attacker could decrypt the communication.
 
Quantum RNG
 
Thankfully there are ways of generating truly random numbers based on physical processes. Atmospheric noise, the cosmic microwave background (the effect that caused static on old TVs) and radioactive decay are good examples. We can measure radio wave or microwave radiation, or the number of clicks from a Geiger counter.
Quantum computers can also be used for this purpose. They are effectively controlled physical experiments leveraging quantum mechanics to perform some computation. Since randomness is an inherent part of quantum mechanics, quantum computers, unlike classical ones used by von Neuman, can serve as a True Random Number Generator (TRNG) (Jacak, 2021). This is because a quantum system can exist in a superposition of possible states, and following a measurement takes on one of these states. Whilst we can know the probability of the system taking each of these states, we cannot know with absolute certainty which it will take (Nielsen & Chuang, 2000).
Below is an example of how n random numbers (from 0 to nmax) can be generated using IBM’s quantum computer. The code (can be found ) creates a quantum circuit with enough qubits to represent the power of 2 greater than nmax. The qubits are then put into a superposition and measured to obtain the random number. This process is repeated 1000 times and sampled n times to produce the numbers.
 

 
The TRNG implemented here is hardly the most practical implementation. It is rather slow, requires access to IBM’s cloud infrastructure, is vulnerable to interception, and runs on a device cooled to nearly absolute zero. A more useful implementation of this concept was done 20 years ago by the Swiss company ID Quantique, using a photonic chip. Newer models can be integrated in desktop PCs with PCIe connectivity or even USB (ID Quantique, n.d.).
 
References
 
ID Quantique. (n.d.). Quantis QRNG Chip. Retrieved from https://www.idquantique.com/random-number-generation/products/quantis-qrng-chip/
Jacak, M. J. (2021). Quantum generators of random numbers. Sci Rep.
Li, A. (2013). Potential Weaknesses In Pseudorandom Number Generators.
Metropolis, N. (1987). The Beginning of the Monte Carlo Method. Los Alamos Science Special Issue, 15.
Neuman, J. v. (1951). Various Techniques Used in Connection With Random Digits. Res. Nat. Bur. Stand. Appl. Math.
Nielsen, M. A., & Chuang, I. L. (2000). Quantum Computation and Quantum Information. Cambridge University Press.
Piovano, I. (2011/2016). In Logic and Belief in Indian Philosophy. Warsaw Indological Studies.
Thomson, W. E. (1958). A Modified Congruence Method of Generating Pseudo-random Numbers. The Computer journal , 83.
Zhou, X., & Tang, X. (2011). Research and implementation of RSA algorithm for encryption and decryption. Research and implementation of RSA algorithm for encryption and decryption.",9
10,AI Emerges as Crucial Tool for Groups Seeking Justice for Syria War Crimes,"AI Emerges as Crucial Tool for Groups Seeking Justice for Syria War Crimes


BERLIN—By most accounts,  has been the most documented war in history.
But that mammoth trove of evidence—millions of videos, photos, social-media posts and satellite imagery—doesn’t easily translate into accountability for crimes committed during the war.
So as the United Nations, European authorities and human-rights groups build war-crimes cases, they have turned to a novel tool: artificial intelligence.
With the regime of President Bashar al-Assad emerging largely victorious from , efforts to bring about some measure of accountability are gaining speed, largely in European courts.
Since the beginning of Syria’s conflict, activists on the ground risked their lives to document human-rights violations, from torture and attacks on protesters to indiscriminate rocket strikes .
A man inspected damage to a home in rebel-held Douma, Syria, in November 2017.PHOTO: BADRA/EPA/SHUTTERSTOCK
An unexploded cluster bomb that landed in the Douma home.PHOTO: BADRA/EPA/SHUTTERSTOCK
Now, AI and machine learning could play an integral role in bringing war criminals to justice for Syria by helping to sort through the huge trove of evidence, and serve as a model for investigations into other modern-day conflicts.
“You have a use of technology both to disseminate the information, capture it, and now to search it that is suddenly very different and changes the way you work,” said Catherine Marchi-Uhel, who heads the United Nations body tasked with collecting Syrian evidence and building cases.
The technology is aimed at helping process, organize and analyze the data and reduce the time human investigators spend sifting through terabytes of traumatic videos and images. AI algorithms help group videos of the same incident and weed out duplicates or unrelated images. Algorithms are also working on object recognition, finding all data relevant to a specific weapon to help build a case.
In 2017, Hadi al-Khateeb, founder of Syrian Archives, an independent human-rights group that has been archiving evidence since the start of the conflict, wanted to assemble a searchable database on all cluster munition attacks. Mr. Khateeb hopes the database will help build a case that the Syrian regime and its main military backer, Russia, used internationally banned weapons during the conflict.
But it was impossible for Mr. Khateeb’s small team to sort through more than 1.5 million videos by hand to find all those related to cluster bombs.
A model of a cluster bomb in Mr. Harvey’s lab.PHOTO: MARZENA SKUBATZ FOR THE WALL STREET JOURNAL
In some cases, the technology to sort through the evidence exists but is too expensive for human-rights groups. More often, the technology to enable machines to recognize the image or sound of cluster munitions or other weapons of war doesn’t exist and needs to be built.
So Mr. Khateeb turned to Adam Harvey, a software engineer based in Berlin who leads VFRAME, an open-source project focused on using machine learning to advance human-rights work, to build an AI detector capable of such a search.
Mr. Harvey expected a coding task on par with creating algorithms for Google image searches. But soon he realized he was missing a crucial component to train the AI: enough existing images and videos of the cluster bombs.
“In looking for these specific items in human-rights investigations you don’t have as many of these as say cats on the internet,” Mr. Harvey said.
So Mr. Harvey spent more than a year creating synthetic data including 2-D images meant to replicate the environments in Syria and using 3-D models to recreate post-blast videos in various locations around Germany.
After training the program on this data, researchers test the accuracy of the algorithm by running it through a known set of images. They then tweak and retrain the data set to improve it.
In Mr. Harvey’s Berlin apartment, a wood drafting table is covered in gray, 3D-printed replicas of cluster bombs—primarily the AO-2.5RT—that were the most common munitions used in the Syrian war.
By mid-2021, Messrs. Khateeb and Harvey hope to have the database completed and ready to start building a case.
Elsewhere, ’s $40 million AI for Humanitarian Action project and a Silicon Valley nonprofit, Benetech, are also involved in the search for evidence of cluster-munition use.
As part of that effort, Microsoft completed a weapons-identification system code last year that searches and detects videos related to cluster munitions—as well as cannon fire, general explosions, U.S. TOW antitank missiles and sirens—based on sound. Cluster bombs have a distinctive popping sound when they explode.
Early on, human-rights activists believed that showing the international community—which had limited access to the country—what was happening would lead to an intervention against the regime of Mr. Assad. But that didn’t occur.
Now with international attention turning toward accountability, that trove of documentation is proving to be unprecedented in its size and variety in the annals of war-crimes prosecutions, according to the U.N.’s International Impartial and Independent Mechanism.
Catherine Marchi-Uhel heads the U.N.’s International Impartial and Independent Mechanism, the body tasked with collecting evidence and building cases.PHOTO: NICOLAS RIGHETTI FOR THE WALL STREET JOURNAL
But a massive amount of documentation doesn’t easily translate into justice.
“Videos don’t speak for themselves, no matter how great the video seems to be, video is not going to win or lose cases in war crimes,” said Keith Hiatt, who oversees information systems management at the IIIM. “Videos can’t show how widespread and systematic attacks are.”
Without the technology, “there would be no way to do it,” he added.
The IIIM is using machine-learning software to scan through hundreds of thousands of Arabic-language documents—much of them low-quality images—extracting patterns that are relevant to war crimes, such as official stamps, letterheads or signatures.
Keith Hiatt oversees information-systems management at the IIIM.PHOTO: NICOLAS RIGHETTI FOR THE WALL STREET JOURNAL
In building the cases, investigators need to establish not only the evidence that war crimes were committed, often found in videos and photos, but also the chain of command that led to those crimes, often found in documents smuggled out of Syria.
“It’s the [only] way that the U.N. would have a chance to be able to make sense of a conflict…as complex as Syria,” he added. “It’s too big to do it the old way.” Members of the IIIM liken it to searching for a needle in a stack of needles.
Compared with the enormous amount of electronic evidence from the Syrian conflict, the international tribunal for the war in the former Yugoslavia had some nine million documents—enough to fit on one modern hard drive.
The IIIM team working on the Syria case includes “e-discovery officers,” in addition to lawyers, investigators and analysts. Some such as Ms. Marchi-Uhel, a former French judge who worked on the international tribunal for the former Yugoslavia, while others worked on the Enron trial.
In Europe, there are dozens of cases and investigations into crimes committed during Syria’s war but none have relied on AI in building their cases. In October, three groups filed a joint complaint in Germany over two chemical-weapons attacks.
Most of the cases have prosecuted people—often low-level government officers or rebel fighters—for torture or detention and have been based in large part on witness testimony of Syrian refugees now living in Europe, Mr. Khateeb said.
But building bigger cases, including the systematic use of cluster munitions, requires more visual evidence found in the archives of groups like his.
Experts say using machine learning will inevitably raise legal questions and challenges—much like fingerprint and DNA evidence did when it was first introduced—but how those will play out won’t be known until cases where the technology is central begin going to trial.
“With any new type of analysis there will always be some lead time for acceptance,” said Carmen Cheung, executive director of the Center for Justice and Accountability. “And we won’t know what issues can arise in the courtroom until the first case where AI and machine learning has played a role.”
https://www.wsj.com/articles/ai-emerges-as-crucial-tool-for-groups-seeking-justice-for-syria-war-crimes-11613228401",10
11,Simulations with Quantum Chemistry,"The ultimate goal of computational quantum chemistry is to tackle the quantum effects that determine the structure and properties of molecules. Reaching this goal is challenging since the energies associated with these effects are typically a tiny fraction of the total energy of the molecule. One of the applications of computational quantum chemistry is the study and design of drugs that block different stages of the virus's life cycle. These types of drugs are called anti-retrovirals and they specifically bind with and block a virus protein or protease. With the protease blocked, the virus cannot make more copies of itself. It is important to perform chemical simulations to confirm that anti-retrovirals bind with the virus protein. However, such simulations are hard and sometimes ineffective on classical supercomputers.
 
Quantum chemistry for HIV
 
HIV is a virus that has presented a global challenge for public health. This virus has an impact on multiple societal dimensions including nutrition, access to health, education, and research funding. To compound the difficulties,  the virus mutates rapidly with different strains having different geographic footprints.
 

 
Current supercomputers lack the ability to simulate HIV molecules and hence no treatment has been generated so far. However, quantum computers promise more accurate simulations allowing for a better drug-design workflow. For instance, the Variational Quantum Eigensolver (VQE) is an algorithm for finding the ground-state of a molecule and simulate other chemical phenomena.
 
How does the VQE algorithm work?
 
The inputs to the  are a molecular Hamiltonian and a parametrized circuit preparing the quantum state of the molecule. If you’re not a physicist, your most probable reaction will be: “what is that?!” Fortunately, you don’t need to know about quantum physics to understand VQEs. So, let me put it into other words. VQE is a hybrid quantum-classical algorithm, which means that the algorithm consists of two stages: a quantum and a classical stage. The output is an approximation of the combination of values that solve a given optimization problem.
 

 
During the quantum stage, a trial molecular state is created on the quantum computer. The trial state is specified by a collection of parameters which are provided and adjusted by the classical stage. After the trial state is created, its energy is calculated on the quantum computer. During the classical stage, a classical optimization algorithm looks at the previous energy levels and the new energy level and decides how to adjust the trial state parameters.
This process repeats until the energy essentially stops decreasing. The output of the whole algorithm is the final set of parameters that produces the winning simulation of our molecule and its chemical properties. Using this algorithm, scientists could find the anti-retrovirals that block the HIV virus among many other viruses that are computationally expensive to simulate.
 
But… what else?
 
Simulating molecules is only one of the multiple applications that the VQE algorithm offers. Its power is also extended to areas such as  and Artificial Intelligence (AI). These fields rely on processing huge amounts of complex datasets. There is also a need to evolve algorithms to allow for better learning, reasoning, and understanding. While some ML and AI algorithms would take years in a classical supercomputer, a quantum computer would solve it in a matter of seconds using VQE’s. To wrap up, quantum computers are leading the way to the next generation of computers by increasing computational capability and power. With algorithms such as VQE’s, fields such as computational chemistry, AI and Machine Learning will enter a new era of power and speed.
 
References
 
• Pennylane.ai. 2021. A brief overview of VQE — PennyLane. [online] Available at: <https://pennylane.ai/qml/demos/tutorial_vqe.html> [Accessed 3 October 2021].
• 2021. [online] Available at: <https://towardsdatascience.com/the-variationalquantum-
eigensolver-explained-adcbc9659c3a.> [Accessed 3 October 2021].
• Qiskit.org. 2021. Simulating Molecules using VQE. [online] Available at: <https://
qiskit.org/textbook/ch-applications/vqe-molecules.html> [Accessed 3 October
2021].",11
12,Despliegue y arquitecturas en AI,"Lo que debe saber sobre despliegue y arquitecturas de solución en Inteligencia Artificial
 
Este artículo aborda las consideraciones de arquitectura para el despliegue y arquitecturas de solución en AI. Está orientado a personas o equipos que se encuentren interesados en iniciar un proyecto de dicha naturaleza, e incluso también es una guía para aquellos que consideren iniciar en un ambiente de pruebas.
Es importante no perder de vista que la inteligencia artificial hace parte de un sistema más grande y que entre sus funciones está apoyar objetivos de negocio específicos. Desde luego es una ciencia que debe ser integrada con otros sistemas nuevos y legados. Para este caso, vale la pena mencionar que la tarea del científico de datos debe incluir estas preocupaciones.
La inteligencia artificial se centra en  que permitan hacer inferencia sobre escenarios definidos. Las técnicas usadas en este tipo de estudios están cobijadas bajo el término “Data Science”.
 
Ciclos de experimentación en inteligencia artificial
 
 Un ciclo de experimentación en inteligencia artificial se compone de tres acciones fundamentales: idear, programar y experimentar. Los Jupyter Notebooks, por ejemplo, son una herramienta que permite apoyar estos ejercicios de experimentación en ambientes en la nube. En términos generales, los notebooks son documentos que contienen una mezcla de markdown (Lenguaje de marcado para documentación) y código ejecutable en vivo en lenguajes de programación  como Python o R.
Estos ejercicios son interesantes, debido a que permiten generar resultados con gran agilidad, pero pierden visibilidad por numerosas razones, la principal de ellas es que este tipo de ambiente es experimental. Por lo anterior, haré hincapié en algunas consideraciones importantes que hay que tener en cuenta a la hora de desplegar soluciones de inteligencia artificial, con el fin de que se conviertan en una herramienta útil de las organizaciones que deseen implementar esta ciencia.
 
Soluciones de inteligencia artificial
 
Un modelo de machine learning eventualmente se convierte en una caja negra que, dado un input de datos, genera un output con la inferencia (que puede ser una predicción, una clasificación, etc.). Esto pasa ya sea, aplicado a un sistema de predicción como el clásico ejemplo de los precios de las viviendas, que dadas características como el tamaño (área construida o habitable), el número de baños, la localización geográfica entre otras cosas permite predecir el precio del inmueble. También podemos verlo en los ejemplos de visión por computadora como el clasificador de gatos y perros que recibe como input una imagen y determina si contiene un gato o un perro. Así mismo, podemos encontrarlo en un chatbot que recibe el input de conversación de un usuario y retorna la respuesta adecuada de acuerdo a su intención.
 


 
Uso y necesidad de usuarios de soluciones AI
 
Teniendo en cuenta este hecho, es más fácil entender qué se quiere hacer con el despliegue de una solución de AI. Ahora puede pensar más en la forma del input de su modelo, ¿se trata de un sistema de respuesta inmediata, como en un chatbot, con una entrada de texto único? O ¿puede ser un sistema de conteo, donde el usuario puede cargar múltiples imágenes para ser procesadas en batch (procesamiento por lotes)?. Lo anterior pone en evidencia que independientemente del modelo, esta es una característica que está definida por el caso de uso y la necesidad de sus usuarios.
Con esto en mente, es momento de pensar en los recursos necesarios para ejecutar la predicción. ¿Su modelo requiere procesamiento que usa de forma intensiva los recursos de hardware?. Si es así, deberá ejecutar estos procesos sobre la GPU (unidad de procesamiento gráfico). ¿Está su modelo preparado para usarla? ¿Requiere que su predicción se haga en tiempo real? ¿Cuántos usuarios van a realizar predicciones? ¿Cuál es el volumen de los datos que van a pasar por el modelo?
Así, empezamos a encontrar algunas soluciones a las necesidades que de pronto ya está empezando a adoptar y que no tienen relación al entrenamiento de un modelo de machine learning, de redes generativas o de cualquier otra técnica de Inteligencia Artificial. Como lo dije anteriormente, un chatbot requiere respuesta inmediata y un manejo del estado de la intención. En este caso se hace preferible el uso de aplicaciones con estado. Si desarrolló su modelo en Python puede usar un framework como Flask o Django, también puede manejar el estado de la conversación desde el frontend y desplegar su modelo como un servicio sin estado al que debe recordarle constantemente el flujo de la conversación.
 


 
¿Qué es aconsejable para cada modelo?
 
En modelos de predicción y clasificación que no son intensivos en cómputo, es aconsejable manejar servicios web. Los contenedores, vistos como unidades de software que empaqueta el código y sus dependencias, son una forma fácil de desplegar este tipo de modelos en instancias que pueden ser escaladas vertical y horizontalmente. Normalmente los contenedores son desplegados sobre servicios de virtualización de costo continuo.
Si su modelo es para un uso más granular o si se trata de un modelo que se ejecuta como una tarea programada, se recomienda el uso de funciones en la nube. Las funciones en la nube son unidades de cómputo que se cobran por segundos de ejecución, en este tipo de servicios la preocupación no radica en la infraestructura, sino en la eficiencia del código.  Si se diera el uso continuo en el tiempo de este servicio, el costo sería mucho más alto que el de una virtualización tradicional.
 


 
Clusters de predicción
 
Por último, suponga que quiere hacer predicciones sobre los datos haciendo uso de un modelo con altos requerimientos computacionales, donde probablemente la predicción toma bastante tiempo en terminar. Una buena solución para este tipo de requerimientos es la creación de clusters de predicción, que no son más que grupos de máquinas virtuales que cumplen las características necesarias para la ejecución del modelo. Estos clusters son fácilmente escalables horizontalmente. La idea de lo anterior es registrar la tarea de predicción en un datastore (Base de datos, cola, etc.) de su elección, un nodo del cluster se apropia de la tarea y escribe el resultado de la predicción asociado a esta tarea.
Este tipo de arquitecturas de solución también es muy usado para la predicción en batch, por medio del uso de colas y definición de servicios sin estado. Las peticiones de los usuarios pueden cambiar de un servidor a otro. En este caso los servidores, nodos o contenedores de inferencia son vistos como trabajadores cuya función es sacar la siguiente tarea de la cola para hacer la inferencia. Nuevamente el resultado de la inferencia debe ser persistido con el registro de la tarea invocada.
 
Despliegues en la nube
 
Los proveedores de infraestructura en la nube como Azure, Amazon y Google tienen frameworks de desarrollo de machine learning, estos frameworks tienen las herramientas adecuadas para lograr este tipo de despliegues en la nube y se puede llegar a resultados muy interesantes de forma ágil. Sin embargo, la herramienta más útil es el registro de modelos. Un registro de modelos es simplemente un repositorio, donde es posible versionar y aprovisionar los modelos de machine learrning para ser desplegados fácilmente.
 
On the edge
 
Hablamos del despliegue de modelos en infraestructuras en la nube, pero también se puede deplegar modelos “on the edge”. Este tipo de despliegues es altamente usado en arquitecturas para IoT y para dispositivos móviles. Un ejemplo de este tipo de despliegues es el reconocimiento facial, que puede ser desplegado en dispositivos móviles o en dispositivos como Raspberry o Jetson.
Se trata del entrenamiento de modelos de forma tradicional y una conversión del modelo para el despliegue en dispositivos con menor capacidad computacional o con requerimientos energéticos especiales. Esta conversión del modelo desfavorece ligeramente la precisión de la inferencia y se hace posible por medio de librerías como Tensorflow. Sin embargo, no todos los modelos pueden ser convertidos y deben ser muy bien diseñados para lograr este cometido.
En el caso de Computer Vision, la mayoría de estos modelos detectan características genéricas de lo que se desea identificar o son modelos de clasificación aun limitados. No es común ver modelos que clasifiquen una gran variedad de personas en imágenes y que estén disponibles para todos los dispositivos móviles del mercado.
 


 
En conclusión
 
Para finalizar, quiero destacar que las ideas presentadas anteriormente, no son inherentes a esta área de trabajo, por el contrario, funcionan para todos los procesos que puedan consumir una cantidad prudente de recursos computacionales y de tiempo. De tal forma que, espero que pueda llevar a la realidad con mayor claridad las estrategias de su próximo proyecto de despliegue de AI. De la misma manera espero que tenga mayor capacidad de respuesta a inconvenientes que se presenten en la implementación de este.
¿Quiere saber más? Conozca nuestro",12
13,Quantum Computing vs an Old Computer Problem,"Quantum Computing vs an old computer problem

Have you ever been in the supermarket and wondered what was the most efficient path to find all your groceries? Or perhaps on holiday, have you ever wanted to plan a journey going through as many tourist attractions in the shortest route? 


If you only have a few items on your list, this isn’t so hard. You might know the quickest way from the vegetables isle to the bakery, but what if your shopping list had a hundred items on it?
Then it becomes harder. The number of different ways you could plan your route gets really big as the number of items increases. For 5 items on a list, there are 120 different routes you could go. For 10 items, that becomes 362,880 different possible routes!
The Travelling Salesman 
This type of problem is known as the travelling salesman problem (TSP), after its popular description in the 1930s [1] where a salesman has to find the shortest route to visit a list of cities. Variants of the TSP can be seen in many areas of business and science. A shipping company wanting to lower its operating costs might want to find the best route for shipping cargo to many different countries. In X-ray crystallography [2] a detector needs to measure the intensity of X-rays from many different positions on a sample. The order in which these measurements are made doesn’t matter, but repositioning the sample requires moving 4 motors, sometimes for hundreds of thousands of samples. 
For a large number of destinations, or X-ray positions, it would be impractical to go through each possible pathway and find the shortest route (the brute-force approach). What this means is we can only try some of the paths and try to make our way to a good solution (not always the best one). There are a range of different algorithms you can use to do this, , , ,  and  Here we will only consider simulated annealing (SA). 

How to solve the problem
Simulated annealing is an optimisation method inspired by metallurgy, a process that heats and cools a metal to improve its physical properties. To get a solution to our TSP, we start from some random path (we shuffle all the cities on our list), and then make changes to our path for some period of time. First, we calculate the total distance across our path by adding up all the distances from each pair of cities. Then, we propose a new path, which is the same as the old path, but we swap at random two of the cities along the list.  
Depending on whether the new path is longer than the old path, we accept or reject it with a probability. If we accept the new path, the old path is replaced by the new path. We repeat this proposal process for many iterations and eventually we reach a path that is much better than the random one we started with. The diagrams below [3] show an example of annealing for points in a 3D grid. The process starts from the image on the left, and after the annealing process, the right. 

Why quantum?

So where does quantum come in? Quantum annealing (QA) follows a similar approach to SA, but it optimises a quantum system instead of a classical system in the case of SA [5]. To do QA, you need a quantum computer known as a quantum annealer. The main difference between QA and SA is that QA maps our problem (finding the best route) to finding the lowest energy state of a physical system. This allows us to use some phenomena from quantum mechanics to potentially solve our problem faster. 
With SA, the space of possible solutions is sampled one at a time. With QA, we can start from a great many possible paths at once. This is known as a superposition and effectively allows us to consider a greater number of paths at one time. QA also takes advantage of quantum tunnelling, a phenomena in which a quantum system has a probability of escaping local minima. In the diagram we see how this works. Starting from the red path we want to get to the blue path but the paths in between have a significant increase in length. Using SA it is very unlikely for the classical path (green) because the increase in length is so great.  But with QA, we have a probability of tunnelling through (purple) to get to the blue path. 

















Whilst theoretically this is great, there are a few limitations in practice. Firstly, the quantum annealers of today are relatively small scale. As of the time of writing, only a path of 9 cities has been solved [9] which is orders of magnitude less than the  cities that can be done on powerful supercomputers [6]. That said, the company who makes quantum annealers, D-Wave, lists  including traffic flow optimisation and waste collection optimisation for sustainable cities [7].
For more information on the travelling salesman problem, . If you’d like to learn more about quantum annealing, I recommend you .  
References
[1] Grötschel, M., Holland, O. Solution of large-scale symmetric travelling salesman problems. Mathematical Programming 51, 141–202 (1991). https://doi.org/10.1007/BF01586932 
[2] Lawler, E. L. (1985).  (Repr. with corrections. ed.). John Wiley & sons.  .
[3] Original image by Panchotera~enwiki - Own work, CC BY-SA 4.0, 
[4] Arnab Das (Theoretical Condensed Matter Physics Division and Centre for Applied Mathematics and Computational Science, Saha Institute of Nuclear Physics, Kolkata, India) - Quantum Annealing and Other Optimization Methods workshop, 2005
[5] Warren, R.H. Solving the traveling salesman problem on a quantum annealer. SN Appl. Sci. 2, 75 (2020). 
[6]  
[7] Sheir Yarkoni, Florian Neukart, Eliane Moreno Gomez Tagle, Nicole Magiera, Bharat Mehta, Kunal Hire, Swapnil Narkhede, and Martin Hofmann. 2020. Quantum Shuttle: traffic navigation with Quantum computing. In Proceedings of the 1st ACM SIGSOFT International Workshop on Architectures and Paradigms for Engineering Quantum Software (APEQS 2020). Association for Computing Machinery, New York, NY, USA, 22–30. DOI:https://doi.org/10.1145/3412451.3428500",13
14,It´s all about data,"If you are an Equinox AI & Data Lab follower, you've probably seen our blogs and Instagram posts where we talk about some solutions Artificial Intelligence can provide to different problems. For example, do you remember our post about how AI contributes to Wildlife Conservation? If you haven't seen it, you can check it on this link! . 
In this case, conservationists and researchers use AI to monitor and preserve animals in their natural habitat. They need to understand how animals repeat their behaviors, reproductive and migration patterns, or hunting routines. In other words, they need to collect a vast amount of DATA.

Taken from zsl.org
Do you see? That's only one example, but the truth is: it's all about data. As you know, AI teaches a computer how to perform a task that humans would typically perform when given a huge dataset. That means accurate forecasting and patterns wouldn't be possible without quality data.
But how do we know what type of data we need to collect, and how can we manage and analyze that data? We need to refer to the Data Life Cycle model to answer these questions. Based on this model, the process starts planning which information is required in order to solve a problem and how we can collect that data. After collecting it, we process and analyze the data. According to the objective, we can use tools for visualizing data, making predictions with the help of AI and Machine Learning, understanding and observing trends in data, among others. After that, we publish the results and share data with the project's stakeholders. Finally, data is preserved and re-used for maintaining the final product updated, considering the different changes to the original data. This process could be used in academic research, business research, real-world problems, organizations, or any other data-based problem.
DATA IN ORGANIZATIONS
 
We've covered many exciting applications on Equinox's blog in some fields such as environment or arts, but what happens in organizations? It is from data that decisions are made, which is why data is recognized nowadays as the most critical asset an organization has. In our laboratory, we know this, and that's why we work on obtaining, processing, and analyzing data, relying on User Experience to understand our clients' business and thus, use their data to generate value from Analytics and Artificial Intelligence.
However, not all companies have understood the value of the data they handle every day. For some decades now, giants like Google, Meta (Facebook), Amazon, and other tech companies have understood that the value of a product is not in its price. For example, Google provides its search, translation, e-mail, storage services, among others, completely free because the real value is in the users' information. Users are classified according to their personality traits, consumption habits, or their network interaction (visits, clicks, page views, search history, and much more). Then, all this information is sold to advertisers or the government. In other contexts, such as small or medium-sized companies, data is an engine that drives the business' core.
 
WHY IS DATA IMPORTANT FOR ORGANIZATIONS?
 
[caption id=""attachment_2420"" align=""aligncenter"" width=""800""]  Taken from associationsnow.com[/caption]
As mentioned before, data help make better decisions. Even small startups generate data either from customers, user habits, demographics, and more. This information could be used to:
Find new customers
Increase customer retention
Predict sales trends
Those are some examples of how data can benefit organizations externally, but there're many things that can be explored internally. For example, employee churn could be analyzed to determine retention plans for them.
Data helps understanding performance. It is essential to be clear about how the different parts of the company are working: teams, departments, budgets, marketing efforts, etc., and an efficient way to do it is by collecting and tracking data to identify bottlenecks.
Data helps understand and improve businesses processes. In that way, wasted money and time could be reduced.
Data helps understanding customers. When an organization knows its customers, it could be easy to know if the products or services offered are attractive and develop marketing campaigns to retain them or attract potential ones.
In conclusion, the fastest-growing companies are the ones who know and take advantage of their clients' and business' data. This gives them a competitive advantage over other companies and allows them to make decisions based on facts that are inherent to the business.
 
References
Anand, A. (October 6, 2021). How AI is revolutionizing Wildlife Conservation. Recovered on January 6, 2022, from: https://www.analyticssteps.com/blogs/how-ai-revolutionizing-wildlife-conservation
GROW. (March 9, 2020). Why is data important for your business? Recovered on January 7, 2022, from: https://www.grow.com/blog/data-important-business
Leonard, K. (October 25, 2018). The Role of Data in Business. Recovered on January 7, 2022, from: https://smallbusiness.chron.com/role-data-business-20405.html
Images
Capgemini Mexico. (June 27, 2019). Improve your business model with big data. Obtained from: https://www.capgemini.com/mx-es/2019/06/mejora-tu-modelo-de-negocio-con-big-data/
zsl.org. (n.d.). Monitoring and Technology - Machine Learning. Obtained from: https://www.zsl.org/conservation/conservation-initiatives/conservation-technology/machine learning
Smith, E. (June 5, 2019). Data Culture: Why Your Organization Should Think Beyond Big Data. Obtained from: https://associationsnow.com/2019/06/data-culture-why-your-organization-should-think-beyond-big-data/",14
15,Graphics Processing Units – GPU vs FPGA,"Inteligencia artificial Graphics Processing Units – GPU vs FPGA
 
La capacidad de ejecución de hardware para el éxito en implementación de proyectos de AI
 
Actualmente, Inteligencia Artificial es uno de los términos más utilizados en muchas áreas, no solo en entornos de tecnología pura, sino también en economía, leyes, medicina, entretenimiento, entre otros. Uno de los objetivos principales de la inteligencia artificial es mejorar el núcleo del negocio en el que está presente. Tradicionalmente, la mejora de un negocio se lleva a cabo por expertos, sin embargo, no siempre es rápida o eficiente. La Inteligencia Artificial (IA) permite en distintos escenarios delegar las tareas más complejas a una máquina, e increíblemente, sin decirle explícitamente cómo hacerlo.
Sin embargo, la  no es algo nuevo, pues la teoría básica de esta existe hace más de 50 años, hay áreas de IA como el  que hacen un trabajo complejo en términos computacionales. ML y DL son el resultado de algoritmos matemáticos complejos que aprovechan la probabilidad y la estadística aplicadas, buscando soluciones óptimas a problemas específicos.
Hoy en día, esta tecnología está presente en cada área de conocimiento, hace algunos años hemos visto grandes mejoras en las implementaciones de algoritmos. El elemento más relevante es la nueva capacidad de hardware, es decir, las máquinas pueden ejecutar miles de millones de operaciones matemáticas por segundo.
 
Alcance y capacidades: Graphics Processing Units – GPU
 
Un computador está compuesto por procesador (CPU), Memoria (RAM), Almacenamiento (Disco duro) y periféricos (Teclado, Mouse, Pantalla y cualquier dispositivo que se pueda conectar), estos elementos trabajan en perfecta sincronía gracias a la CPU, esta es la encargada de controlar cada una de las partes del computador. *CPU por sus siglas en inglés, Central Processing Unit.
Existe un elemento que no está presente en todos los computadores, de manera general se conocen como dispositivos de refuerzo (Boosting Devices), pues esta es su tarea justamente, son dispositivos especializados en una sola tarea por lo que la realizan excepcionalmente bien y a su vez liberan a la CPU de esta tarea. Las nuevas capacidades en cada una de las partes de estas máquinas han permitido realizar las aplicaciones que hoy usamos de
 

Núcleos de Procesamiento CPU vs GPU
Para ser más precisos, la llegada de elementos de refuerzo como las GPU (Por sus siglas en Ingles, Graphics Processing Units) nos da una gran parte de esas capacidades. Esta es posible debido a la gran cantidad de procesadores dedicados que componen estos dispositivos. Las GPU son hasta ahora el dispositivo de refuerzo más popular, pero no es el único, otra opción para hacer esas toneladas de matemáticas son los FPGA (Por sus siglas en Ingles, Field Programble Gate Array). Estos dispositivos no son realmente populares, porque son más difíciles de configurar.
Por lo general, la eficiencia se mide en la cantidad de operaciones de punto flotante por segundo (FLOPS) que es capaz de ejecutar y la energía necesaria para hacerlo. Una de las mejores GPU es la Tesla V100 que puede realizar 125 Tera FLOPS y utiliza 250 vatios de potencia, por otro lado, una de las mejores FPGA es la Virtex UltraScale+ VU19P que tiene una capacidad de 256 Tera FLOPS para esto requiere 225 vatios. Como se ve claramente en términos de eficiencia relativa, es mejor usar la FPGA con más de 1 Tera FLOP/W que la GPU con 500 Giga FLOP/W. Esa es una declaración que ocurre bajo condiciones desiguales.
Las GPU como se mencionó anteriormente son dispositivos de refuerzo, esto significa que no pueden realizar ninguna tarea por sí mismos, pero son excelentes para mejorar el rendimiento de una máquina que los aloja. Bajo esta claridad, los FPGA no son dispositivos de refuerzo, ya que ellos son capaces de realizar cualquier tarea de manera eficiente. Si estos dos dispositivos se evalúan en igualdad de condiciones, lo correcto sería configurar una FPGA para ser utilizada como dispositivo de refuerzo. Donde evidentemente por capacidad de computo ganaría la FPGA.
 
Entonces, ¿por qué las GPU se usan más que los Field Programable Gate Array FPGA?
Principalmente por la dificultad de programarla / configurarla.
 
Las GPU son dispositivos de arquitectura basados en instrucciones, como las CPU, mientras que las FPGA requieren un lenguaje de descripción de hardware (HDL). Es decir, los primeros están programados con lenguajes como C, Python o Java, los segundos están configurados con VHLD o Verilog. Es necesario aclarar que a través de cualquiera de los lenguajes es posible implementar cualquier cálculo deseado, pero el desarrollo en FPGA sigue siendo un conocimiento más difícil o más especializado.
Existe una alternativa aún más compleja de desarrollar y mucho más eficiente, son los ASIC (por sus siglas en Ingles, Application-Specific Integrated Circuit). No tendremos en cuenta este dispositivo, pues sus costos de fabricación son muy elevados y solo se justifica para producción en masa.


La síntesis de alto nivel (HLS) es una tendencia en este momento, ya que permite la distribución del circuito diseñado en el espacio disponible en el FPGA a través de un proceso de optimización, sin embargo, esta sigue siendo una de las dificultades de desarrollo en estos dispositivos, porque La compilación típica de un programa puede tomar entre 4 y 12 horas.
 
Ventajas de las implementaciones basadas en FPGA
 
Dejando en claro que el tiempo de desarrollo en FPGA puede ser más largo, estos dispositivos nos permiten adaptarnos a nuevas arquitecturas que nos permiten mejorar el rendimiento de nuestra aplicación sin tener que invertir en nuevo hardware. Esto no es posible cuando se usa una GPU, ya que en el fondo es un circuito integrado de aplicación específica (ASIC), por lo tanto, no es posible modificar su arquitectura, que, aunque está optimizado para ciertas operaciones, no es para todos y todavía es un circuito de propósito general para procesamiento de imágenes.
Los FPGA son especialmente buenos si la aplicación requiere baja latencia, lo que significa que el tiempo de respuesta entre el estímulo de entrada y el resultado de salida debe ser muy bajo. En este caso, las GPU son más lentas porque requieren comunicación con la CPU y ejecutar instrucciones. Un buen tiempo de respuesta para una GPU es de 50 microsegundos, mientras que un FPGA puede obtener tiempos de alrededor de 1 microsegundo o menos. Una de las razones de esta baja latencia es que no depende de un sistema operativo o comunicación entre sus partes mediante protocolos estándar como PCIe o USB. Esta baja latencia en gran medida está dada por no tener que usar dichos protocolos y así maximizar el ancho de banda de las comunicaciones.
Finalmente, la comunicación entre sensores o actuadores es más eficiente porque pueden conectarse directamente a los pines del procesador y una vez más la eliminación del uso de protocolos estándar lo que nuevamente permite incrementar el ancho de banda. Adicionalmente las FPGA gracias a su mayor capacidad de memoria Cache pueden almacenar modelos mucho más grandes que una GPU por lo que la eficiencia se ve potenciada.

En resumen, los FPGA son quizás la mejor opción para la implementación de algoritmos que requieren una gran capacidad de computo con una latencia increíblemente baja. Es en estas aplicaciones específicas donde tiene sentido el costo adicional de desarrollo en FPGA, ya que los beneficios son considerables.
 
¿En qué tipo de aplicaciones existentes sería razonable el desarrollo en FPGA?
 
 es actualmente una de las industrias que está optando por esta implementación, porque no solo se obtiene una mejora en el tiempo de ejecución, sino también una reducción en el consumo de energía en comparación con las GPU. Esta mejora se debe principalmente a la naturaleza de las operaciones realizadas allí. Esto permite determinar que, aunque los FPGA tienen un rendimiento más alto que las GPU, esto se debe también a la naturaleza de las operaciones. Las operaciones de punto flotante no son el punto más fuerte de los FPGA, pero las operaciones que realizan modificaciones a nivel de bit se realizan en menos tiempo.
Esto nos permite volver al tema de la inteligencia artificial IA, ya que este es un campo que, beneficiado por la potencia de procesamiento que ofrecen las GPU, es un candidato perfecto para analizar si las operaciones que deben llevarse a cabo se beneficiarán de implementarse en un FPGA. La IA está en auge, por lo que nada está escrito en piedra y el estado del arte se reinventa nuevamente, esto permite que el hardware reconfigurable como FPGA proporcione adaptabilidad a medida que se desarrollan nuevos algoritmos.
Independientemente del algoritmo utilizado o la categoría a la que pertenece (ML o DL), es posible generalizar correctamente los procesos de un algoritmo de inteligencia artificial en dos pasos (una vez que se procesa previamente la información). Estos son entrenamiento e inferencia, el entrenamiento consiste en utilizar la información existente para que el modelo (algoritmo) se adapte de manera generalizada a la información de entrada. La inferencia utiliza el resultado del entrenamiento para determinar un nuevo resultado basado en su entrada.


La inferencia en aplicaciones como la conducción autónoma, la detección de objetos en tiempo real, el seguimiento de objetos en tiempo real o cualquier evento en el que la baja latencia desempeñe un papel determinante, motiva la investigación del desarrollo de arquitecturas de inteligencia artificial en FPGA. Esto no significa que el entrenamiento de modelos no sea posible, esto también tendría una mejora, pero el fuerte de estas implementaciones está en la inferencia.
 
¿Para mi implementación qué es mejor? Graphics Processing Units – GPU vs FPGA
 
Las GPU son más comerciales y no requieren un desarrollo previo para implementar modelos de inteligencia artificial, pues entre desarrolladores y fabricantes trabajan para facilitar el trabajo de los usuarios, el desempeño es óptimo y se han solucionado inconvenientes a lo largo de los años.
El uso de FPGA actualmente se limita a investigaciones o aplicaciones puntuales en donde la baja latencia es un factor importante, por lo tanto, la decisión de que dispositivo usar para una implementación de inteligencia artificial radica en el tiempo y costo de desarrollo dispuesto a invertir, buscando no solo una buena solución, sino la mejor solución posible para cada aplicación.
El desarrollo de herramientas de refuerzo da mayor capacidad de computo por lo tanto permite dar soporte a modelos más complejos y ejecutarlos de manera más ágil, si se desea buscar una GPU que soporte cualquier modelo que este implementando, NVIDIA tiene hoy la batalla ganada en este mercado, por otro lado si le interesa el mundo de las FPGA la solución la tiene Intel, esta compañía trabaja en cerrar la brecha entre estos dispositivos y regresar a la pelea por el mercado del desarrollo de inteligencia artificial.
Las GPU son dispositivos especializados, aunque cumplen con distintas funciones, todas ellas dentro del procesamiento de matrices, es decir, imágenes, video, modelos de IA, entre otros, todos ellos de fácil uso. No es posible modificar la arquitectura interna del dispositivo, por lo tanto, si se requiere una arquitectura más veloz se debe comprar un nuevo dispositivo.
La FPGA permite cualquier tipo de aplicación, obteniendo los mejores resultados posibles, en contra parte se deberá desarrollar por completo cualquier funcionalidad. En caso de desear modificar la aplicación o cambiarla es posible, también modificar la arquitectura interna del dispositivo, por lo cual no requiere una nueva inversión en equipo.
 
Recomendaciones finales
 
Actualmente solo es recomendable realizar un desarrollo en FPGA cuando la aplicación requiere capacidad computacional extremadamente elevada y tiempos de respuesta muy cortos, pues solo cumpliendo con estos requisitos la aplicación sacaría el mayor provecho a una FPGA respecto a una GPU en términos de tiempo y costos. En el resto de situaciones el uso de una GPU es muy apropiado pues reduce los costos respecto a un FPGA y otorga un rendimiento muy superior al prestado por una CPU.",15
16,Does your city have crazy weather AI can help ,"Does your city have crazy weather? AI can help


















Taken from ECMWF:  

It is another beautiful day in your home city, and you see throughout your window up in the blue sky… no sign of rain the whole day. Having concluded that, you apply your sunscreen and decide to wear light clothes and before going out you wonder to yourself: ‘do I need to take out my umbrella? No, why would I put extra weight in my bag without need of that.’ 

The day goes by and just two hours after leaving  your home, in a blink of an eye, it starts to get cloudy and darker and here comes the rain. The cities’ crazy weather is a fact that frustrates a big majority of its citizens each day, nonetheless, AI can come to the rescue by helping the weather forecast field in a short term period.

Forecasting the rain is not just helpful to save yourself from getting wet, but also to a lot of industries which depend on the weather, like outdoor events, aerial services, tourism agencies and farms, among others. The predictions with AI and deep learning methods are being tested for tornadoes’ and hurricanes’ formations as well; by this way, the people living under, or close to the possible affected area, can be informed on time to find refuge.

Just to have a context, in tornado prone regions, the residents have on average an early alert of 16 minutes before the tornado impacts the zone [1]. This time range may be not enough for some people to find a safe place and thus their lives can be in danger. For this specific case, with the data collected from humidity, air pressure and temperature sensors during the previous minutes, with AI, the alert can go up to an hour before the tornado comes [1].



















Taken from Analytics Insight:  

Long-range and short-range forecasting
Now, let’s point out an important difference between two types of forecasting: long and short term.

Long term forecasting refers to the prediction of weather up to a month. This is done by mathematical and physical models aimed to simulate the physics of fluid dynamics weather [2]. These models usually are based on satellite images taken commonly in a year/month window. Because of the volatile and complex weather patterns, the aforementioned models are huge and take a lot of computer resources to be performed.

For short term weather forecasting, also called nowcasting, the window size is reduced up to a few hours [3]. Here the mathematical models can become too complex due to the chaotic behaviour of short climate periods. Is at this point where AI and deep learning are used to simplify the computational calculations and reduce the time on which these are performed.

Where to get the data to build the models?
There are a lot of sources from which scientists collect essential data to build the deep learning models, some more reliable and with more resolution than others. Some of the most used are:

Radiosondes are balloons that measure atmospheric characteristics, such as temperature, pressure and humidity as they move through the air. These radiosondes use a radio signal to communicate the data to a station [4].

Radar stands for radio detection and ranging and consists of sending out radio waves that bounce off the nearest object and return to a receiver. It can sense many precipitation characteristics such as location, motion and intensity [4]. 

Satellites are geostationary objects that rotate with the earth and can be splitted into three categories [4]:
Visible satellites record storms, clouds, fires and smog.
Infrared satellites record clouds, water, land temperatures and features on the ocean.
Water vapor satellites look for the moisture content in the upper half of the atmosphere.
Drones (Still in research) are a new technology used in this field to help with short-range forecasting. They are equipped with pressure, humidity and temperature sensors which generate data sent to a base station.

Drones for weather forecasting
The use of drones on weather predictions is a young field of research that so far has shown promising results. These researches focus the attention to the lowest layer of earth’s atmosphere which is called the boundary layer, here is where most of the earth’s weather is happening. 

Scientists have traditionally used weather balloons or weather stations to collect data and build the models; nonetheless,  for the first tool, there’s the disadvantage of not being able to be fully controlled, and depends on the wind direction. For the second method, these stations cannot be as high as needed and they must be attached to a grounded object [1].

Drones have neither of these limitations and they can flight with headwind and even within a storm. However, because drones can provide higher resolution in terms of the sensed data, this issue creates challenges in both the computational expense and in the complexity of the physics parameterisations required in the forecasting models themselves [6].

One of the ongoing projects is directed by professor Philip Chilson at the University of Oklahoma. He envisions a drone network over a region equipped with a complete set of sensors, these non tripulated vehicles will be launched hourly and instead of causing more complex theoretical models, they will feed with data some AI methods [1].
 















Taken from Nextgov:  

A model approach
The artificial intelligence company DeepMind, owned by Google-Parent-Alphabet, is one with the most advantage of research in weather prediction through AI. In the paper called “Skilful precipitation nowcasting using deep generative models of radar” [5], they found that 56 government meteorologists prefer the AI prediction model instead of other short-term forecasting methods in 89% of the cases.

The DeepMind model predicts up to two hours ahead thanks to weather data coming. The company developed a deep generative model (DGM), which is a statistical model that learns probability distributions of data and allows for easy generation of samples from their learned distributions. The DGM is especially helpful to both learn from observational data as well as represent low uncertainty across a variety of spatial and temporal scales.

The DGM in this case is taken as a probabilistic nowcasting of precipitation that addresses the almost random behaviour of the climate. The methodology used helps to improve the current forecast quality and consistency through predictions over regions near to 1536 km x 1280 km with lead times from 5-90 minutes ahead.

These models are the indicated to predict smaller scale weather phenomena that are inherently hard to predict due to underlying stochasticity, that is critical for nowcasting. The input data to feed the model are radar-based observations of surface precipitation at a given time range and territory area. The mentioned observations keep updating the model at a frequency of 4 frames per 20 minutes. With this, the model is able to sample 18 frames of future precipitations in a period of 90 minutes [5].

The learning process step is driven by two loss functions and a regularization term, which guide parameter adjustment by comparing real radar observations to those generated by the model. The first loss is defined by a spatial discriminator, which is a convolutional neural network that aims to distinguish individual observed radar fields from generated fields, ensuring spatial consistency and discouraging blurry predictions.

The other loss is defined by a temporal discriminator which is a three dimensional convolutional neural network that aims to distinguish captured data from generated radar sequences; it imposes temporal consistency and penalizes jumpy predictions. Together with this, a regularization term is introduced to improve accuracy; this regularizer penalizes deviations at the grid search resolution between the real radar sequences and the model predictive mean. This feature also produces location accurate predictions and improves overall performance.

Speaking now about the training process, this model was prepared with a large dataset of precipitation events, which are 256 x 256 crops extracted from a radar source. The length is around 110 minutes with 22 frames. These images correspond to observations for the UK for years 2016-2018 and evaluated on a test set from 2019.



As a way of conclusion
Throughout all this blog we have been discussing in which way AI is helping the weather forecast and how this will help a lot of economical activities. With the guidance of high resolution data from drones, for instance, the AI models can be fed with better sensed physical variables and thus, they can give more concise results. The way until a highly precise tool for weather prediction is being walked with big steps and soon whether to take out the umbrella or not will not be an issue anymore.

By Favio Acosta.

Quotes
[1] Elizabeth Ciobanu, “How drones are helping with weather forecasting” in Drone blog, Jan. 2022. [Online]. Available:  

[2] Aryan Thodupunuri, “The future of artificial intelligence in weather forecasting” in Towards AI, Aug. 2021. [Online]. Available:  

[3] James J., “How is long-range weather forecasting different than short-range forecasting?” in Socratic, Jul. 2015. [Online]. Available:  

[4] “Collecting weather data” in Lumen. [Online]. Available:  

[5] Suman Ravuri, Karen Lenc et al., “Skilful precipitation nowcasting using deep generative models of radar” in Nature, Jul. 2021. [Online]. Available:  

[6] Alex Lynn, “Weather forecasting helps the drone industry take flight” in Electronic Specifier, Jul. 2021. [Online]. Available:  



Taken from ECMWF: 
 
It is another beautiful day in your home city, and you see throughout your window up in the blue sky… no sign of rain the whole day. Having concluded that, you apply your sunscreen and decide to wear light clothes, and before going out, you wonder to yourself: ‘do I need to take out my umbrella? No, why would I put extra weight in my bag without the need of that.’
The day goes by, and just two hours after leaving
your home, in a blink of an eye, it starts to get cloudy and darker, and here comes the rain. The cities’ crazy weather is a fact that frustrates a big majority of its citizens each day; nonetheless, AI can come to the rescue by helping the weather forecast field in a short term period.
Forecasting the rain is not just helpful to save yourself from getting wet, but also to many industries that depend on the weather, like outdoor events, aerial services, tourism agencies, and farms, among others. The predictions with AI and deep learning methods are being tested for tornadoes’ and hurricanes’ formations as well; by this way, the people living under or close to the possible affected area can be informed on time to find refuge.
To have a context, in tornado-prone regions, the residents have, on average, an early alert of 16 minutes before the tornado impacts the zone [1]. However, this time range may not be enough for some people to find a safe place, and thus their lives can be in danger. For this specific case, with the data collected from humidity, air pressure, and temperature sensors during the previous minutes, with AI, the alert can go up to an hour before the tornado comes [1].
 

Taken from Analytics Insight: 
 
Long-range and short-range forecasting
 
Now, let’s point out an important difference between two types of forecasting: long and short term.
Long-term forecasting refers to the prediction of weather for up to a month. This is done by mathematical and physical models aimed to simulate the physics of fluid dynamics weather [2]. These models usually are based on satellite images taken commonly in a year/month window. Because of the volatile and complex weather patterns, the aforementioned models are huge and take a lot of computer resources to be performed.
For short-term weather forecasting, also called nowcasting, the window size is reduced up to a few hours [3]. Here the mathematical models can become too complex due to the chaotic behavior of short climate periods. At this point, AI and deep learning are used to simplify the computational calculations and reduce the time at which these are performed.
 
Where to get the data to build the models?
 
There are a lot of sources from which scientists collect essential data to build the deep learning models, some more reliable and with more resolution than others. Some of the most used are:
-         Radiosondes are balloons that measure atmospheric characteristics, such as temperature, pressure, and humidity as they move through the air. These radiosondes use a radio signal to communicate the data to a station [4].
-         Radar stands for radio detection and ranging and sends out radio waves that bounce off the nearest object and return to a receiver. It can sense many precipitation characteristics such as location, motion, and intensity [4]. 
-         Satellites are geostationary objects that rotate with the earth and can be split into three categories [4]:
-         Visible satellites record storms, clouds, fires, and smog.
-         Infrared satellites record clouds, water, land temperatures, and features on the ocean.
-         Water vapor satellites look for the moisture content in the upper half of the atmosphere.
-         Drones (Still in research) are a new technology used in this field to help with short-range forecasting. They are equipped with pressure, humidity, and temperature sensors that generate data sent to a base station.
 
Drones for weather forecasting
 
The use of drones on weather predictions is a young field of research that so far has shown promising results. These researches focus on the lowest layer of the earth’s atmosphere called the boundary layer, where most of the earth’s weather is happening. 
Scientists have traditionally used weather balloons or weather stations to collect data and build the models; nonetheless, for the first tool, there’s the disadvantage of not being able to be fully controlled and depends on the wind direction. For the second method, these stations cannot be as high as needed, and they must be attached to a grounded object [1].
Drones have neither of these limitations, and they can fly with headwinds and even within a storm. However, because drones can provide higher resolution in terms of the sensed data, this issue creates challenges in the computational expense and the complexity of the physics parameterizations required in the forecasting models themselves [6].
One of the ongoing projects is directed by professor Philip Chilson at the University of Oklahoma. He envisions a drone network over a region equipped with a complete set of sensors. These non tripulated vehicles will be launched hourly. Instead of causing more complex theoretical models, they will feed with data some AI methods [1].

Taken from Nextgov: 
 
A model approach
 
The artificial intelligence company DeepMind, owned by Google-Parent-Alphabet, has the most advantage of research in weather prediction through AI. In the paper called “Skilful precipitation nowcasting using deep generative models of radar” [5], they found that 56 government meteorologists prefer the AI prediction model instead of other short-term forecasting methods in 89% of the cases.
The DeepMind model predicts up to two hours ahead thanks to weather data coming. The company developed a deep generative model (DGM), which is a statistical model that learns probability distributions of data and allows for the easy generation of samples from their learned distributions. The DGM is especially helpful to both learn from observational data as well as represent low uncertainty across a variety of spatial and temporal scales.
The DGM, in this case, is taken as probabilistic nowcasting of precipitation that addresses the almost random behavior of the climate. The methodology used helps to improve the current forecast quality and consistency through predictions over regions near to 1536 km x 1280 km with lead times from 5-90 minutes ahead.
These models are indicated to predict smaller-scale weather phenomena that are inherently hard to predict due to underlying stochasticity, which is critical for nowcasting. The input data to feed the model are radar-based observations of surface precipitation at a given time range and territory area. The mentioned observations keep updating the model at a frequency of 4 frames per 20 minutes. With this, the model is able to sample 18 frames of future precipitations in a period of 90 minutes [5].
The learning process step is driven by two loss functions and a regularization term, which guide parameter adjustment by comparing real radar observations to those generated by the model. The first loss is defined by a spatial discriminator, which is a convolutional neural network that aims to distinguish individual observed radar fields from generated fields, ensuring spatial consistency and discouraging blurry predictions.
The other loss is defined by a temporal discriminator, which is a three-dimensional convolutional neural network that aims to distinguish captured data from generated radar sequences; it imposes temporal consistency and penalizes jumpy predictions. Together with this, a regularization term is introduced to improve accuracy; this regularizer penalizes deviations at the grid search resolution between the real radar sequences and the model predictive mean. This feature also produces accurate location predictions and improves overall performance.
Speaking now about the training process, this model was prepared with a large dataset of precipitation events, which are 256 x 256 crops extracted from a radar source. The length is around 110 minutes with 22 frames. These images correspond to observations for the UK for years 2016-2018 and evaluated on a test set from 2019.
 
As a way of conclusion
 
Throughout all this article, we have been discussing how AI is helping the weather forecast and how this will help a lot of economic activities. With the guidance of high-resolution data from drones, for instance, the AI models can be fed with better sensed physical variables, and thus, they can give more concise results. The way until an exact tool for weather prediction is being walked with significant steps and soon whether to take out the umbrella or not will not be an issue anymore.
If you want to read more of our content follow the link : 
Quotes
 
[1] Elizabeth Ciobanu, “How drones are helping with weather forecasting” in Drone blog, Jan. 2022. [Online]. Available: 
 
[2] Aryan Thodupunuri, “The future of artificial intelligence in weather forecasting” in Towards AI, Aug. 2021. [Online]. Available: 
 
[3] James J., “How is long-range weather forecasting different than short-range forecasting?” in Socratic, Jul. 2015. [Online]. Available: 
 
[4] “Collecting weather data” in Lumen. [Online]. Available: 
 
[5] Suman Ravuri, Karen Lenc et al., “Skilful precipitation nowcasting using deep generative models of radar” in Nature, Jul. 2021. [Online]. Available:",16
17,Interfaces Invisibles. Pensando más allá de las pantallas.,"¿Cómo se ve el futuro de la interacción humano-máquina?
 
Últimamente, la gran pregunta que nos reta todos los días es: ¿cómo podemos hacer que nuestros productos tecnológicos simplifiquen cada vez más la vida de las personas y suplan las limitaciones de la tecnología actual? La idea de llegar a una experiencia agradable y que impacte positivamente a los usuarios desvela tanto a pequeños empresarios como a las grandes multinacionales.
Con el objetivo de empezar a soñar experiencias nuevas, más allá del monitor, el teclado y el ratón; primero, debemos revisar cuál ha sido la historia de la relación humano-máquina para luego – sobre experiencias aprendidas – podamos contemplar las relaciones usuario-interfaz del futuro.
Según lo anterior, este artículo tiene como objetivo hacer un recuento histórico de lo que ha sido tal relación para luego proponer posibles vías de innovación en lo que refiere a este campo, tales como Interfaces invisibles.
 
Un recorrido por la historia de la relación humano-máquina
 

Fotografía por Gilles Lambert en Unsplash
 
La relación entre los seres humanos y las máquinas (acotando la definición de este concepto a la tecnología computacional) data desde 1980, tras la primera ola de computadores personales y de escritorio. Si bien previamente ya existían las interfaces de líneas de comando (CLIs), no fue hasta que las primeras computadoras invadieron los hogares y oficinas que el público general tuvo la posibilidad de interactuar con estos sistemas electrónicos. A partir de ese momento, se democratizó el uso de estas máquinas por medio de las interfaces gráficas de usuario (GUI) y su metáfora del “escritorio”.
Entre 1990 y principios de los 2000, las personas empezaron a utilizar los computadores como medio de comunicación para socializar con los demás, colaborando y difundiendo conocimiento a través del correo electrónico. A esta nueva experiencia se sumaron las mejoras en la sensibilidad de los ratones al tacto y la presión. No obstante, las innovaciones en las GUIs no fueron demasiadas, manteniendo los conceptos de ventanas y archivos junto con el gesto de “arrastrar y soltar” del cursor.
Durante los siguientes 10 años, los usuarios empezaron a interactuar con el contenido digital de manera revolucionaria gracias a las pantallas táctiles y el infinito universo de las aplicaciones. Estas innovaciones permitieron que la tecnología tomara un rol más amplio en la vida de las personas. Por ejemplo, se empezó a usar como una , un medio de autoexpresión y una herramienta de apoyo en procesos de crecimiento personal, moldeando sus hábitos y protagonizando la paradoja de “estar solos pero acompañados”.
Desde entonces, las interfaces táctiles han permeado todos los aspectos de la vida humana. Sin embargo, en el 2011 surge un nuevo tipo de interacciones humano-máquina. Por un lado, están las asistentes de voz y las interfaces conversacionales, las cuales han ganado popularidad gracias a su interacción natural a partir del lenguaje humano. Por otro lado, existen las experiencias inmersivas que le agregan una nueva dimensión a las interfaces al utilizar la totalidad del campo visual humano; y los ambientes inteligentes (AmI), que toman los avances de la IA, la computación ubicua y los sensores sin contacto para crear espacios físicos sensibles y responsivos.
 
¿Cuál es el futuro de las interfaces? La necesidad de una nueva generación de interacciones más humanas
 

Fotografía por Philipp Berndt en Unsplash
 
Al mirar hacia atrás, nos damos cuenta que – durante los últimos 30 años – la relación humano-máquina se ha basado principalmente en interacciones generadas a partir de teclados, ratones y paneles táctiles. Sin embargo, estos periféricos no han logrado cerrar la brecha entre las capacidades computacionales y el comportamiento humano. Por ejemplo, estos siguen presentando inconvenientes al resultar inaccesibles para minorías como los adultos mayores (quienes encuentran las pantallas complejas y ambiguas) y las personas con habilidades especiales (que se frustran tras no poder interactuar del todo). Sin tener que ir más lejos, que estas interfaces involucren mayoritariamente el sentido de la vista, hace que al menos  se vean perjudicados.
A pesar de que las últimas tendencias en tecnología – mencionadas anteriormente – eliminan las barreras físicas y se acercan más a la naturaleza humana, estas sólo corresponden al comienzo de una nueva generación de interfaces. Entonces, ¿cómo sería una relación entre las personas y las máquinas en la que estas últimas entiendan e interpreten los comportamientos humanos físicos y mentales? ¿Qué pasaría si las interfaces permiten una experiencia a partir de los 5 sentidos, percibiendo nuestro cuerpo en el espacio e incluyendo funcionalidades gestuales?
Definitivamente, el siguiente camino de la innovación en interfaces debe salirse de los límites de las pantallas bidimensionales de vidrio, a las que nos hemos acostumbrado. Los nuevos avances tecnológicos deben permitir interacciones más cercanas a la biología humana, favoreciendo que la ejecución de cualquier tarea sea aún más intuitiva y ampliando el espectro de experiencias.
Ahora bien, antes de empezar a imaginarnos un mundo en el que reinen las PVUIs (Projected Visual User Interfaces) o en el que toda solución se inspire en los principios de diseño del HUD (Head-Up Display), considero que se ha de reflexionar acerca de las posibles características que deben tener las relaciones humano-máquina del futuro. Al tomar las propuestas por la startup alemana Senic, a continuación menciono las más relevantes, en mi opinión:
Descentralizadas, interfaces invisibles y mágicas.
Las interacciones se alejarán de las GUI y pasarán a ser omnipresentes, estando siempre donde se las necesite. Éstas harán parte de nuestro entorno, en las paredes o ventanas de la casa u oficina, y darán lugar a escenarios en los que podamos hacer un gesto en el aire para prender la luz de una habitación. Incluso, serán capaces de leer nuestras emociones y entender lo que estamos haciendo para anticipar nuestras necesidades. Esta idea resulta valiosa al permitir que las personas no tengan que sostener una pantalla o bajar la mirada para verla, concentrándose en situaciones más importantes.
Específicas.
Esta es la premisa más beneficiosa y retadora de todas. Pasar de pensar en soluciones que todo el mundo pueda utilizar, como en su momento lo diseñó Steve Jobs, y empezar a ofrecer experiencias únicas para situaciones concretas. Ejemplo de esto sería diseñar interfaces que brinden interacciones hápticas especiales para personas con profesiones en la industria creativa, o que apliquen métodos de aprendizaje basados en la gamificación para niños.
Centradas en los humanos.
Este concepto recalca la necesidad de incursionar con máquinas que perciban, entiendan y procesen tanto el cuerpo como la mente humana desde sus diferentes aproximaciones. Así, esto nos lleva a imaginarnos sistemas que detecten movimientos cerebrales y que entreguen información desde los múltiples sentidos.
Instantáneas.
Aquí, nos cuestionamos si se puede realizar una tarea sin necesidad de pasar por varios pasos, ni siquiera dos o tres. Resolver las actividades humanas al instante y reducir la carga cognitiva de las personas resultará en una mejor y menos estresante experiencia para el usuario.
 
Conclusión
 
Mejorar la experiencia de los usuarios se trata de un trabajo continuo. Siempre habrá más por aprender acerca de las diversas maneras en las que, como seres humanos, nos relacionamos con la tecnología. Las situaciones actuales nos dan la oportunidad de ser creativos y entregar valor al crear experiencias  que no se limiten a las superficies de interacción existentes. Se ha de seguir aprovechando y potenciando las posibilidades que nos aporta la Inteligencia Artificial, con sus capacidades de , o los beneficios que trae consigo la realidad virtual y aumentada. No obstante, se ha de ir aún más lejos y crear experiencias que verdaderamente simplifiquen la vida de todos sin excepción.
 
REFERENCIAS
 
AltexSoft Inc. (2018, 1 julio). Principles of Interaction Design: What It Takes to Create Positive Human-Computer Interactions. Medium. UX Planet. https://bit.ly/3aHjQLF
Asher, M. (2017, 18 julio). The History Of User Interfaces—And Where They Are Heading. Adobe. https://adobe.ly/3tYVHrq
Campbell, C. (2021, 22 enero). Ambient Intelligence — The Invisible Interface. Medium. Start It Up. https://bit.ly/3sUS9VR
Ekenstam, L. (2015, 25 octubre). Magic Leap — The next big frontier in human+computer revolution. Medium. https://bit.ly/2PqWp1R
Kim, A. (2018, 11 julio). What is Human Computer Interaction (HCI)? Medium. https://bit.ly/2R4DzxG
La OMS presenta el primer Informe mundial sobre la visión. (2019, 8 octubre). Organización mundial de la Salud. https://bit.ly/3gMDyJX
Senic. (2015, 8 enero). The Future of Human Computer Interaction. Medium. https://bit.ly/3gH7Eyc
Usabilla. (2017, 15 mayo). A Short History of Computer User Interface Design. Medium. The UX Blog. https://bit.ly/3nmT9Bb
What is Human-Computer Interaction (HCI)? (2021). The Interaction Design Foundation. https://bit.ly/2R1To8o
Xiao, L. (2017, 17 julio). A Brief History of Human-Computer Interaction (HCI). Medium. Prototypr.io. https://bit.ly/3vgHsyo",17
18,Los modelos matemáticos detrás de Machine Learning,"Las soluciones a las que lleguemos pueden ser tan inteligentes como las bases sobre las cuales las construimos
 
Desde el inicio de los tiempos, las ciencias han acompañado al ser humano, quien impulsado por la lógica y la razón que lo caracterizan, ha logrado avances significativos en este campo tan amplio y complejo. Sin ir tan lejos, podemos evidenciar la creatividad y el ingenio humano en un hecho tan simple (aparentemente) como construir un objeto redondo y ponerlo a rodar, tal invención combina ciencias como la física y las matemáticas.
Y es que a medida que la humanidad ha ido avanzando, las ciencias se han ido desarrollando de tal manera que se han convertido en el motor evolutivo del hombre en la sociedad, un gran reflejo de ello son todos los avances tecnológicos que ya hacen parte de la cotidianidad.
En años recientes con el auge de la , hemos visto como las máquinas han sido capaces de aprender y analizar información, el desarrollo de estas capacidades es conocido en las ciencias de la computación como Machine Learning – ML. Así pues, el objetivo de este artículo es explicar cómo en este momento de la humanidad, las matemáticas continúan jugando un papel protagónico (aunque se tenga poca conciencia de ello), en la implementación de Modelos matemáticos detrás de Machine Learning.
 
¿Cuáles son los modelos matemáticos detrás de Machine Learning?
 
Generalmente, se tiene la concepción que para construir modelos de machine learning basta con utilizar aquellos que se encuentran en librerías, con lenguajes de programación predeterminados y que sólo requieren de la acción de introducir la información necesaria.
Lo anterior es totalmente incorrecto, ya que, primero se debe saber qué información es relevante para el correcto funcionamiento de un modelo, pues como dice un conocido dicho dentro de este campo «basura entra, basura sale», y segundo y más importante, no conocer los fundamentos de estas herramientas generalmente, termina en alternativas que al final no van a solucionar ningún problema.
Por tal razón a continuación, desarrollaré los fundamentos a los que hice referencia anteriormente:
 
Álgebra lineal
 
Vectores, ecuaciones lineales, matrices todos estos conceptos son claves dentro del Machine Learning, y obligatoriamente deben conocerse, pues de entrada nos encontramos soluciones donde operaciones cíclicas como un For o While son reemplazadas por operaciones entre matrices, con el fin de buscar eficiencia al realizar estos cálculos, si seguimos avanzando encontramos representaciones de objetos en espacios vectoriales, temas complejos como el análisis de componentes principales (PCA siglas en inglés), en donde a través de operaciones matriciales y proyección de vectores, podemos reducir las características dentro de un análisis.
 

 – Machine Learning
 
¿De qué manera se aplica al mundo real?
 
Para dar claridad a lo anterior tomemos como ejemplo el campo de la psicología de la personalidad; todos tenemos diferentes personalidades y estas tienen diferentes matices, alguien se puede parecer mucho a otra persona en unos aspectos, pero a su vez diferir completamente en otros. ¿Existe alguna manera de cuantificar esto?
Desde hace aproximadamente 100 años se ha intentado dar respuesta a esta pregunta, con resultados que, si bien no acaban de abarcar toda la magnitud de esta incógnita, si han podido representar de manera parcial la personalidad de un ser humano.
Ahora cabe aclarar que dichas interpretaciones siempre se han realizado a través de criterios cualitativos, pero ¿será posible representar la personalidad de alguien de una manera cuantitativa? Para ello tomaremos como referente principal, la hipótesis léxica cuya idea principal se basa en que cualquier rasgo de la personalidad de un individuo siempre debe corresponder a una palabra del idioma, por ejemplo, una persona es valiente, sensible o tímida.
En un principio, psicólogos que se dedicaron al estudio del tema, encontraron alrededor de 4500 palabras que describían los diferentes rasgos humanos, más adelante profesionales de la misma rama se dieron a la tarea de agrupar dichas palabras hasta reducir la cifra a 500. ¿Pero qué ocurre si experimentalmente tomamos estas 500 palabras y les aplicamos PCA?
Lo que obtenemos es la reducción de estas 500 características a 5 rasgos con los cuales se puede clasificar la personalidad de un individuo (extroversión, responsabilidad, neocriticismo, cordialidad y apertura a la experiencia), dichos rasgos son conocidos en psicología como el modelo de los 5 grandes. Una muestra del porque el álgebra lineal jamás pasará de moda.
 
Cálculo diferencial, integral, multivariado
 
Implícito en los otros modelos matemáticos detrás de Machine Learning, resulta bastante útil conocer sobre integrales y derivadas parciales al momento de revisar las funciones de optimización, o para hallar matrices Hessianas, con las cuales podemos encontrar la convexidad de una función, esta característica es muy importante debido a que nos ayuda a elegir o descartar dicha función, así como a su vez para hallar sus puntos mínimos, lo que se traduce a la respuesta más óptima que podemos hallar en dicho procedimiento. Esto también es conocido como un híper parámetro, una variable dentro de una función que a medida que es ajustada hace que el modelo que estamos generando mejore o empeore.
 


Hay que tener en cuenta que cuando hablamos de entrenar modelos, estamos hablando de analizar cantidades gigantes de información, acción que supone tiempo y costos elevados de procesamiento, por lo que responder a preguntas como: ¿cuál será ese número mágico que debemos colocar en nuestro híper parámetro, que hará que nuestro modelo funcione a la perfección?, a través del instinto y experiencia no es suficiente. Se debe conocer la teoría sobre cómo funciona el modelo que estamos aplicando para de esta manera tener la certeza de que lo que estamos haciendo tiene sentido y no hay nada mejor que el cálculo para respaldar este tipo de decisiones.
 
Estadística y probabilidad
 
Gran parte de modelos matemáticos detrás de Machine Learning son o conllevan elementos estadísticos y probabilísticos, de esta manera los conocimientos sobre teoría de probabilidad, combinatorias, teoría de conjuntos, ley de Bayes entre los más relevantes, sirven de aliados para enfrentarnos a los diversos problemas que se nos planteen.
Un ejemplo de lo anterior se ve en los árboles de decisión, una técnica muy popular dentro del ML cuyas raíces están en el teorema de la probabilidad condicional. De la misma manera encontramos las máquinas de soporte vectorial que en palabras coloquiales se pueden definir como algoritmos clasificadores, cuyas respuestas están sustentadas en la probabilidad de que un punto pertenezca a una u otra clase, dentro de un espacio vectorial.


 
Se han preguntado alguna vez de qué manera funciona la predicción del clima o las predicciones demográficas. La respuesta es modelos de series de tiempo, que no es más que la aplicación de
diversos métodos estadísticos tales como estimaciones de tendencias, cálculo de estacionalidad, análisis de correlaciones, entre los más destacados, para un conjunto de datos que se han medido en diferentes etapas del tiempo y están ordenados de manera cronológica. Así sin duda alguna nos podemos referir a la estadística y probabilidad como el núcleo del ML.
 
Optimización
 
El concepto matemático para optimización está basado en unos criterios de selección y un conjunto de elementos, el reto está en hallar el elemento que mejor se ajuste a dichos criterios, siempre teniendo en cuenta la eficiencia del proceso y el uso de recursos, no es lo mismo encontrar un resultado exacto gastándose 3 días en encontrarlo y la capacidad de 10 servidores, a hallar un resultado no tan exacto, pero en 10 minutos y con un solo servidor.
Un claro ejemplo de este tema se da en el Gradiente Descendiente el cual es un algoritmo que aplica técnicas de optimización para encontrar un valor mínimo local dentro de una función. Explicado de una forma sencilla , suponga que usted está caminando en una montaña y de repente la niebla cubre el ambiente, en ese momento, su único objetivo es bajar la montaña para encontrar la planicie, así que precavidamente y paso a paso usted empieza a bajar hasta que en un punto encuentra terreno plano, en este escenario su mínimo local sería el terreno plano, su función seria bajar de esa montaña y el gradiente seria usted mismo pues es quien va recorriendo toda la montaña para encontrar el punto plano.
 


 
¿Cómo podría empezar a aprender del tema?
 
El principal problema que enfrentan la mayoría de las personas que quieren aprender de este campo tan interesante de la AI, es la gran cantidad frentes de acción que tiene, lo que se traduce en aprender sobre conceptos nuevos, herramientas nuevas, temas que a primera vista parecen enredados, pero que en la medida en que se conozcan sus bases, se hacen más sencillos de entender.
Así que, desde un punto de vista más personal, y como recomendación lo mejor es empezar por la raíz de todo, la estadística y probabilidad, para tener un acercamiento a los tipos de problemas que nos enfrentaremos, de allí podremos continuar con un repaso de algebra lineal, la cual me resulta
divertida pero no deja de ser desafiante, siguiendo nuestra hoja de ruta estaría todo lo relacionado con cálculo, tener claros conceptos básicos de derivadas, integrales y dimensiones es lo primordial para así finalizar con optimización que como ya lo vimos anteriormente tiene por dentro álgebra lineal y cálculo.
 
Conclusión
 
Conocer los modelos matemáticos detrás de Machine Learning, nos ayuda en el día a día a resolver cuestiones que son clave dentro de nuestra labor, como, por ejemplo: seleccionar el algoritmo que más se ajuste a nuestro problema, escoger los híper parámetros que mejor se ajustan a nuestro modelo, identificar problemas en los modelos como sesgos o sobre entrenamiento, hallar una función óptima para resolver nuestro problema, entre las tareas de gran relevancia.
Es por lo anterior que este tema es indispensable para alguien que está interesado en aprender de ML. Hay que recordar que, a grandes rasgos estamos intentando construir tecnología capaz de emular tareas y comportamientos humanos, y si no sabemos cómo estamos construyendo estas tecnologías, lo más probable es que las soluciones a las que lleguemos sean tan inteligentes como las bases sobre las cuales las construimos.",18
19,AI for more competitive retailers,"As artificial intelligence capabilities expand, these solutions continue to transform the landscape of retail businesses across the spectrum. From streamlining administrative tasks with automation to virtual shopping experiences created through real-time advertising, artificial intelligence has made it simpler to increase business output's speed, efficiency, and accuracy. This enhanced performance is linked closely to advanced data and predictive analytics systems that help companies make data-driven business decisions.

Retailers are increasingly looking for ways to optimize their business by adapting to new trends, understanding new market strategies, and utilizing new technologies. They are looking to integrate technologies such as artificial intelligence, data analytics, and RPA (Robotic Process Automation) into their businesses.

RPA (link to RPA section) consists of automating processes with robots, following established rules, so these robots can perform repetitive tasks, such as: Entering and reviewing data, pressing buttons, uploading or downloading files, making or paying invoices, among others, which will save you money and also helps your workers because they have more time to focus on more important tasks. 

At least £216 billion of economic growth in the UK (representing 10.1% of the UK economy per year) was achieved thanks to technology. Of this £54 billion was achieved by London demonstrating the great power that many companies are already harnessing to boost their business.

AI allows you to evade manual errors, empower human capabilities, automate processes and provide accurate results, according to Forbes magazine AI will be able to increase the workforce by 70% by 2025, (Gow, 2022), also the industry is expected to increase its value by $36.8 billion by 2025, which changes consumer preferences and customer buying patterns, meaning a big factor for industry growth in the following years.

AI also offers useful insight into consumers' behavioural analytics, which retailers can utilise to gain insights to help enhance different touchpoints throughout the customer journey. Our mission at Equinox is to build a retail solution that caters to the individual needs of your business.

Equinox and its impact on retailers

We at Equinox can provide you with complete solutions, as we always seek to implement more than one technology when designing our projects, all our solutions implement AI+Data+RPA because we will always give you the most complete solution, that intervenes in more than one problem.

We achieve this thanks to our interdisciplinary team (I do not remember if it is this word), which can address any need that our customers may present, since in Equinox we have developers, designers, AI engineers, data analysts, data scientists, and RPA specialists, among others.



Better performance:

Over time people have realized the almost infinite potential of artificial intelligence (link to artificial intelligence section) or AI for short, being an excellent tool when it comes to selling. For example, AI can learn about your customers, so you can learn about their preferences, behaviour and needs.

AI also allows you to create advanced algorithms that can know what your customers might be interested in based on demographics, social media behaviour and buying patterns.

We can help you improve a lot of processes within your company, such as synchronizing your physical and virtual store, which will allow you to know the status and behaviour of these channels at all times, being your virtual store a complement to your physical store and vice versa, which will allow these two channels do not overlap but work in harmony, complementing each other.

We can also optimize a lot of internal processes such as the supply chain, making it much more efficient and faster as we will be able to track exactly where your supplies are at all times, on the other hand, we can use the RPA specifically for the supply chain. 

In this way the robots will be able to program shipping schedules, they can also know the state of supply and demand so they can notify you when a specific product is running out or even better, they can order the product and make the payment automatically.   

A study conducted by the IGS (Information Service Group) found that using RPA in the supply chain reduces approximately 43% of the resources implemented for this end-to-end process, which includes tasks such as invoicing, credits, collections, pricing and others (Pillai, 2021).  

User understanding and assistance

Thanks to AI, RPA and Data tools you will know exactly what your customers require for you to sell more, you will be able to know the most optimal time to push your products and you will even be able to provide them with the products your customers need before they even know they need them.

You will also be able to optimise the user experience of your customers because you will offer them exactly what they are looking for at a price they are willing to pay or for example create a relationship of loyalty with your customers because you will always have at hand what they expect when they need it.

Another very important AI tool is Machine Learning (link to Machine learning section), this will allow you to find patterns in your customers' purchases by analyzing what they buy the most and when they buy it, which will allow you to recommend products they can buy together, for example, this is what Amazon is currently doing:


The image is taken from Google Images

AI could even recommend which products you should put together in your physical store, making you sell much more. AI has also empowered different businesses with high-level data which has exponentially improved their internal operations, as well as help you find new business opportunities.

AI also allows you to create interactive chats, or Chatbots (hyperlinks). These can converse with your customers, allowing you to improve your customer service, as they can answer frequently asked questions, report on the status of their orders and provide them with the help they need 24/7.

In addition, these bots can collect information, so they can learn from your customers, helping you to have valuable information that will allow you to make better business decisions.

Image recognition and analysis is another great AI tool that helps your customers discover new or related products, allowing you to create recommendations based on the aesthetics and similarities of the products.

Internal processes

A problem that retailers have always had is the disconnect between their physical store and their online store, as they usually work in very different ways and have different approaches, causing your shoppers to feel an unpleasant shopping experience, leading to inefficient operations in both channels.

AI is a great solution since it can synchronize your different channels, giving you, for example, A list of your complete inventory, and ways to take advantage of your online store and your physical store at the same time, allowing you to know what your shoppers prefer, whether to buy online or have the products in material, which will allow you to prioritize one channel, but of course without forgetting the potential of the other channels.


User experience

Now, we can also use AI for more experimental products such as virtual fitting rooms that offer a much more personalized experience to your customers, as this allows them to easily find the perfect outfit, plus your customers will be able to try on the different outfits, without the need to put them on.

This is achieved thanks to augmented reality, in this way your customers will have the outfit on, without the need to have it, it could also change its colour, size or design simply by pressing a button, which greatly streamlines the selection of items and customers can see a large catalogue without the need to search for them in the store.


The image is taken from Kid, A. R. (2018, 6 April). The virtual fitting room is the new trend in augmented reality. Augmented and Virtual Reality Company. https://ardev.es/en/virtual-fitting-room-augmented-reality/.
 
AI can also help you in the security of your shoppers' data, recording the movements inside the store, it also allows you to encrypt their sensitive personal information, such as credit card information, personal data or shopping lists.

Also thanks to video analytics you will be able to be checking in real-time your physical store products and could notice suspicious activities in this way would give you a notification of theft instantly and you could take action as quickly as possible, in this way AI will help you to prevent and alert about theft or events inside your stores.

In conclusion AI, data analysis and RPA are very powerful tools that are changing the retail industry and we want you to be part of that change, Equinox IA Lab can be your guide in this process, if any of this information caught your attention please contact us or schedule a meeting with the help of our ChatBot.

Equinox Editorial Team
Author: Cristian Zorrilla

Bibliography:

Gow, G. (2022, 30 October). The Argument For An AI Augmented Workforce. Forbes. 
Kid, A. R. (2018, 6 April). Virtual fitting room, the new trend in augmented reality. Augmented and Virtual Reality Company. 
How Artificial Intelligence (AI) Can Help Retail. (2019, 16 June). LiveAbout. 
AI In Retail: Uses Of Artificial Intelligence In Retail Business. (2022, 8 June). USM. 
Digital britain: How small businesses are turning the tide on tech. (2022). Sage.
Pillai, S. (2021, 22 septiembre). Why Supply Chain Agility Is Critical Today. Default. 


Post 1:

AI and all its advantage for retailers

Post 2:

The retail business is constantly changing, and AI is a tool that is going to maintain your business update. As time goes by, it will be an indispensable tool. 	

Post 3:

AI is going to be an indispensable tool for retails, Equinox AI lab wants to be your partner in this important step. AI helps you in almost every aspect of your business, from logistics to user experience. Retailers that don't adapt to new technologies are more prone to disappear in the near future.",19
20,Diseño UX y AI confianza y reciprocidad,"En este artículo Diseño UX y AI: una relación de confianza y reciprocidad abordaremos cómo la Inteligencia Artificial (IA) se ha convertido en una poderosa herramienta que, basada en conceptos de ciencia y computación, ha logrado incorporarse de manera sutil pero contundente en diferentes campos del conocimiento. El diseño, por su parte, se ha visto en la necesidad de entrar al terreno de la IA (inteligencia artificial) y así irse adaptando a las nuevas posibilidades que esta permite.
Para lograrlo, ha sido necesario devolverse a la esencia misma del diseño: el usuario, y de esta manera recordar siempre que el verdadero propósito de cada creación es el de mejorar la experiencia humana. El objetivo de este texto es indagar sobre las posibles relaciones que puede haber entre el Diseño UX y AI. De este modo, se plantea una relación basada en la teoría de la guía práctica de People + AI de Google, que se fortalece a través de la idea de construir una relación humano-maquina, según lo propuesto por IBM en “Design for AI”.
La creación de una experiencia basada en  se fundamenta a partir de dos elementos: el usuario y el modelo AI. La integración de estos componentes debe darse en un entorno de interacción capaz de conectar a las personas con la tecnología de manera eficiente. Es decir, se debe adecuar el medio a través del diseño, de modo que el usuario pueda interactuar con la solución de AI de manera efectiva y amigable. Como plantea IBM, en un diseño enfocado en AI, esta interacción se logra a través de la construcción de una relación humano-maquina, que esté basada principalmente en confianza y reciprocidad.
 
Elementos relación Humano-Máquina
 
Llegado a este punto, es pertinente ahondar en las características y atributos de los elementos que intervienen en el proceso: Usuario y Modelo AI. Este último corresponde a la solución final o modelo final. La Inteligencia Artificial, como explica , se refiere a los sistemas de autoaprendizaje que, a través de la recolección de datos, tienen la capacidad de agregar capacidades cognitivas propias de los seres humanos a las máquinas.
Estos sistemas tienen cuatro (4) características: entienden, razonan, aprenden e interactúan. El entendimiento se refiere a la habilidad de comprender los datos, que pueden ser desde tablas de texto hasta imágenes y audios. El razonamiento corresponde a la capacidad que tiene un modelo de formular hipótesis y de dar argumentos sustentados a partir de la comprensión de los datos. Por último, el aprendizaje y la interacción están relacionados con el entrenamiento, es decir, con la posibilidad de refinar el sistema a través de las múltiples interacciones que mantiene con el usuario y que así estas sean más precisas.
Así pues, es claro que los modelos de  tienen la habilidad de interactuar con el usuario y dar resultados de valor a partir de datos.  Sin embargo, hay que tener en cuenta que cada una de las soluciones de AI deben estar pensadas y diseñadas para servir al usuario: “La IA no se trata de datos, se trata de ideas. Los datos son el combustible para el automóvil, las ideas son los destinos. Esto es lo que le importa a la gente” (IBM). Es decir, su verdadero propósito es el de amplificar las capacidades humanas, lo que se traduce en el concepto de Inteligencia Amplificada.
El otro elemento, el usuario, es la representación digital de la persona que hará uso del sistema de manera habitual. Al respecto, son importantes las motivaciones, expectativas y necesidades del sujeto, las cuales se extraen a través de un proceso de investigación basado en observación directa. Este conocimiento ayudará a identificar oportunidades de intervención, que a su vez moldean el objetivo de la solución final. Asimismo, va a facilitar la toma de decisiones posteriores, como la cuestión de si se debe utilizar la IA para automatizar o potenciar una actividad o proceso. Todas estas decisiones se toman en la primera fase de la metodología que se implementa en la compañía (ASESOFTWARE y EQUINOX) cuyas bases están fundamentadas en la guía práctica People + AI.
 

 – Imagine inventariar productos a la velocidad de un click
 
Enfoques del proceso AI
 
Cuando se diseña para la AI se deben tener en cuenta tres enfoques: (i) el propósito «¿qué quiero lograr?», (ii) el valor «¿cómo estoy ayudando?» y (iii) la confianza «¿existe una relación emocional de un usuario hacia el sistema?». Respecto al último, entran en juego distintos factores: la seguridad del intercambio de datos, la voluntad de controlar el sistema y la expectativa de calidad y certeza sobre los resultados.
El propósito se establece a partir de las necesidades del usuario, identificadas en la primera etapa de la metodología. El valor, por otra parte, se define en una segunda etapa: la de ideación, en la que prima el componente de innovación. Durante esta fase se toman en cuenta las características del usuario para determinar los atributos que van a componer el modelo de AI. Surgen ciertas preguntas: ¿qué se necesita conocer del usuario para mejorar su experiencia? ¿qué datos necesita recolectar el modelo para mejorar esta experiencia?
De este modo, el valor de la solución radicará en la efectividad del resultado para el usuario y en lo diferente e innovadora que se considere la solución propuesta. Sumado a esto, una pregunta clave en esta etapa del proceso es la siguiente: ¿cómo se construirá una relación entre este modelo AI y el usuario? Esto, debido a que la construcción de interacciones efectivas garantizará una adecuada usabilidad y ayudará a la generación de un vínculo de confianza. En este sentido, el rol del diseño UX no solo se centra en conocer las motivaciones y necesidades de las personas, como en cualquier proceso de diseño centrado en el usuario, sino también busca construir una relación de confianza entre la tecnología y aquel.
 
¿Cómo se construye esta relación humano-máquina?
 
Para responder la pregunta, es importante dar una mirada a la psicología presente detrás de la construcción de las relaciones. IBM propone El modelo de relación creado por Mark Knapp, profesor de la Universidad de Texas y experto en la comunicación humana no verbal, en donde clasifica la construcción de una relación en 10 etapas, 5 de ellas enfocadas en la unión de las personas y las otras 5 en la separación.  Para el propósito del texto, se hará énfasis en las primeras 5.
La primera de ellas, La iniciación representa aquel momento en el cual, a partir de juicios rápidos se decide o no continuar a una segunda etapa en la relación.
La segunda de ellas, La experimentación, corresponde a la fase de exploración en donde el propósito de los involucrados es el de encontrar puntos en común entre si.
La tercera de ellas es la Intensificación. Esta ocurre cuando se quiere compartir más acerca de los intereses en común encontrados.
La integración, hace referencia a la discreción que puede llegar a tener un modelo de AI. El modelo de AI debe llegar a interactuar de manera natural con las personas y esto significa, en muchas ocasiones, pasar desapercibido.
Por último, está La vinculación, que ocurre cuando ambos lados están totalmente conectados a través de una relación de confianza.
Se mencionan algunos de los elementos que se deben tener en cuenta para diseñar esta relación efectiva. Durante todo el proceso de la construcción de una relación humano-máquina acertada, se deben entender cuales son estos comportamientos e intenciones que mueven al usuario. La intención mueve la acción y la acción se traduce en estos comportamientos que van sujetos a los conocimientos previos que tiene el sujeto. Estas representan las estructuras mentales y de las cuales hace tanto énfasis la guía de Google. Estas estructuras mentales ayudan a dar una idea inicial de cómo debe ser el diseño de estos entornos de interacción.
¿Qué va a hacer el usuario? ¿Qué esperaría que sucediera? Estos comportamientos son la base de la construcción de las interacciones pues marcarán la dirección de la usabilidad. Muchas de estas estructuras mentales se identifican en un proceso de experimentación y es por esto que el pasar rápidamente a una fase de prototipado garantiza una agilidad y asertividad en el crecimiento de la propuesta final.
 

 – Chatbot multiplataformas, de fácil integración y uso.
 
Iniciación
 
Empecemos por la primera etapa, la iniciación, en donde se crean juicios rápidos. Dentro de esta etapa se deben tratar dos temas importantes, del lado del sistema, aparecen los sesgos y estos representan todos aquellos prejuicios que pueden aparecer en un sistema a medida que se procesan los datos. Estos prejuicios en la medida que no se eliminan se escalan.
Por el otro lado están los prejuicios por parte del usuario los cuales inician a disminuir si se introduce la solución de AI de manera transparente. Es acá donde se inicia la construcción de una relación de confianza.
Esta relación de confianza se irá fortaleciendo a lo largo de las siguientes etapas tan solo si se diseña de manera efectiva cada una de las interacciones. Dentro de un primer acercamiento se debe siempre dejar claras las expectativas, los beneficios de la solución, sus limitantes y alcances. Esto mantiene las expectativas del usuario siempre conectadas con los resultados que dará el modelo. El reto está en entregarle de manera adecuada esta información al usuario.
Por otro lado, la transparencia, el usuario tiene la necesidad de saber siempre que datos está estregando y cuál es el uso que se le están dando. La transparencia de esta información siempre será bien recibida.
 
Experimentación e intensificación
 
A lo largo de estas dos etapas el usuario capta los puntos que tiene en común con el modelo AI. ¿Las respuestas son acertadas? ¿si me está entendiendo? o inclusive ¿si estoy entendiendo lo que me está diciendo el sistema? Es acá donde se tiene que comenzar a planear un sistema de co-aprendizaje en donde las interacciones se vuelvan bilaterales. En este sentido se debe pensar en la recolección del feedback del usuario, bien sea de manera explícita o implícita.
En este punto el sistema podrá entender si realmente las recomendaciones que está dando son las correctas o por el contrario, fallan. En caso dado, que no sean exitosas, se tiene que comenzar a pensar como “fallar con gracia”, es decir, de que manera voy a mantener la atención del usuario aun sabiendo que le di una respuesta equivocada. El humor puede ser una buena alternativa.
 
Integración
 
En esta etapa se naturaliza la relación y siempre se debe mantener claro cual va a ser el grado de control que se le dará al usuario durante la experiencia. A menos que se comunique de manera explícita desde el principio, este control lo irá descubriendo el usuario a media que interactúa con el sistema. Para la última etapa de este modelo de relaciones podemos dar por hecho que se ha generado un vínculo de confianza entre el modelo y el usuario.
 
Conclusión
 
Es importante resaltar que cada una de estas etapas, posibles fallas, expectativas y comunicación de los alcances deben pasar por una fase de “prototipado” en donde de manera ágil y poco costosa, se prueben cada uno de los componentes anteriores, se reciba feedback del usuario y se hagan los ajustes pertinentes. El proceso de construcción de una solución de AI se debe basar en la iteración. De iteración a iteración se llega a una mejor solución para llegar al balance perfecto de Diseño UX y AI: confianza y reciprocidad.",20
21,BI for making decisions,"Making decisions has been decisive in the course of human history, and it is fundamental to demarcate what can happen in the future. This process is carried out in a constant way to solve from the simplest problem such as choosing which transport you want to use to get to your work faster, to even for more complex issues, for example, growing a company more.

 
For this, the human being must realize a thought process that allows him to choose the best possible decision that can solve the problem, but in that case, would it be better to let ourselves be carried away by our intuition? Or is it better to do a complex process that allows us to analyze the problem and thus find the best alternative deeply?
It is clear that in order to solve a simple problem that is from our daily lives, it is enough to use our intuition, and surely we will be able to make a wise decision to solve the problem. Still, can this intuition be applied to solve more complex problems? The answer is that possibly the decision that is made will not be the most successful or the most efficient; it is here where Business Intelligence (BI) takes a fundamental role.
 
What is BI?
 
""Business Intelligence (BI) is an umbrella term for technology that enables data preparation, mining, management, and visualization. Business Intelligence tools and processes enable users to identify actionable insights from unprocessed data, making it easier to make data-driven decisions across companies across various industries."" IBM. What is BI? [1]
*Want to know first the relevance of data? Read our blog It´s all about data here.
 

 
In other words, Business Intelligence is a set of tools that allows a person or organization to take advantage of a fundamental input such as data to generate ""insights"". In a few words, information and knowledge can be extracted from the data, which serves as a basis to improve the decision-making process.
 
Why use BI?
 
BI offers organizations the opportunity to extract insights from data that serve as a basis for improving business decision-making. In addition, this allows organizations to operate in a more agile and efficient way by being able to find in the data the information they require to make intelligent decision-making related to any field of the organization [2].

BI process
 
Applying Business Intelligence starts with analyzing the organization's requirements and business questions. This first step is essential since it is necessary that the business questions and requirements are well defined to continue the process [3]. From this, you begin to follow the procedure shown in the following image.

Data Sources: This step identifies the needed data sources to solve the requirements and business questions.
Extract-Transform-Load DW: First of all, you must have the data model based on the requirements given by the organization. Subsequently, we proceed to carry out the ETL process from the Data Sources to the DWH.
Data Mining: This process analyzes large quantities of data to find patterns such as groups of records, unusual records, and dependency [4].
Data Analysis Reporting: In this step, data visualizations are used to allow us to find insights that enable us to respond to the requirements and business questions raised.
Decision Making: This last step is where the relevant conclusions could be drawn to make way for intelligent decision making.
Applications of BI
 
BI can have different applications for one organization; among them, we can find [2]:
Identify ways to increase productivity
Analyze the behavior of an organization's customers
Compare data with competitor information.
Track performance
Optimize operations
Predict success
Identify market trends
Detect inconveniences or problems
Detect the risk of flight of employees of the organization
 

 
In conclusion, BI is important for organizations because it can help answer many business questions and for intelligent decisions that allow a relevant solution to a respective problem.
As seen above, BI could have different applications, all to improve aspects related to any area or field of the organization, which shows that it is currently an essential tool for the growth of organizations.
 
References
 
[1] IBM. ¿Qué es la Inteligencia Empresarial. Recuperado 6 de Enero de 2022, de 
[2] Tableau. ¿Qúe es la inteligencia de negocios? Guia sobre la inteligencia de negocios y por qué es importante. Recuperado 6 de Enero de 2022, de 
[3] Sherman, R. (2015). Business Intelligence Guidebook From Data Integration to Analytics. Chapter 3, pág 44-45.
[4] Sherman, R. (2015). Business Intelligence Guidebook From Data Integration to Analytics. Chapter 1,  pág 17.",21
22,"Inteligencia artificial, la tecnología que nos permite ser más humanos ","Inteligencia artificial, la tecnología que nos permite ser más humanos

Es un día lluvioso, te levantas muy temprano para ir a trabajar, y abres tu celular. Revisas tu correo, redes sociales y contestas algunos mensajes. Te arreglas para tu día, y mientras que ves las noticias te comes unos huevos con un café. Te alistas para salir, y pides un Uber para llegar al trabajo, mientras que escuchas Spotify o Apple Music y conoces tu nueva canción favorita. En solo de pronto dos o tres horas del día, has utilizado inteligencia artificial casi cada minuto de tu día. Desde la música que Spotify te recomienda mientras que te bañas o vas al trabajo, hasta los filtros de spam que utiliza Gmail o Outlook para evitar que veas esa propaganda en tu correo por veinteava vez. 
La mayor virtud y utilidad de la inteligencia artificial no es lo que nos venden las películas, robots andantes que aprenden de nuestros movimientos para hacer una revolución. La inteligencia artificial, es lo que nos permite ser más eficientes cada instante, nos permite ser más asertivos al tomar decisiones, incluso más importante nos permite tener una mejor experiencia como humanos. Cuando se desarrolla inteligencia artificial no solo se debe hacer en un esquema ético y legal, sino también se debe hacer con el propósito de complementar la inteligencia humana. Los humanos tenemos una capacidad mental inigualable, en últimas somos los creadores de la inteligencia artificial. Lo que nos permite esta nueva tecnología, es que esa inteligencia sea usada para innovar, crear, soñar, romper los límites de lo que pensábamos era imposible. 
Si lo vemos en estadísticas “la inteligencia artificial tiene un margen de error del 3% mientras que los humanos lo tenemos del 5%” (Retina, 2019) en actividades automatizables como lo son complejos cálculos, análisis de datos, probabilidades, entre otros. Pero en contraste a esto, al comparar ambas inteligencias con actividades como argumentación, curiosidad, creatividad e inteligencia emocional, los humanos se llevan la delantera (Fresno, 2018). No se debe entender ambas inteligencias como contrincantes, sino como complementarias, como se han demostrado en diferentes áreas. 


Un ejemplo se puede ver en el sector de la salud. Este trabaja todos los días para entender como el cuerpo humano y las enfermedades funcionan. Uno de estos es el cáncer de mamá, el cual es uno de los canceres más frecuentes con un porcentaje de 11,6% de todos los cánceres diagnosticados (Quironsalud). En mayo del 2019, el Computer Science and Artificial Intelligence Laboratory desarrollo un algoritmo que logra predecir la aparición de cáncer de mama hasta con 5 años de antelación (RGT Consultores Internacionales, 2020). Incluso una alianza entre el MIT y ocho diferentes farmacéuticas que utilizan inteligencia artificial para sintetizar productos químicos y hacer estudios entre moléculas y funciones biológicas para crear medicinas en tiempo récord (RGT Consultores Internacionales, 2020).
Por otro lado, 15% de los niños en Colombia sufren de dificultades de aprendizaje, lo que les dificulta seguir su crecimiento profesional desde una edad temprana. Sin embargo, se ha creado una herramienta con IA que logra seleccionar recursos adaptados para este tipo de dificultades, así dando un enfoque especial para todos estos estudiantes (Inspiratics). 
La revolución tecnológica ya ha empezado y va con mayor fuerza que nunca. Es una oportunidad para empresas, gobiernos y personas particulares de cambiar para mejor la forma en que viven y operan todos los días. Es la oportunidad de explorar todo lo que nos hace humanos, de una forma eficiente e innovativa. Confiar en esta tecnología, no es saltar a una piscina con los ojos cerrados sin saber lo que hay abajo, es saltar viendo el panorama completo. Mucho se ha desarrollado en Colombia y en el mundo sobre acuerdos legales para que el desarrollo de esta tecnología tenga un enfoque social y justo. Adicionalmente, empresas líderes como Asesoftware, han tomado el liderazgo en este sector desarrollando cada día servicios y productos para que la mejor tecnología y el futuro llegue a tus manos. ¿Qué esperas para hacer parte del cambio?




Referencias 
Retina, E. P. (2019, February 26). ""La inteligencia artificial cuenta con un margen de error del 3%; los humanos, un 5%"". EL PAÍS. .
Fresno, B. G. del. (2018, May 21). Inteligencia artificial (AI): tres razones por las que los humanos son irreemplazables: BBVA. BBVA NOTICIAS. .
RGT Consultores Internacionales. (2020, May 15). Inteligencia artificial en el sector salud. RGT Consultores Internacionales. .
Inspiratics. (n.d.). 5 usos que ya tiene la Inteligencia Artificial en el aula. Inspiratics. .
Quironsalud. (n.d.). Tipos de cáncer más frecuentes. Instituto Oncológico de Zaragoza. .
“Por medio del cual se establece la Inclusión Educativa de personas con Dislexia, Trastorno por Déficit de Atención con Hiperactividad –TDAH- y otras dificultades de Aprendizaje - DA”, PL 108-18, (2018)

Es un día lluvioso, te levantas muy temprano para ir a trabajar, y abres tu celular. Revisas tu correo, redes sociales y contestas algunos mensajes. Te arreglas para tu día, y mientras que ves las noticias te comes unos huevos con un café. Te alistas para salir, y pides un Uber para llegar al trabajo, mientras que escuchas Spotify o Apple Music y conoces tu nueva canción favorita. En solo de pronto dos o tres horas del día, has utilizado inteligencia artificial casi cada minuto de tu día. Desde la música que Spotify te recomienda mientras que te bañas o vas al trabajo, hasta los filtros de spam que utiliza Gmail o Outlook para evitar que veas esa propaganda en tu correo por veinteava vez.
 
 
Fotografía tomada de Freepik[/caption]
La mayor virtud y utilidad de la inteligencia artificial no es lo que nos venden las películas, robots andantes que aprenden de nuestros movimientos para hacer una revolución. La inteligencia artificial, es lo que nos permite ser más eficientes cada instante, nos permite ser más asertivos al tomar decisiones, incluso más importante nos permite tener una mejor .
Cuando se desarrolla inteligencia artificial no solo se debe hacer en un esquema ético y legal, sino también se debe hacer con el propósito de complementar la inteligencia humana. Los humanos tenemos una capacidad mental inigualable, en últimas somos los creadores de la inteligencia artificial. Lo que nos permite esta nueva tecnología, es que esa inteligencia sea usada para innovar, crear, soñar, romper los límites de lo que pensábamos era imposible.
Si lo vemos en estadísticas “la inteligencia artificial tiene un margen de error del 3% mientras que los humanos lo tenemos del 5%” (Retina, 2019) en actividades automatizables como lo son complejos cálculos, análisis de datos, probabilidades, entre otros. Pero en contraste a esto, al comparar ambas inteligencias con actividades como argumentación, curiosidad, creatividad e inteligencia emocional, los humanos se llevan la delantera (Fresno, 2018). No se debe entender ambas inteligencias como contrincantes, sino como complementarias, como se han demostrado en diferentes áreas.
Un ejemplo se puede ver en el sector de la salud. Este trabaja todos los días para entender como el cuerpo humano y las enfermedades funcionan. Uno de estos es el cáncer de mamá, el cual es uno de los canceres más frecuentes con un porcentaje de 11,6% de todos los cánceres diagnosticados (Quironsalud). En mayo del 2019, el Computer Science and Artificial Intelligence Laboratory desarrollo un algoritmo que logra predecir la aparición de cáncer de mama hasta con 5 años de antelación (RGT Consultores Internacionales, 2020).
Incluso una alianza entre el MIT y ocho diferentes farmacéuticas que utilizan inteligencia artificial para sintetizar productos químicos y hacer estudios entre moléculas y funciones biológicas para crear medicinas en tiempo récord (RGT Consultores Internacionales, 2020).
Por otro lado, 15% de los niños en Colombia sufren de dificultades de aprendizaje, lo que les dificulta seguir su crecimiento profesional desde una edad temprana. Sin embargo, se ha creado una herramienta con IA que logra seleccionar recursos adaptados para este tipo de dificultades, así dando un enfoque especial para todos estos estudiantes (Inspiratics).
La revolución tecnológica ya ha empezado y va con mayor fuerza que nunca. Es una oportunidad para empresas, gobiernos y personas particulares de cambiar para mejor la forma en que viven y operan todos los días. Es la oportunidad de explorar todo lo que nos hace humanos, de una forma eficiente e innovativa. Confiar en esta tecnología, no es saltar a una piscina con los ojos cerrados sin saber lo que hay abajo, es saltar viendo el panorama completo. Mucho se ha desarrollado en Colombia y en el mundo sobre acuerdos legales para que el desarrollo de esta tecnología tenga un enfoque social y justo.
Adicionalmente, empresas líderes como , han tomado el liderazgo en este sector desarrollando cada día servicios y productos para que la mejor tecnología y el futuro llegue a tus manos. ¿Qué esperas para hacer parte del cambio?
Referencias
Retina, E. P. (2019, February 26). ""La inteligencia artificial cuenta con un margen de error del 3%; los humanos, un 5%"". EL PAÍS. .
Fresno, B. G. del. (2018, May 21). Inteligencia artificial (AI): tres razones por las que los humanos son irreemplazables: BBVA. BBVA NOTICIAS. .
RGT Consultores Internacionales. (2020, May 15). Inteligencia artificial en el sector salud. RGT Consultores Internacionales. .
Inspiratics. (n.d.). 5 usos que ya tiene la Inteligencia Artificial en el aula. Inspiratics. .
Quironsalud. (n.d.). Tipos de cáncer más frecuentes. Instituto Oncológico de Zaragoza. .
“Por medio del cual se establece la Inclusión Educativa de personas con Dislexia, Trastorno por Déficit de Atención con Hiperactividad –TDAH- y otras dificultades de Aprendizaje - DA”, PL 108-18, (2018)",22
23,Aprendizaje profundo o nadar en la orilla,"¿Aprendizaje profundo o nadar en la orilla?
 
Si está pensando en implementar su primera solución de inteligencia artificial, este artículo le interesa
 
El aprendizaje profundo (deep learning) es un subconjunto de los algoritmos de aprendizaje autónomo, que se ha convertido en un estándar de facto y es una de las primeras opciones a considerar cuando se aborda un nuevo problema. Internet está lleno de información sobre por qué hacerlo, cómo hacerlo y quién está dando el salto a lo profundo. En este momento es como el martillo que todo lo ve como un clavo. No sobra una aclaración, no pretendo decir que el aprendizaje profundo sea inútil o que no sea un gran avance, porque indiscutiblemente lo es. Por el contrario, el propósito de este artículo es explicar a una audiencia fuera de la ciencia de datos, ¿Cuándo el Deep Learning es la mejor opción? y cuándo es mejor optar por un enfoque más convencional. Si está pensando en implementar su primera solución de inteligencia artificial, este artículo le interesa.
 
Machine Learning y aprendizaje profundo
 
A modo de introducción y para dar algo de contexto, explicaré brevemente algunos conceptos. En primer lugar, el aprendizaje autónomo (machine learning) agrupa un conjunto de algoritmos y herramientas estadísticas para que un  explícitamente en la forma de resolverla. Los modelos de aprendizaje autónomo aprenden de la experiencia. En este contexto, experiencia significa “datos” (principalmente ejemplos resueltos y etiquetados de la tarea a realizar). Aunque existe un conjunto amplio de posibilidades entre estos algoritmos, las redes neuronales son probablemente las más conocidas.


El aprendizaje profundo se refiere estrictamente a las redes neuronales profundas, es decir, aquellas redes que tienen más de una capa de entrada y una de salida. También es posible decir que son aquellas redes con una capa oculta o más. En la práctica, se va mucho más allá de estos números. Más de 50 capas y millones de parámetros están detrás de los grandes titulares de la inteligencia artificial (conducción autónoma, visión por computador, procesamiento de lenguaje natural, reconocimiento de voz, entre muchos más ejemplos). Cada capa adicional le brinda al modelo de reconocer patrones más complejos detectando patrones en la capa anterior. La imagen a continuación busca explicar este concepto.
 

By Sven Behnke – Own work, CC BY-SA 4.0,  in Deep Learning
En el ejemplo se observa que las capas iniciales están basadas en conceptos simples como rectas, pero estas van evolucionando (profundizando) a conceptos más complejos como figuras geométricas, hasta llegar a conceptos del contexto del problema que se está abordando (en este caso, representaciones de animales).
 
Retos y dificultades del aprendizaje profundo
 
Exigencia por grandes volúmenes de datos
El incremento de capas hace a las redes más flexibles, de manera que, con la arquitectura adecuada, pueden modelar un gran numero de patrones y funciones. Su capacidad de representación crece con cada capa y estos modelos pueden resultar bastante grandes. De hecho, este es el primer problema que se puede presentar: estos son modelos de peso pesado. Las redes neuronales profundas necesitan cantidades gigantescas de información para entrenar sus decenas de capas y millones de parámetros.

Así que la primera regla sería “no use aprendizaje profundo si no tiene información para alimentarlo”.
Ahora es donde vienen las objeciones de ¿y la transferencia de aprendizaje (transfer learning)? Sí, es una opción. La transferencia de aprendizaje toma un modelo existente funcional y entrena de nuevo parte de él para un nuevo propósito. No obstante, el desempeño de la solución lograda usando transferencia de aprendizaje depende también de la cantidad de datos. Entrenar exitosamente solo las capas finales del modelo también necesita una cantidad significativa de datos (aunque menor que para entrenar la red entera).Hago un paréntesis para profundizar un poco en la transferencia de aprendizaje (transfer learning). Ésta es una técnica en la que una red neuronal entrenada para una tarea es adaptada para cumplir otra función. Por ejemplo, una aplicación para reconocer gatos puede ser adaptada para en su lugar, reconocer leones o algún otro animal. El principio es que las capas más bajas de la red aprenden a reconocer bordes y otros patrones, mientras que el conocimiento necesario para separar un gato de un león depende de características más complejas y, por ende, más adelante en el proceso. Para hacer la transferencia, se “congelan” las capas más bajas de la red y se usan ejemplos de la nueva tarea para continuar el entrenamiento de las capas superiores, aprovechando lo que el modelo ya ha aprendido en niveles más básicos.
Sobreajuste vs generalización
El segundo problema que puede presentarse con el aprendizaje profundo es el sobreajuste (overfitting). Un modelo con sobreajuste ha aprendido *demasiado* bien de los datos de entrenamiento, llegando así a un modelo que no generaliza suficientemente bien y presenta un desempeño más bajo en los casos generales. Las redes neuronales, sobre todo las más grandes, son propensas al sobreajuste por su gran cantidad de parámetros a entrenar. Por supuesto, existen muchos métodos para solucionar esto y obtener un modelo de buen desempeño, pero se salen del alcance de este texto.

Dificultad para interpretar cómo funciona el modelo
Ahora bien, hay suficientes datos, hay cómo solucionar el sobreajuste, ¿usamos aprendizaje profundo? No tan rápido. Los modelos de aprendizaje profundo son cajas negras en su mayoría. Con la excepción de algunas aplicaciones de visión por computador, es difícil, si no imposible, saber qué está haciendo el modelo internamente. Si se requiere comprensión del modelo por parte de las personas, opte por algo más convencional como un árbol de decisión o cualquier otro modelo “traducible” para los seres humanos.
Poder de cómputo exigido
Parece que ya lo hemos revisado todo, pero aún nos falta un punto crítico: el poder de cómputo necesario para el entrenamiento. Hemos hablado del tamaño gigantesco de estos modelos, de la cantidad de datos que requieren y un par de cosas más. Bueno, todo lo anterior se traduce en una demanda enorme de poder de procesamiento para entrenar un modelo de aprendizaje profundo. No es extraño escuchar tiempos de entrenamiento de días o meses. Más poder de cómputo significa casi siempre menos tiempo de entrenamiento y, entre más grande sea la red a entrenar, más lento será su entrenamiento. Es necesario prever el costo de una estación de trabajo equipada con varias GPUs (tarjetas gráficas) o estar dispuesto a rentar en la nube una máquina como ésta por horas.
 
CONCLUSIONES
 
En resumen, ¿Qué se debe tener en cuenta antes de dar un salto a lo profundo? Aquí está la lista de chequeo:
» HAY SUFICIENTES DATOS.
» HAY COMO SUPERAR EL SOBREAJUSTE.
» NO ES NECESARIO COMPRENDER EL FUNCIONAMIENTO INTERNO DEL MODELO.
» HAY PODER DE CÓMPUTO PARA ENTRENAR (O CÓMO PAGARLO).
 
 
¿ESTÁ TODO? ¡FELICITACIONES! DÉ EL SALTO Y DISFRUTE DE LAS VENTAJAS DEL APRENDIZAJE PROFUNDO. ¿FALTA UNO DE LOS CUATRO PUNTOS?
 
Piénselo, tal vez hay otras alternativas de solución a lo que busca. Pero si faltan dos o más puntos, mi sugerencia es antes de sumergirse en lo profundo, nadar en la orilla.
¿Quiere saber más? Conózca nuestro",23
