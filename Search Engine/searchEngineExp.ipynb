{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test of different tokenization methods and Word Embedding for the search engine of the articles of Equinox by Asesoftware"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV of Articles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV Columns: “article_name”, “enumeration_in_article”, “content” \n",
    "“stringWithAllText”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"articles.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whitespace Tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Per Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import spacy\n",
    "\n",
    "'''\n",
    "In this example, we use the Spacy library to preprocess and tokenize the text, \n",
    "lowercasing the text, removing punctuation, lemmatizing the words, and removing stopwords \n",
    "and short words. We then apply this function to each paragraph in the 'content' column of the CSV file using a for loop, \n",
    "and append the resulting list of tokens to a list of lists. The final result is a list of lists, where each \n",
    "sublist contains the tokens of each paragraph.\n",
    "\n",
    "'''\n",
    "\n",
    "# load spacy nlp model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# define function for pre-processing and tokenization\n",
    "def preprocess_text(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # lemmatize\n",
    "    doc = nlp(text)\n",
    "    lemmatized_text = [token.lemma_ for token in doc]\n",
    "    # remove stopwords and short words\n",
    "    stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    tokens = [token for token in lemmatized_text if token not in stopwords and len(token) > 2]\n",
    "    return tokens\n",
    "\n",
    "# apply pre-processing and tokenization to the 'content' column of each row\n",
    "tokenized_paragraphs = []\n",
    "for paragraph in df['content']:\n",
    "    tokens = preprocess_text(paragraph)\n",
    "    tokenized_paragraphs.append(tokens)\n",
    "\n",
    "# print the resulting list of lists of tokens\n",
    "print(tokenized_paragraphs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Full String"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-based Tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Per Paragraph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Full String"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-word Tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Per paragrpah"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Full String"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Per Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "Here we train the Word2Vec model with a list of lists where each sublist is a tokenized paragraph.\n",
    "After we get the word vectors per paragraph, we compute our paragraph meaning vector as the mean\n",
    "of its word vectors.\n",
    "'''\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = gensim.models.Word2Vec(tokenized_paragraphs, size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Calculate the meaning vector per paragraph\n",
    "paragraph_vectors = []\n",
    "for paragraph_tokens in tokenized_paragraphs:\n",
    "    vectors = []\n",
    "    for token in paragraph_tokens:\n",
    "        if token in model.wv.vocab:\n",
    "            vectors.append(model.wv[token])\n",
    "    if len(vectors) > 0:\n",
    "        paragraph_vectors.append(np.mean(vectors, axis=0))\n",
    "    else:\n",
    "        paragraph_vectors.append(np.zeros(model.vector_size))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Full String"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
